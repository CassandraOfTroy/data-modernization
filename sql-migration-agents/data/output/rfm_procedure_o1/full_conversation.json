[
  {
    "content": "\n    Please migrate this SQL stored procedure to PySpark for Microsoft Fabric:\n    \n    ```sql\n    CREATE\nOR\nALTER PROCEDURE [dbo].[usp_CustomerAnalytics_Processing] @DataDate DATE = NULL,\n                                                                          @ProcessType VARCHAR(20) = 'FULL',\n                                                                                       @DebugMode BIT = 0,\n                                                                                                        @RetentionPeriodDays INT = 365,\n                                                                                                                                   @HighValueThreshold DECIMAL(15, 2) = 5000.00,\n                                                                                                                                                       @LoyaltyThreshold INT = 5,\n                                                                                                                                                                               @ChurnDays_VeryHigh INT = 180,\n                                                                                                                                                                                                         @ChurnDays_High INT = 90,\n                                                                                                                                                                                                                               @ChurnDays_Medium INT = 60,\n                                                                                                                                                                                                                                                       @ChurnDays_Low INT = 30 AS BEGIN\nSET NOCOUNT ON; -- Error handling variables\nDECLARE @ErrorMessage NVARCHAR(4000); DECLARE @ErrorSeverity INT; DECLARE @ErrorState INT; DECLARE @ErrorLine INT; DECLARE @ErrorProcedure NVARCHAR(200); -- Processing variables\nDECLARE @ProcessStartTime DATETIME2 = GETDATE(); DECLARE @StepStartTime DATETIME2; DECLARE @StepName VARCHAR(100); DECLARE @RowCount INT = 0; -- Set default date if not provided\nIF @DataDate IS NULL\nSET @DataDate = DATEADD(DAY, -1, CAST(GETDATE() AS DATE)); -- Logging helper table\n\nCREATE TABLE #ProcessLog (LogID INT IDENTITY(1, 1), StepName VARCHAR(100), StartTime DATETIME2, EndTime DATETIME2, RowCount INT, Message VARCHAR(1000)); BEGIN TRY -- Log start\n\nSET @StepName = 'Initialize';\nSET @StepStartTime = GETDATE();\nINSERT INTO #ProcessLog (StepName, StartTime, RowCount, Message)\nVALUES (@StepName, @StepStartTime, 0, 'Starting Customer Analytics processing for date: ' + CONVERT(VARCHAR(10), @DataDate, 120) + ' - Type: ' + @ProcessType); -- Temporary tables for processing\n IF OBJECT_ID('tempdb..#CustomerBase') IS NOT NULL DROP TABLE #CustomerBase; IF OBJECT_ID('tempdb..#TransactionSummary') IS NOT NULL DROP TABLE #TransactionSummary; IF OBJECT_ID('tempdb..#CustomerMetrics') IS NOT NULL DROP TABLE #CustomerMetrics; IF OBJECT_ID('tempdb..#CustomerSegments') IS NOT NULL DROP TABLE #CustomerSegments; -- Step 1: Extract base customer data\n SET @StepName = 'Extract Customer Base'; SET @StepStartTime = GETDATE(); CREATE TABLE #CustomerBase (CustomerID INT, CustomerName NVARCHAR(100), Email NVARCHAR(200), Phone VARCHAR(20), Address NVARCHAR(500), PostalCode VARCHAR(10), City NVARCHAR(100), State NVARCHAR(100), Country NVARCHAR(100), CustomerType VARCHAR(20), AccountManager INT, CreatedDate DATE, ModifiedDate DATE, IsActive BIT, LastContactDate DATE); -- Create index for better performance\n CREATE CLUSTERED INDEX IX_CustomerBase_CustomerID ON #CustomerBase(CustomerID); INSERT INTO #CustomerBase SELECT c.CustomerID, c.FirstName + ' ' + c.LastName AS CustomerName, c.Email, c.Phone, a.StreetAddress, a.PostalCode, a.City, a.State, a.Country, c.CustomerType, c.AccountManagerID, c.CreatedDate, c.ModifiedDate, CASE WHEN c.Status = 'Active' THEN 1 ELSE 0 END AS IsActive, MAX(i.ContactDate) AS LastContactDate FROM dbo.Customers c LEFT JOIN dbo.CustomerAddresses a ON c.CustomerID = a.CustomerID AND a.AddressType = 'Primary' LEFT JOIN dbo.CustomerInteractions i ON c.CustomerID = i.CustomerID WHERE (@ProcessType = 'FULL' OR c.ModifiedDate >= @DataDate OR i.ContactDate >= @DataDate) GROUP BY c.CustomerID, c.FirstName, c.LastName, c.Email, c.Phone, a.StreetAddress, a.PostalCode, a.City, a.State, a.Country, c.CustomerType, c.AccountManagerID, c.CreatedDate, c.ModifiedDate, c.Status; SET @RowCount = @@ROWCOUNT; UPDATE #ProcessLog SET EndTime = GETDATE(), RowCount = @RowCount, Message = 'Extracted base customer data' WHERE StepName = @StepName; -- Step 2: Process Transaction Summary\n SET @StepName = 'Process Transaction Summary'; SET @StepStartTime = GETDATE(); INSERT INTO #ProcessLog (StepName, StartTime, RowCount, Message) VALUES (@StepName, @StepStartTime, 0, 'Calculating transaction metrics'); CREATE TABLE #TransactionSummary (CustomerID INT, TotalOrders INT, TotalSpent DECIMAL(15,2), AvgOrderValue DECIMAL(15,2), FirstPurchaseDate DATE, LastPurchaseDate DATE, DaysSinceLastPurchase INT, TopCategory VARCHAR(50), TopProduct VARCHAR(100), ReturnRate DECIMAL(5,2)); -- Create index for better performance\n CREATE CLUSTERED INDEX IX_TransactionSummary_CustomerID ON #TransactionSummary(CustomerID); -- First, calculate base order metrics\n WITH OrderSummary AS (SELECT o.CustomerID, COUNT(DISTINCT o.OrderID) AS TotalOrders, SUM(od.Quantity * od.UnitPrice * (1 - od.Discount)) AS TotalSpent, MIN(o.OrderDate) AS FirstPurchaseDate, MAX(o.OrderDate) AS LastPurchaseDate FROM dbo.Orders o JOIN dbo.OrderDetails od ON o.OrderID = od.OrderID WHERE o.Status = 'Completed' GROUP BY o.CustomerID) SELECT os.CustomerID, os.TotalOrders, os.TotalSpent, CASE WHEN os.TotalOrders > 0 THEN os.TotalSpent / os.TotalOrders ELSE 0 END AS AvgOrderValue, os.FirstPurchaseDate, os.LastPurchaseDate, DATEDIFF(DAY, os.LastPurchaseDate, @DataDate) AS DaysSinceLastPurchase, NULL AS TopCategory,\n\n                                                                                                                                                        -- Will update later\n NULL AS TopProduct, -- Will update later\n 0 AS ReturnRate -- Will update later\n INTO #TempOrderSummary FROM OrderSummary os WHERE os.CustomerID IN (SELECT CustomerID FROM #CustomerBase); -- Create index for the temp table\n CREATE CLUSTERED INDEX IX_TempOrderSummary_CustomerID ON #TempOrderSummary(CustomerID); -- Now insert the initial data into transaction summary\n INSERT INTO #TransactionSummary SELECT * FROM #TempOrderSummary; -- Update top category\n WITH ProductCategories AS (SELECT o.CustomerID, p.Category, SUM(od.Quantity * od.UnitPrice * (1 - od.Discount)) AS CategorySpend, ROW_NUMBER() OVER (PARTITION BY o.CustomerID ORDER BY SUM(od.Quantity * od.UnitPrice * (1 - od.Discount)) DESC) AS CategoryRank FROM dbo.Orders o JOIN dbo.OrderDetails od ON o.OrderID = od.OrderID JOIN dbo.Products p ON od.ProductID = p.ProductID WHERE o.Status = 'Completed' GROUP BY o.CustomerID, p.Category) UPDATE ts SET ts.TopCategory = pc.Category FROM #TransactionSummary ts JOIN ProductCategories pc ON ts.CustomerID = pc.CustomerID AND pc.CategoryRank = 1; -- Update top product\n WITH TopProducts AS (SELECT o.CustomerID, p.ProductName, SUM(od.Quantity) AS TotalQuantity, ROW_NUMBER() OVER (PARTITION BY o.CustomerID ORDER BY SUM(od.Quantity) DESC) AS ProductRank FROM dbo.Orders o JOIN dbo.OrderDetails od ON o.OrderID = od.OrderID JOIN dbo.Products p ON od.ProductID = p.ProductID WHERE o.Status = 'Completed' GROUP BY o.CustomerID, p.ProductName) UPDATE ts SET ts.TopProduct = tp.ProductName FROM #TransactionSummary ts JOIN TopProducts tp ON ts.CustomerID = tp.CustomerID AND tp.ProductRank = 1; -- Update return rate\n WITH RETURNS AS (SELECT r.CustomerID, COUNT(DISTINCT r.ReturnID) AS TotalReturns, tos.TotalOrders, CASE WHEN tos.TotalOrders > 0 THEN CAST(COUNT(DISTINCT r.ReturnID) AS DECIMAL(5,2)) / CAST(tos.TotalOrders AS DECIMAL(5,2)) * 100 ELSE 0 END AS ReturnRate FROM dbo.Returns r JOIN #TempOrderSummary tos ON r.CustomerID = tos.CustomerID GROUP BY r.CustomerID, tos.TotalOrders) UPDATE ts SET ts.ReturnRate = r.ReturnRate FROM #TransactionSummary ts JOIN RETURNS r ON ts.CustomerID = r.CustomerID; -- Drop the temporary table now that we're done with it\n DROP TABLE #TempOrderSummary; SET @RowCount = @@ROWCOUNT; UPDATE #ProcessLog SET EndTime = GETDATE(), RowCount = @RowCount, Message = 'Calculated transaction summary metrics' WHERE StepName = @StepName; -- Step 3: Customer Metrics Calculation\n SET @StepName = 'Calculate Customer Metrics'; SET @StepStartTime = GETDATE(); INSERT INTO #ProcessLog (StepName, StartTime, RowCount, Message) VALUES (@StepName, @StepStartTime, 0, 'Calculating advanced customer metrics'); CREATE TABLE #CustomerMetrics (CustomerID INT, CustomerLifetimeValue DECIMAL(15,2), RecencyScore INT, FrequencyScore INT, MonetaryScore INT, RFMScore INT, ChurnProbability DECIMAL(5,2), NextPurchasePropensity DECIMAL(5,2), LoyaltyIndex DECIMAL(5,2), CustomerHealth VARCHAR(20)); -- Create index for better performance\n CREATE CLUSTERED INDEX IX_CustomerMetrics_CustomerID ON #CustomerMetrics(CustomerID); -- First calculate RFM scores\n WITH RFMScores AS (SELECT ts.CustomerID, -- Recency: 5 is most recent\n NTILE(5) OVER (ORDER BY ISNULL(ts.DaysSinceLastPurchase, 999999) ASC) AS RecencyScore, -- Frequency: 5 is most frequent\n NTILE(5) OVER (ORDER BY ISNULL(ts.TotalOrders, 0) DESC) AS FrequencyScore, -- Monetary: 5 is highest value\n NTILE(5) OVER (ORDER BY ISNULL(ts.TotalSpent, 0) DESC) AS MonetaryScore FROM #TransactionSummary ts) SELECT rfm.CustomerID, rfm.RecencyScore, rfm.FrequencyScore, rfm.MonetaryScore, (rfm.RecencyScore + rfm.FrequencyScore + rfm.MonetaryScore) AS RFMScore INTO #TempRFM FROM RFMScores rfm; -- Create index for the temp table\n CREATE CLUSTERED INDEX IX_TempRFM_CustomerID ON #TempRFM(CustomerID); -- Now calculate churn probabilities and other metrics\n WITH ChurnCalculation AS (SELECT ts.CustomerID, -- Churn probability based on configurable parameters\n CASE WHEN ts.DaysSinceLastPurchase > @ChurnDays_VeryHigh THEN 0.8 WHEN ts.DaysSinceLastPurchase > @ChurnDays_High THEN 0.5 WHEN ts.DaysSinceLastPurchase > @ChurnDays_Medium THEN 0.3 WHEN ts.DaysSinceLastPurchase > @ChurnDays_Low THEN 0.1 ELSE 0.05 END AS ChurnProbability, -- Next purchase propensity\n CASE WHEN ts.DaysSinceLastPurchase < @ChurnDays_Low AND ts.TotalOrders > 3 THEN 0.8 WHEN ts.DaysSinceLastPurchase < @ChurnDays_Medium AND ts.TotalOrders > 2 THEN 0.6 WHEN ts.DaysSinceLastPurchase < @ChurnDays_High THEN 0.4 ELSE 0.2 END AS NextPurchasePropensity, -- Loyalty index - a normalized score (0-1) based on order count and return rate\n CASE WHEN ts.TotalOrders >= @LoyaltyThreshold AND ts.ReturnRate < 10 THEN (ts.TotalOrders * 0.6) + ((100 - ts.ReturnRate) * 0.4) ELSE (ts.TotalOrders * 0.4) END / 10 AS LoyaltyIndex FROM #TransactionSummary ts) INSERT INTO #CustomerMetrics SELECT rfm.CustomerID, \n                 \n                                                                                                                                                       -- Customer Lifetime Value (simplified calculation using churn probability)\n ts.TotalSpent * (1 + (1 - ch.ChurnProbability)) AS CustomerLifetimeValue, rfm.RecencyScore, rfm.FrequencyScore, rfm.MonetaryScore, rfm.RFMScore, ch.ChurnProbability, ch.NextPurchasePropensity, ch.LoyaltyIndex, -- Customer Health derived from metrics\n CASE WHEN ch.ChurnProbability > 0.7 THEN 'At Risk' WHEN ch.LoyaltyIndex > 0.8 THEN 'Excellent' WHEN ch.LoyaltyIndex > 0.6 THEN 'Good' WHEN ch.LoyaltyIndex > 0.4 THEN 'Average' ELSE 'Needs Attention' END AS CustomerHealth FROM #TempRFM rfm JOIN #TransactionSummary ts ON rfm.CustomerID = ts.CustomerID JOIN ChurnCalculation ch ON rfm.CustomerID = ch.CustomerID; -- Drop the temporary RFM table now that we're done with it\n DROP TABLE #TempRFM; SET @RowCount = @@ROWCOUNT; UPDATE #ProcessLog SET EndTime = GETDATE(), RowCount = @RowCount, Message = 'Calculated advanced customer metrics' WHERE StepName = @StepName; -- Step 4: Customer Segmentation\n SET @StepName = 'Customer Segmentation'; SET @StepStartTime = GETDATE(); INSERT INTO #ProcessLog (StepName, StartTime, RowCount, Message) VALUES (@StepName, @StepStartTime, 0, 'Performing customer segmentation'); CREATE TABLE #CustomerSegments (CustomerID INT, ValueSegment VARCHAR(50), BehaviorSegment VARCHAR(50), LifecycleSegment VARCHAR(50), TargetGroup VARCHAR(50), MarketingRecommendation VARCHAR(MAX)); -- Create index for better performance\n CREATE CLUSTERED INDEX IX_CustomerSegments_CustomerID ON #CustomerSegments(CustomerID); -- Insert into segments table\n INSERT INTO #CustomerSegments SELECT cb.CustomerID,\n                                                                                                                                                  -- Value Segmentation\n CASE WHEN cm.CustomerLifetimeValue >= @HighValueThreshold THEN 'High Value' WHEN cm.CustomerLifetimeValue >= @HighValueThreshold * 0.5 THEN 'Medium Value' ELSE 'Low Value' END AS ValueSegment, -- Behavior Segmentation\n CASE WHEN cm.RFMScore >= 13 THEN 'Champions' WHEN cm.RecencyScore >= 4 AND cm.FrequencyScore >= 3 THEN 'Loyal Customers' WHEN cm.RecencyScore >= 4 AND cm.FrequencyScore <= 2 THEN 'Potential Loyalists' WHEN cm.RecencyScore <= 2 AND cm.FrequencyScore >= 3 AND cm.MonetaryScore >= 3 THEN 'At Risk' WHEN cm.RecencyScore <= 2 AND cm.FrequencyScore <= 2 AND cm.MonetaryScore <= 2 THEN 'Hibernating' WHEN cm.RecencyScore <= 1 THEN 'Lost' ELSE 'Others' END AS BehaviorSegment, -- Lifecycle Segmentation\n CASE WHEN ts.TotalOrders = 1 AND ts.DaysSinceLastPurchase <= 30 THEN 'New Customer' WHEN ts.TotalOrders > 1 AND cm.ChurnProbability < 0.3 THEN 'Active' WHEN cm.ChurnProbability >= 0.3 AND cm.ChurnProbability < 0.7 THEN 'At Risk' WHEN cm.ChurnProbability >= 0.7 THEN 'Churned' ELSE 'Inactive' END AS LifecycleSegment, -- Target Group\n CASE WHEN cm.CustomerLifetimeValue >= @HighValueThreshold AND cm.ChurnProbability < 0.3 THEN 'VIP' WHEN cm.CustomerLifetimeValue >= @HighValueThreshold * 0.5 AND cm.ChurnProbability >= 0.3 THEN 'Retention Priority' WHEN ts.TotalOrders = 1 AND ts.DaysSinceLastPurchase <= 30 THEN 'Nurture New' WHEN cm.NextPurchasePropensity > 0.6 THEN 'Growth Opportunity' WHEN ts.ReturnRate > 20 THEN 'Service Improvement' ELSE 'Standard' END AS TargetGroup, -- Marketing Recommendation\n CASE WHEN cm.CustomerLifetimeValue >= @HighValueThreshold AND cm.ChurnProbability < 0.3 THEN 'Exclusive offers, VIP events, Personal shopping assistance, Early access to new products' WHEN cm.CustomerLifetimeValue >= @HighValueThreshold * 0.5 AND cm.ChurnProbability >= 0.3 THEN 'Re-engagement campaign, Loyalty rewards, Personalized recommendations based on past purchases' WHEN ts.TotalOrders = 1 AND ts.DaysSinceLastPurchase <= 30 THEN 'Welcome series, Educational content, First purchase follow-up, Category exploration' WHEN cm.NextPurchasePropensity > 0.6 THEN 'Cross-sell/upsell, Bundle offers based on ' + ISNULL(ts.TopCategory, 'preferred category') + ', Category expansion' WHEN ts.ReturnRate > 20 THEN 'Satisfaction survey, Improved product information, Size/fit guides, Service recovery' ELSE 'Standard seasonal promotions, Category newsletters, Reactivation after ' + CAST(ISNULL(ts.DaysSinceLastPurchase, 0) / 30 AS VARCHAR) + ' months' END AS MarketingRecommendation FROM #CustomerBase cb JOIN #CustomerMetrics cm ON cb.CustomerID = cm.CustomerID JOIN #TransactionSummary ts ON cb.CustomerID = ts.CustomerID; SET @RowCount = @@ROWCOUNT; UPDATE #ProcessLog SET EndTime = GETDATE(), RowCount = @RowCount, Message = 'Completed customer segmentation' WHERE StepName = @StepName; -- Step 5: Insert into target tables\n SET @StepName = 'Data Persistence'; SET @StepStartTime = GETDATE(); INSERT INTO #ProcessLog (StepName, StartTime, RowCount, Message) VALUES (@StepName, @StepStartTime, 0, 'Persisting data to target tables'); -- Set the isolation level explicitly for data modifications\n SET TRANSACTION ISOLATION LEVEL READ COMMITTED; -- Customer Profile table update - using batching for improved performance\n DECLARE @BatchSize INT = 10000; DECLARE @BatchCount INT = 0; DECLARE @TotalCustomers INT = (SELECT COUNT(*) FROM #CustomerBase); DECLARE @LastCustomerID INT = 0; WHILE @BatchCount < @TotalCustomers BEGIN BEGIN TRANSACTION; -- MERGE statement for Customer Profile (batched)\n\nMERGE INTO dbo.CustomerProfiles AS target USING\n  (SELECT TOP (@BatchSize) cb.CustomerID,\n          cb.CustomerName,\n          cb.Email,\n          cb.Phone,\n          cb.Address,\n          cb.PostalCode,\n          cb.City,\n          cb.State,\n          cb.Country,\n          cb.CustomerType,\n          cb.AccountManager,\n          cb.CreatedDate,\n          cb.IsActive,\n          cb.LastContactDate,\n          ts.TotalOrders,\n          ts.TotalSpent,\n          ts.AvgOrderValue,\n          ts.FirstPurchaseDate,\n          ts.LastPurchaseDate,\n          ts.DaysSinceLastPurchase,\n          ts.TopCategory,\n          ts.TopProduct,\n          ts.ReturnRate\n   FROM #CustomerBase cb\n   LEFT JOIN #TransactionSummary ts ON cb.CustomerID = ts.CustomerID\n   WHERE cb.CustomerID > @LastCustomerID\n   ORDER BY cb.CustomerID) AS SOURCE ON (target.CustomerID = source.CustomerID) WHEN MATCHED THEN\nUPDATE\nSET target.CustomerName = source.CustomerName,\n    target.Email = source.Email,\n    target.Phone = source.Phone,\n    target.Address = source.Address,\n    target.PostalCode = source.PostalCode,\n    target.City = source.City,\n    target.State = source.State,\n    target.Country = source.Country,\n    target.CustomerType = source.CustomerType,\n    target.AccountManager = source.AccountManager,\n    target.IsActive = source.IsActive,\n    target.LastContactDate = source.LastContactDate,\n    target.TotalOrders = source.TotalOrders,\n    target.TotalSpent = source.TotalSpent,\n    target.AvgOrderValue = source.AvgOrderValue,\n    target.FirstPurchaseDate = source.FirstPurchaseDate,\n    target.LastPurchaseDate = source.LastPurchaseDate,\n    target.DaysSinceLastPurchase = source.DaysSinceLastPurchase,\n    target.TopCategory = source.TopCategory,\n    target.TopProduct = source.TopProduct,\n    target.ReturnRate = source.ReturnRate,\n    target.ModifiedDate = GETDATE() WHEN NOT MATCHED THEN\nINSERT (CustomerID,\n        CustomerName,\n        Email,\n        Phone,\n        Address,\n        PostalCode,\n        City,\n        State,\n        Country,\n        CustomerType,\n        AccountManager,\n        CreatedDate,\n        IsActive,\n        LastContactDate,\n        TotalOrders,\n        TotalSpent,\n        AvgOrderValue,\n        FirstPurchaseDate,\n        LastPurchaseDate,\n        DaysSinceLastPurchase,\n        TopCategory,\n        TopProduct,\n        ReturnRate,\n        ModifiedDate)\nVALUES (source.CustomerID,\n        source.CustomerName,\n        source.Email,\n        source.Phone,\n        source.Address,\n        source.PostalCode,\n        source.City,\n        source.State,\n        source.Country,\n        source.CustomerType,\n        source.AccountManager,\n        source.CreatedDate,\n        source.IsActive,\n        source.LastContactDate,\n        source.TotalOrders,\n        source.TotalSpent,\n        source.AvgOrderValue,\n        source.FirstPurchaseDate,\n        source.LastPurchaseDate,\n        source.DaysSinceLastPurchase,\n        source.TopCategory,\n        source.TopProduct,\n        source.ReturnRate,\n        GETDATE());\nSET @LastCustomerID =\n  (SELECT MAX(CustomerID)\n   FROM #CustomerBase\n   WHERE CustomerID > @LastCustomerID\n     AND CustomerID <= @LastCustomerID + @BatchSize);\nSET @BatchCount = @BatchCount + @@ROWCOUNT;\nCOMMIT TRANSACTION; END -- Customer Analytics table update - replace data for the date (also batched)\n -- First delete existing records for this date\n BEGIN TRANSACTION;\nDELETE\nFROM dbo.CustomerAnalytics\nWHERE ProcessDate = @DataDate;\n  COMMIT TRANSACTION; -- Now insert in batches\n\n  SET @BatchCount = 0;\n  SET @LastCustomerID = 0; WHILE @BatchCount < @TotalCustomers BEGIN BEGIN TRANSACTION;\n  INSERT INTO dbo.CustomerAnalytics (ProcessDate, CustomerID, CustomerLifetimeValue, RecencyScore, FrequencyScore, MonetaryScore, RFMScore, ChurnProbability, NextPurchasePropensity, LoyaltyIndex, CustomerHealth, ValueSegment, BehaviorSegment, LifecycleSegment, TargetGroup, MarketingRecommendation)\n  SELECT TOP (@BatchSize) @DataDate,\n         cm.CustomerID,\n         cm.CustomerLifetimeValue,\n         cm.RecencyScore,\n         cm.FrequencyScore,\n         cm.MonetaryScore,\n         cm.RFMScore,\n         cm.ChurnProbability,\n         cm.NextPurchasePropensity,\n         cm.LoyaltyIndex,\n         cm.CustomerHealth,\n         cs.ValueSegment,\n         cs.BehaviorSegment,\n         cs.LifecycleSegment,\n         cs.TargetGroup,\n         cs.MarketingRecommendation\n  FROM #CustomerMetrics cm\n  JOIN #CustomerSegments cs ON cm.CustomerID = cs.CustomerID\n  WHERE cm.CustomerID > @LastCustomerID\n  ORDER BY cm.CustomerID;\n  SET @RowCount = @@ROWCOUNT;\n  SET @BatchCount = @BatchCount + @RowCount;\n  SET @LastCustomerID =\n    (SELECT MAX(CustomerID)\n     FROM #CustomerMetrics\n     WHERE CustomerID > @LastCustomerID\n       AND CustomerID <= @LastCustomerID + @BatchSize);\n  COMMIT TRANSACTION; END -- Cleanup data older than retention period\n IF @DebugMode = 0 BEGIN BEGIN TRANSACTION;\n  DELETE\n  FROM dbo.CustomerAnalytics\n  WHERE ProcessDate < DATEADD(DAY, -@RetentionPeriodDays, @DataDate);\n    COMMIT TRANSACTION; END\n  UPDATE #ProcessLog\n  SET EndTime = GETDATE(),\n      RowCount = @BatchCount,\n      Message = 'Data persistence completed'\n  WHERE StepName = @StepName; -- Step 6: Generate summary report\n\n    SET @StepName = 'Generate Summary';\n    SET @StepStartTime = GETDATE();\n    INSERT INTO #ProcessLog (StepName, StartTime, RowCount, Message)\n  VALUES (@StepName,\n          @StepStartTime,\n          0,\n          'Generating summary statistics'); -- Insert into summary log\n BEGIN TRANSACTION;\n  INSERT INTO dbo.CustomerAnalyticsSummary (ProcessDate, ProcessType, TotalCustomers, ActiveCustomers, NewCustomers, ChurningCustomers, HighValueCustomers, AverageLifetimeValue, TotalProcessingTimeMs, ProcessingStatus)\n  SELECT @DataDate,\n         @ProcessType,\n         COUNT(DISTINCT cb.CustomerID),\n         SUM(CASE\n                 WHEN cs.LifecycleSegment = 'Active' THEN 1\n                 ELSE 0\n             END),\n         SUM(CASE\n                 WHEN cs.LifecycleSegment = 'New Customer' THEN 1\n                 ELSE 0\n             END),\n         SUM(CASE\n                 WHEN cs.LifecycleSegment = 'At Risk'\n                      OR cs.LifecycleSegment = 'Churned' THEN 1\n                 ELSE 0\n             END),\n         SUM(CASE\n                 WHEN cs.ValueSegment = 'High Value' THEN 1\n                 ELSE 0\n             END),\n         AVG(cm.CustomerLifetimeValue),\n         DATEDIFF(MILLISECOND, @ProcessStartTime, GETDATE()),\n         'Success'\n  FROM #CustomerBase cb\n  JOIN #CustomerMetrics cm ON cb.CustomerID = cm.CustomerID\n  JOIN #CustomerSegments cs ON cb.CustomerID = cs.CustomerID;\n  SET @RowCount = @@ROWCOUNT;\n  COMMIT TRANSACTION;\n  UPDATE #ProcessLog\n  SET EndTime = GETDATE(),\n      RowCount = @RowCount,\n      Message = 'Summary statistics generated'\n  WHERE StepName = @StepName; -- Log all process steps for debugging\n IF @DebugMode = 1 BEGIN\n    SELECT *\n    FROM #ProcessLog\n    ORDER BY LogID; END -- Cleanup temporary tables\n IF OBJECT_ID('tempdb..#CustomerBase') IS NOT NULL\n    DROP TABLE #CustomerBase; IF OBJECT_ID('tempdb..#TransactionSummary') IS NOT NULL\n    DROP TABLE #TransactionSummary; IF OBJECT_ID('tempdb..#CustomerMetrics') IS NOT NULL\n    DROP TABLE #CustomerMetrics; IF OBJECT_ID('tempdb..#CustomerSegments') IS NOT NULL\n    DROP TABLE #CustomerSegments; IF OBJECT_ID('tempdb..#TempOrderSummary') IS NOT NULL\n    DROP TABLE #TempOrderSummary; IF OBJECT_ID('tempdb..#TempRFM') IS NOT NULL\n    DROP TABLE #TempRFM; END TRY BEGIN CATCH -- Error handling\n\n  SELECT @ErrorMessage = ERROR_MESSAGE(),\n         @ErrorSeverity = ERROR_SEVERITY(),\n         @ErrorState = ERROR_STATE(),\n         @ErrorLine = ERROR_LINE(),\n         @ErrorProcedure = ERROR_PROCEDURE(); -- Rollback any open transaction\n IF @@TRANCOUNT > 0\n  ROLLBACK TRANSACTION; -- Log error to process log if it still exists\n IF OBJECT_ID('tempdb..#ProcessLog') IS NOT NULL BEGIN\n  INSERT INTO #ProcessLog (StepName, StartTime, EndTime, RowCount, Message)\n  VALUES (@StepName,\n          @StepStartTime,\n          GETDATE(),\n          -1,\n          'ERROR: ' + @ErrorMessage); END -- Insert error record into summary\n\n  INSERT INTO dbo.CustomerAnalyticsSummary (ProcessDate, ProcessType, TotalCustomers, TotalProcessingTimeMs, ProcessingStatus, ErrorMessage)\n  VALUES (@DataDate,\n          @ProcessType,\n          0,\n          DATEDIFF(MILLISECOND, @ProcessStartTime, GETDATE()),\n          'Failed',\n          @ErrorMessage + ' at line ' + CAST(@ErrorLine AS VARCHAR(10))); -- Output for debugging\n IF @DebugMode = 1\n  AND OBJECT_ID('tempdb..#ProcessLog') IS NOT NULL BEGIN\n  SELECT *\n  FROM #ProcessLog\n  ORDER BY LogID; END -- Cleanup temporary tables before exiting - check if each exists first\n IF OBJECT_ID('tempdb..#CustomerBase') IS NOT NULL\n  DROP TABLE #CustomerBase; IF OBJECT_ID('tempdb..#TransactionSummary') IS NOT NULL\n  DROP TABLE #TransactionSummary; IF OBJECT_ID('tempdb..#CustomerMetrics') IS NOT NULL\n  DROP TABLE #CustomerMetrics; IF OBJECT_ID('tempdb..#CustomerSegments') IS NOT NULL\n  DROP TABLE #CustomerSegments; IF OBJECT_ID('tempdb..#TempOrderSummary') IS NOT NULL\n  DROP TABLE #TempOrderSummary; IF OBJECT_ID('tempdb..#TempRFM') IS NOT NULL\n  DROP TABLE #TempRFM; IF OBJECT_ID('tempdb..#ProcessLog') IS NOT NULL\n  DROP TABLE #ProcessLog; -- Rethrow the error\n RAISERROR(@ErrorMessage, @ErrorSeverity, @ErrorState); END CATCH; END;\n    ```\n    \n    Additional context: \n    \n    I need a complete migration plan and implementation following the medallion architecture with:\n    1. Business analysis and migration plan\n    2. PySpark code for bronze, silver, and gold layers\n    3. Implementation guidelines and best practices\n    4. Test cases to validate the migration\n    \n    The final PySpark code should be production-ready, well-commented, and follow Microsoft Fabric best practices.\n    \n    BusinessAnalyst - analyze business requirements\n    ProductOwner - create the migration plan\n    AzureDataEngineer - translate SQL to PySpark with medallion architecture\n    TechLead - review code quality and architecture\n    TestingAgent - create test cases\n    ",
    "role": "user",
    "name": "User"
  },
  {
    "content": "Below is a comprehensive plan and example implementation for migrating the given SQL stored procedure to a PySpark solution within Microsoft Fabric, following a Medallion (Bronze–Silver–Gold) architecture approach. Since the request explicitly includes multiple roles and perspectives, this answer consolidates them into four sections:\n\n1) Business Analysis and Migration Plan  \n2) PySpark Code for Bronze, Silver, and Gold Layers  \n3) Implementation Guidelines and Best Practices  \n4) Test Cases for Validation  \n\n────────────────────────────────────────────────────────────────────────\n1) BUSINESS ANALYSIS AND MIGRATION PLAN (BusinessAnalyst and ProductOwner)\n────────────────────────────────────────────────────────────────────────\n\n• Business Purpose  \n  - The original SQL stored procedure, usp_CustomerAnalytics_Processing, delivers insights about customer behavior, lifetime value, churn risk, and segmentation.  \n  - It helps marketing, sales, and customer success teams identify loyalty trends, churn risks, high-value customers, and potential product/category preferences.  \n  - It produces daily summary statistics and updates analytics tables for further insights and dashboards.\n\n• Key Business Metrics  \n  - RFM (Recency, Frequency, Monetary) scores for each customer to measure engagement.  \n  - Calculations for churn probability, loyalty index, and next-purchase propensity.  \n  - Segmentation (ValueSegment, BehaviorSegment, LifecycleSegment, TargetGroup, MarketingRecommendation).  \n  - High-level summaries such as Active vs. Churning customers, average lifetime value, etc.\n\n• Data Sources  \n  - Customers, CustomerAddresses, and CustomerInteractions for basic customer data.  \n  - Orders, OrderDetails, Products, and Returns for purchase and return details.  \n  - Derived or aggregated tables in existing data warehouse (CustomerProfiles, CustomerAnalytics, CustomerAnalyticsSummary).\n\n• Business Logic and Rules Extracted  \n  1. Only consider ‘Active’ or ‘Modified’ records since a given date (supporting incremental runs or full runs).  \n  2. Churn Probability thresholds (ChurnDays_VeryHigh = 180, High = 90, Medium = 60, Low = 30).  \n  3. Loyalty Threshold (e.g., 5 or more orders considered loyal).  \n  4. Retention Period (default 365 days) for trimming analytics data.  \n  5. Dynamic classification based on RFM and threshold parameters (HighValueThreshold = 5000, etc.).\n\n• Migration Approach   \n  - Use the Medallion architecture in Microsoft Fabric to maintain a clean data flow:  \n    1. Bronze Layer: Ingest raw tables (Customers, Orders, etc.) from the source system.  \n    2. Silver Layer: Apply transformations (RFM calculations, churn probability, join with addresses, etc.).  \n    3. Gold Layer: Deliver aggregated analytics tables (CustomerAnalytics, summary tables) for end-user consumption.  \n    4. Leverage Lakehouse or Warehouse features in Fabric for incremental ingestion, Delta merges, and optimal concurrency.  \n\n────────────────────────────────────────────────────────────────────────\n2) P Y S P A R K   C O D E   F O R   B R O N Z E,   S I L V E R,   A N D   G O L D\n────────────────────────────────────────────────────────────────────────\nBelow is an example PySpark notebook or script structure that reflects the stored procedure logic. This code assumes Microsoft Fabric’s Lakehouse or Warehouse as both source and target, using Delta tables. Adjust table paths/names based on your specific environment.\n\n--------------------------------------------------------------------------------\n-- BRONZE LAYER (Raw Ingestion)\n--------------------------------------------------------------------------------\nfrom pyspark.sql import SparkSession, functions as F, Window\nfrom pyspark.sql.types import DecimalType, IntegerType, StringType, DateType\n\n# In Microsoft Fabric, SparkSession is typically available by default in notebooks:\nspark = SparkSession.builder.getOrCreate()\n\n# Parameters (matching SP defaults)\ndataDate = None  # If None, defaults to yesterday\nprocessType = 'FULL'\ndebugMode = False\nretentionPeriodDays = 365\nhighValueThreshold = 5000.00\nloyaltyThreshold = 5\nchurnDaysVeryHigh = 180\nchurnDaysHigh = 90\nchurnDaysMedium = 60\nchurnDaysLow = 30\n\n# Set data date if not provided\nimport datetime\nif dataDate is None:\n    dataDate = datetime.date.today() - datetime.timedelta(days=1)\n  \n# 1) Read Raw Data from Source Tables (BRONZE)\ncustomersBronzeDF = spark.read.table(\"lakehouse_raw.Customers\")\naddressesBronzeDF = spark.read.table(\"lakehouse_raw.CustomerAddresses\")\ninteractionsBronzeDF = spark.read.table(\"lakehouse_raw.CustomerInteractions\")\nordersBronzeDF = spark.read.table(\"lakehouse_raw.Orders\")\norderDetailsBronzeDF = spark.read.table(\"lakehouse_raw.OrderDetails\")\nproductsBronzeDF = spark.read.table(\"lakehouse_raw.Products\")\nreturnsBronzeDF = spark.read.table(\"lakehouse_raw.Returns\")\n\n# You might also ingest them from files in a \"Bronze\" folder. \n# E.g.: spark.read.format(\"delta\").load(\"/path/bronze/Customers\") etc.\n\n--------------------------------------------------------------------------------\n-- SILVER LAYER (Transformations & Joins)\n--------------------------------------------------------------------------------\n\n# ---- Step 1: Extract Customer Base (like #CustomerBase in stored procedure) ----\n# Join with addresses & interactions; filter & group. We replicate logic from SP.\n\ncustomerBaseDF = (\n    customersBronzeDF.alias(\"c\")\n    .join(\n        addressesBronzeDF.alias(\"a\"),\n        (F.col(\"c.CustomerID\") == F.col(\"a.CustomerID\")) & (F.col(\"a.AddressType\") == \"Primary\"),\n        \"left\"\n    )\n    .join(\n        interactionsBronzeDF.alias(\"i\"),\n        F.col(\"c.CustomerID\") == F.col(\"i.CustomerID\"),\n        \"left\"\n    )\n    .where(\n        (processType == 'FULL') | \n        (F.col(\"c.ModifiedDate\") >= F.lit(dataDate)) | \n        (F.col(\"i.ContactDate\") >= F.lit(dataDate))\n    )\n    .groupBy(\n        \"c.CustomerID\",\n        \"c.FirstName\",\n        \"c.LastName\",\n        \"c.Email\",\n        \"c.Phone\",\n        \"a.StreetAddress\",\n        \"a.PostalCode\",\n        \"a.City\",\n        \"a.State\",\n        \"a.Country\",\n        \"c.CustomerType\",\n        \"c.AccountManagerID\",\n        \"c.CreatedDate\",\n        \"c.ModifiedDate\",\n        \"c.Status\"\n    )\n    .agg(F.max(\"i.ContactDate\").alias(\"LastContactDate\"))\n    .select(\n        F.col(\"CustomerID\"),\n        F.concat_ws(\" \", F.col(\"FirstName\"), F.col(\"LastName\")).alias(\"CustomerName\"),\n        F.col(\"Email\"),\n        F.col(\"Phone\"),\n        F.col(\"StreetAddress\").alias(\"Address\"),\n        F.col(\"PostalCode\"),\n        F.col(\"City\"),\n        F.col(\"State\"),\n        F.col(\"Country\"),\n        F.col(\"CustomerType\"),\n        F.col(\"AccountManagerID\").alias(\"AccountManager\"),\n        F.col(\"CreatedDate\"),\n        F.col(\"ModifiedDate\"),\n        (F.col(\"Status\") == \"Active\").cast(\"boolean\").alias(\"IsActive\"),\n        F.col(\"LastContactDate\").alias(\"LastContactDate\")\n    )\n)\n\n# ---- Step 2: Process Transaction Summary (like #TransactionSummary) ----\n# 2a) Calculate base metrics\norderSummaryDF = (\n    ordersBronzeDF.alias(\"o\")\n    .join(\n        orderDetailsBronzeDF.alias(\"od\"),\n        \"OrderID\",  # same col in both\n        \"inner\"\n    )\n    .where(F.col(\"o.Status\") == \"Completed\")\n    .groupBy(\"o.CustomerID\")\n    .agg(\n        F.countDistinct(\"o.OrderID\").alias(\"TotalOrders\"),\n        F.sum(F.col(\"od.Quantity\") * F.col(\"od.UnitPrice\") * (1 - F.col(\"od.Discount\"))).alias(\"TotalSpent\"),\n        F.min(\"o.OrderDate\").alias(\"FirstPurchaseDate\"),\n        F.max(\"o.OrderDate\").alias(\"LastPurchaseDate\")\n    )\n)\n\n# 2b) Join with #CustomerBase subset\ntransactionSummaryDF = (\n    orderSummaryDF.alias(\"os\")\n    .join(\n        customerBaseDF.select(\"CustomerID\").alias(\"cb\"),\n        F.col(\"os.CustomerID\") == F.col(\"cb.CustomerID\"),\n        \"inner\"\n    )\n    .select(\n        \"os.CustomerID\",\n        \"os.TotalOrders\",\n        \"os.TotalSpent\",\n        F.when(F.col(\"os.TotalOrders\") > 0,\n               F.col(\"os.TotalSpent\") / F.col(\"os.TotalOrders\")).otherwise(F.lit(0)).alias(\"AvgOrderValue\"),\n        \"os.FirstPurchaseDate\",\n        \"os.LastPurchaseDate\",\n        (F.datediff(F.lit(dataDate), F.col(\"os.LastPurchaseDate\"))).alias(\"DaysSinceLastPurchase\"),\n    )\n    # Will fill placeholders for TopCategory, TopProduct, ReturnRate next\n)\n\n# 2c) Highest-spend category per customer\ncategorySpendDF = (\n    ordersBronzeDF.alias(\"o\")\n    .join(orderDetailsBronzeDF.alias(\"od\"), \"OrderID\")\n    .join(productsBronzeDF.alias(\"p\"), \"ProductID\")\n    .where(F.col(\"o.Status\") == \"Completed\")\n    .groupBy(\"o.CustomerID\", \"p.Category\")\n    .agg(F.sum(F.col(\"od.Quantity\") * F.col(\"od.UnitPrice\") * (1 - F.col(\"od.Discount\"))).alias(\"CategorySpend\"))\n)\n\nwindowCategory = Window.partitionBy(\"CustomerID\").orderBy(F.col(\"CategorySpend\").desc())\ncategoryRankedDF = categorySpendDF.withColumn(\n    \"CategoryRank\",\n    F.row_number().over(windowCategory)\n).where(F.col(\"CategoryRank\") == 1)\n\n# 2d) Most frequently purchased product\nproductQtyDF = (\n    ordersBronzeDF.alias(\"o\")\n    .join(orderDetailsBronzeDF.alias(\"od\"), \"OrderID\")\n    .join(productsBronzeDF.alias(\"p\"), \"ProductID\")\n    .where(F.col(\"o.Status\") == \"Completed\")\n    .groupBy(\"o.CustomerID\", \"p.ProductName\")\n    .agg(F.sum(\"od.Quantity\").alias(\"TotalQuantity\"))\n)\n\nwindowProduct = Window.partitionBy(\"CustomerID\").orderBy(F.col(\"TotalQuantity\").desc())\nproductRankedDF = productQtyDF.withColumn(\n    \"ProductRank\",\n    F.row_number().over(windowProduct)\n).where(F.col(\"ProductRank\") == 1)\n\n# 2e) Return rate\nreturnsGroupedDF = (\n    returnsBronzeDF.alias(\"r\")\n    .groupBy(\"r.CustomerID\")\n    .agg(F.countDistinct(\"ReturnID\").alias(\"TotalReturns\"))\n)\n\n# Join all partial metrics to produce the final transaction summary:\ntransactionSummaryDF = (\n    transactionSummaryDF.alias(\"ts\")\n    # Join top category\n    .join(\n        categoryRankedDF.alias(\"cat\"),\n        \"CustomerID\",\n        \"left\"\n    )\n    .join(\n        productRankedDF.alias(\"prod\"),\n        \"CustomerID\",\n        \"left\"\n    )\n    .join(\n        returnsGroupedDF.alias(\"ret\"),\n        \"CustomerID\",\n        \"left\"\n    )\n    .select(\n        F.col(\"ts.CustomerID\"),\n        F.col(\"ts.TotalOrders\"),\n        F.col(\"ts.TotalSpent\"),\n        F.col(\"ts.AvgOrderValue\"),\n        F.col(\"ts.FirstPurchaseDate\"),\n        F.col(\"ts.LastPurchaseDate\"),\n        F.col(\"ts.DaysSinceLastPurchase\"),\n        F.col(\"cat.Category\").alias(\"TopCategory\"),\n        F.col(\"prod.ProductName\").alias(\"TopProduct\"),\n        # ReturnRate = (TotalReturns / TotalOrders) * 100\n        F.when(F.col(\"ts.TotalOrders\") > 0,\n               (F.col(\"ret.TotalReturns\").cast(\"double\") / F.col(\"ts.TotalOrders\")) * 100\n              ).otherwise(F.lit(0.0)).alias(\"ReturnRate\")\n    )\n)\n\n# ---- Step 3: Calculate Customer Metrics (RFM, churn probability, etc.) ----\n# 3a) RFM scoring approach\n# Recency higher is worse => we invert the order for \"score\" (NTILE(5) => 5 is most recent).\n# We approximate by creating separate windows and ranking approach.\n\nwindowRecency = Window.orderBy(F.col(\"DaysSinceLastPurchase\").asc())\ntransactionSummaryDF = transactionSummaryDF.withColumn(\n    \"RecencyRank\",\n    F.ntile(5).over(windowRecency)\n)\n\nwindowFrequency = Window.orderBy(F.col(\"TotalOrders\").desc())\ntransactionSummaryDF = transactionSummaryDF.withColumn(\n    \"FrequencyRank\",\n    F.ntile(5).over(windowFrequency)\n)\n\nwindowMonetary = Window.orderBy(F.col(\"TotalSpent\").desc())\ntransactionSummaryDF = transactionSummaryDF.withColumn(\n    \"MonetaryRank\",\n    F.ntile(5).over(windowMonetary)\n)\n\nrfmDF = transactionSummaryDF.select(\n    \"CustomerID\",\n    F.col(\"RecencyRank\").alias(\"RecencyScore\"),\n    F.col(\"FrequencyRank\").alias(\"FrequencyScore\"),\n    F.col(\"MonetaryRank\").alias(\"MonetaryScore\"),\n).withColumn(\n    \"RFMScore\",\n    F.col(\"RecencyScore\") + F.col(\"FrequencyScore\") + F.col(\"MonetaryScore\")\n)\n\n# 3b) Calculate churn probability, next purchase propensity, loyalty index\nchurnDF = transactionSummaryDF.select(\n    \"CustomerID\",\n    # Churn Probability\n    F.when(F.col(\"DaysSinceLastPurchase\") > churnDaysVeryHigh, 0.8)\n     .when(F.col(\"DaysSinceLastPurchase\") > churnDaysHigh, 0.5)\n     .when(F.col(\"DaysSinceLastPurchase\") > churnDaysMedium, 0.3)\n     .when(F.col(\"DaysSinceLastPurchase\") > churnDaysLow, 0.1)\n     .otherwise(0.05)\n     .alias(\"ChurnProbability\"),\n    # NextPurchasePropensity\n    F.when((F.col(\"DaysSinceLastPurchase\") < churnDaysLow) & (F.col(\"TotalOrders\") > 3), 0.8)\n     .when((F.col(\"DaysSinceLastPurchase\") < churnDaysMedium) & (F.col(\"TotalOrders\") > 2), 0.6)\n     .when(F.col(\"DaysSinceLastPurchase\") < churnDaysHigh, 0.4)\n     .otherwise(0.2)\n     .alias(\"NextPurchasePropensity\"),\n    # LoyaltyIndex simplified\n    (\n        F.when((F.col(\"TotalOrders\") >= loyaltyThreshold) & (F.col(\"ReturnRate\") < 10),\n               (F.col(\"TotalOrders\")*0.6) + ((100 - F.col(\"ReturnRate\"))*0.4)\n              )\n        .otherwise(F.col(\"TotalOrders\")*0.4)\n    / 10\n    ).alias(\"LoyaltyIndex\")\n)\n\n# 3c) Join to get final CustomerMetrics\ncustomerMetricsDF = (\n    rfmDF.alias(\"r\")\n    .join(transactionSummaryDF.alias(\"t\"), \"CustomerID\")\n    .join(churnDF.alias(\"ch\"), \"CustomerID\")\n    .select(\n        F.col(\"r.CustomerID\"),\n        # CustomerLifetimeValue: approximate => TotalSpent * (1 + (1 - ChurnProbability))\n        (F.col(\"t.TotalSpent\") * (1 + (1 - F.col(\"ch.ChurnProbability\")))).alias(\"CustomerLifetimeValue\"),\n        \"r.RecencyScore\",\n        \"r.FrequencyScore\",\n        \"r.MonetaryScore\",\n        \"r.RFMScore\",\n        F.col(\"ch.ChurnProbability\"),\n        F.col(\"ch.NextPurchasePropensity\"),\n        F.col(\"ch.LoyaltyIndex\"),\n        # CustomerHealth\n        F.when(F.col(\"ch.ChurnProbability\") > 0.7, \"At Risk\")\n         .when(F.col(\"ch.LoyaltyIndex\") > 0.8, \"Excellent\")\n         .when(F.col(\"ch.LoyaltyIndex\") > 0.6, \"Good\")\n         .when(F.col(\"ch.LoyaltyIndex\") > 0.4, \"Average\")\n         .otherwise(\"Needs Attention\")\n         .alias(\"CustomerHealth\")\n    )\n)\n\n# ---- Step 4: Customer Segmentation (like #CustomerSegments) ----\nsegmentationDF = (\n    customerBaseDF.alias(\"cb\")\n    .join(customerMetricsDF.alias(\"cm\"), \"CustomerID\")\n    .join(transactionSummaryDF.alias(\"ts\"), \"CustomerID\")\n    .select(\n        F.col(\"cb.CustomerID\"),\n        # ValueSegment\n        F.when(F.col(\"cm.CustomerLifetimeValue\") >= highValueThreshold, \"High Value\")\n         .when(F.col(\"cm.CustomerLifetimeValue\") >= highValueThreshold*0.5, \"Medium Value\")\n         .otherwise(\"Low Value\")\n         .alias(\"ValueSegment\"),\n        # BehaviorSegment (RFM-based logic, approximate from SP)\n        F.when(F.col(\"cm.RFMScore\") >= 13, \"Champions\")\n         .when((F.col(\"cm.RecencyScore\") >= 4) & (F.col(\"cm.FrequencyScore\") >= 3), \"Loyal Customers\")\n         .when((F.col(\"cm.RecencyScore\") >= 4) & (F.col(\"cm.FrequencyScore\") <= 2), \"Potential Loyalists\")\n         .when((F.col(\"cm.RecencyScore\") <= 2) & (F.col(\"cm.FrequencyScore\") >= 3) & (F.col(\"cm.MonetaryScore\") >= 3), \"At Risk\")\n         .when((F.col(\"cm.RecencyScore\") <= 2) & (F.col(\"cm.FrequencyScore\") <= 2) & (F.col(\"cm.MonetaryScore\") <= 2), \"Hibernating\")\n         .when(F.col(\"cm.RecencyScore\") <= 1, \"Lost\")\n         .otherwise(\"Others\")\n         .alias(\"BehaviorSegment\"),\n        # LifecycleSegment\n        F.when((F.col(\"ts.TotalOrders\") == 1) & (F.col(\"ts.DaysSinceLastPurchase\") <= 30), \"New Customer\")\n         .when((F.col(\"ts.TotalOrders\") > 1) & (F.col(\"cm.ChurnProbability\") < 0.3), \"Active\")\n         .when((F.col(\"cm.ChurnProbability\") >= 0.3) & (F.col(\"cm.ChurnProbability\") < 0.7), \"At Risk\")\n         .when(F.col(\"cm.ChurnProbability\") >= 0.7, \"Churned\")\n         .otherwise(\"Inactive\")\n         .alias(\"LifecycleSegment\"),\n        # TargetGroup\n        F.when((F.col(\"cm.CustomerLifetimeValue\") >= highValueThreshold) & (F.col(\"cm.ChurnProbability\") < 0.3), \"VIP\")\n         .when((F.col(\"cm.CustomerLifetimeValue\") >= highValueThreshold*0.5) & (F.col(\"cm.ChurnProbability\") >= 0.3), \"Retention Priority\")\n         .when((F.col(\"ts.TotalOrders\") == 1) & (F.col(\"ts.DaysSinceLastPurchase\") <= 30), \"Nurture New\")\n         .when(F.col(\"cm.NextPurchasePropensity\") > 0.6, \"Growth Opportunity\")\n         .when(F.col(\"ts.ReturnRate\") > 20, \"Service Improvement\")\n         .otherwise(\"Standard\")\n         .alias(\"TargetGroup\"),\n        # MarketingRecommendation\n        F.when((F.col(\"cm.CustomerLifetimeValue\") >= highValueThreshold) & (F.col(\"cm.ChurnProbability\") < 0.3),\n               \"Exclusive offers, VIP events, Personal shopping assistance, Early access to new products\")\n         .when((F.col(\"cm.CustomerLifetimeValue\") >= highValueThreshold*0.5) & (F.col(\"cm.ChurnProbability\") >= 0.3),\n               \"Re-engagement campaign, Loyalty rewards, Personalized recommendations based on past purchases\")\n         .when((F.col(\"ts.TotalOrders\") == 1) & (F.col(\"ts.DaysSinceLastPurchase\") <= 30),\n               \"Welcome series, Educational content, First purchase follow-up, Category exploration\")\n         .when(F.col(\"cm.NextPurchasePropensity\") > 0.6,\n               F.concat(F.lit(\"Cross-sell/upsell, Bundle offers based on \"),\n                        F.coalesce(F.col(\"ts.TopCategory\"), F.lit(\"preferred category\")),\n                        F.lit(\", Category expansion\")))\n         .when(F.col(\"ts.ReturnRate\") > 20,\n               \"Satisfaction survey, Improved product information, Size/fit guides, Service recovery\")\n         .otherwise(\n               F.concat(F.lit(\"Standard seasonal promotions, Category newsletters, Reactivation after \"),\n                        (F.col(\"ts.DaysSinceLastPurchase\")/30).cast(\"int\"),\n                        F.lit(\" months\"))\n         )\n         .alias(\"MarketingRecommendation\")\n    )\n)\n\n# Write the Silver tables (e.g., Delta format, or to Lakehouse tables)\ncustomerBaseDF.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"lakehouse_silver.CustomerBase\")\ntransactionSummaryDF.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"lakehouse_silver.TransactionSummary\")\ncustomerMetricsDF.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"lakehouse_silver.CustomerMetrics\")\nsegmentationDF.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"lakehouse_silver.CustomerSegments\")\n\n--------------------------------------------------------------------------------\n-- GOLD LAYER (Upserts into Analytics & Summary Tables)\n--------------------------------------------------------------------------------\n# We emulate the final merges/inserts from the stored procedure into dimensional/analytic tables.\n# Example: merging CustomerProfiles, CustomerAnalytics, and a summary table.\n\n# 1) CustomerProfiles upsert\ncustomerProfileGoldDF = (\n    customerBaseDF.alias(\"cb\")\n    .join(transactionSummaryDF.alias(\"ts\"), \"CustomerID\", \"left\")\n    .select(\n        \"cb.CustomerID\",\n        \"cb.CustomerName\",\n        \"cb.Email\",\n        \"cb.Phone\",\n        \"cb.Address\",\n        \"cb.PostalCode\",\n        \"cb.City\",\n        \"cb.State\",\n        \"cb.Country\",\n        \"cb.CustomerType\",\n        \"cb.AccountManager\",\n        \"cb.CreatedDate\",\n        \"cb.IsActive\",\n        \"cb.LastContactDate\",\n        \"ts.TotalOrders\",\n        \"ts.TotalSpent\",\n        \"ts.AvgOrderValue\",\n        \"ts.FirstPurchaseDate\",\n        \"ts.LastPurchaseDate\",\n        \"ts.DaysSinceLastPurchase\",\n        \"ts.TopCategory\",\n        \"ts.TopProduct\",\n        \"ts.ReturnRate\",\n        F.current_timestamp().alias(\"ModifiedDate\")\n    )\n)\n\n# Example approach: perform a Delta merge into an existing Gold table \n# For demonstration, we show an overwrite. In production, use MERGE for upsert logic.\n\n(\n    customerProfileGoldDF\n    .write\n    .format(\"delta\")\n    .mode(\"overwrite\")\n    .option(\"overwriteSchema\", \"true\")\n    .saveAsTable(\"lakehouse_gold.CustomerProfiles\")\n)\n\n# 2) CustomerAnalytics upsert\ncustomerAnalyticsGoldDF = (\n    customerMetricsDF.alias(\"cm\")\n    .join(segmentationDF.alias(\"cs\"), \"CustomerID\")\n    .select(\n        F.lit(dataDate).alias(\"ProcessDate\"),\n        \"cm.CustomerID\",\n        \"cm.CustomerLifetimeValue\",\n        \"cm.RecencyScore\",\n        \"cm.FrequencyScore\",\n        \"cm.MonetaryScore\",\n        \"cm.RFMScore\",\n        \"cm.ChurnProbability\",\n        \"cm.NextPurchasePropensity\",\n        \"cm.LoyaltyIndex\",\n        \"cm.CustomerHealth\",\n        \"cs.ValueSegment\",\n        \"cs.BehaviorSegment\",\n        \"cs.LifecycleSegment\",\n        \"cs.TargetGroup\",\n        \"cs.MarketingRecommendation\"\n    )\n)\n\n(\n    customerAnalyticsGoldDF\n    .write\n    .format(\"delta\")\n    .mode(\"append\")\n    .saveAsTable(\"lakehouse_gold.CustomerAnalytics\")\n)\n\n# (Optional) Retention: remove older data\nspark.sql(f\"\"\"\n    DELETE FROM lakehouse_gold.CustomerAnalytics\n    WHERE ProcessDate < DATEADD(DAY, -{retentionPeriodDays}, '{str(dataDate)}')\n\"\"\")\n\n# 3) Generate summary\nsummaryDF = (\n    customerBaseDF.alias(\"cb\")\n    .join(customerMetricsDF.alias(\"cm\"), \"CustomerID\")\n    .join(segmentationDF.alias(\"cs\"), \"CustomerID\")\n    .select(\n        F.lit(dataDate).alias(\"ProcessDate\"),\n        F.lit(processType).alias(\"ProcessType\"),\n        F.count_distinct(\"cb.CustomerID\").over(Window.partitionBy()),\n        F.sum(\n            F.when(F.col(\"cs.LifecycleSegment\") == \"Active\", 1).otherwise(0)\n        ).over(Window.partitionBy()),\n        F.sum(\n            F.when(F.col(\"cs.LifecycleSegment\") == \"New Customer\", 1).otherwise(0)\n        ).over(Window.partitionBy()),\n        F.sum(\n            F.when((F.col(\"cs.LifecycleSegment\") == \"At Risk\") | (F.col(\"cs.LifecycleSegment\") == \"Churned\"), 1).otherwise(0)\n        ).over(Window.partitionBy()),\n        F.sum(\n            F.when(F.col(\"cs.ValueSegment\") == \"High Value\", 1).otherwise(0)\n        ).over(Window.partitionBy()),\n        F.avg(\"cm.CustomerLifetimeValue\").over(Window.partitionBy()),\n        (F.unix_timestamp(F.current_timestamp()) - F.unix_timestamp(F.lit(str(dataDate))))*1000,  # Approx processing time\n        F.lit(\"Success\")\n    ).distinct()  # Distinct so we only get one row\n\nsummaryRenamedDF = summaryDF.selectExpr(\n    \"ProcessDate\",\n    \"ProcessType\",\n    \"`count_distinct(cb.CustomerID)` as TotalCustomers\",\n    \"`sum(CASE WHEN (cs.LifecycleSegment = Active) THEN 1 END)` as ActiveCustomers\",\n    \"`sum(CASE WHEN (cs.LifecycleSegment = New Customer) THEN 1 END)` as NewCustomers\",\n    \"`sum(CASE WHEN (At Risk or Churned) THEN 1 END)` as ChurningCustomers\",\n    \"`sum(CASE WHEN (ValueSegment = High Value) THEN 1 END)` as HighValueCustomers\",\n    \"`avg(cm.CustomerLifetimeValue)` as AverageLifetimeValue\",\n    \"((unix_timestamp(current_timestamp()) - unix_timestamp(lit(str(dataDate))))*1000) as TotalProcessingTimeMs\",\n    \"'Success' as ProcessingStatus\"\n)\n\n(\n    summaryRenamedDF\n    .write\n    .format(\"delta\")\n    .mode(\"append\")\n    .saveAsTable(\"lakehouse_gold.CustomerAnalyticsSummary\")\n)\n\n# If debugMode is True, you may display intermediate results or logs as needed:\nif debugMode:\n    print(\"Sample summary results:\")\n    summaryRenamedDF.show()\n\n# End of PySpark job\n\n────────────────────────────────────────────────────────────────────────\n3) IMPLEMENTATION GUIDELINES AND BEST PRACTICES (TechLead)\n────────────────────────────────────────────────────────────────────────\n\n• Use Parameter Tables or Config Files:  \n  - Instead of hardcoding parameters in the code, store them in config tables or key-value stores. This ensures consistent parameter management across environments.\n\n• Apply Incremental Loads Where Possible:  \n  - The stored procedure logic partly relies on “ModifiedDate >= DataDate.” Use Delta’s MERGE or incremental capture to optimize the load rather than scanning the entire dataset every day.\n\n• Optimize with Delta Lake Format:  \n  - Leverage Delta Lake features for ACID transactions, upserts, and time travel, ensuring data consistency.  \n  - Create partitioning strategies if you have large datasets (e.g., partition by date or region).\n\n• Handle Data Quality:  \n  - Validate raw data in Bronze, e.g. filtering out records with null keys.  \n  - Use Silver layer transformations to correct/clean data before writing to Gold.\n\n• Error Handling:  \n  - In PySpark, wrap transformations in try/except blocks or rely on pipeline orchestration logs to handle and track errors.  \n  - Use a separate log table or file for errors, akin to #ProcessLog in SQL.\n\n• Production Considerations:  \n  - Schedule the job in Microsoft Fabric pipelines (Data Factory or Synapse pipeline tasks) to run daily.  \n  - Monitor run times, row counts, and summary outcomes for anomalies.\n\n────────────────────────────────────────────────────────────────────────\n4) TEST CASES FOR VALIDATION (TestingAgent)\n────────────────────────────────────────────────────────────────────────\n\nBelow are some recommended test scenarios to ensure the migration is correct:\n\n1. Basic Full Load Test  \n   - Input: Use a known data set with a mix of new, active, churned, high-value, and low-value customers.  \n   - Expected Outcome: The final Gold tables (CustomerAnalytics, CustomerProfiles, and CustomerAnalyticsSummary) accurately reflect RFM scores, churn predictions, segmentations, and summary counts.\n\n2. Incremental Load Test (ProcessType = ‘INCREMENTAL’)  \n   - Input: Add new customers/transactions with a date greater than yesterday.  \n   - Expected Outcome: Only those new and changed records appear in the silver/gold layers. Confirm no duplicates in the final tables.\n\n3. Parameter Variation Test  \n   - Change thresholds (e.g., churnDaysVeryHigh = 200, highValueThreshold = 10000).  \n   - Expected Outcome: Churn probabilities, ValueSegments, etc., adjust accordingly.\n\n4. Edge Cases (No Orders or Interactions)  \n   - Input: Include customers who have never placed an order or had an interaction.  \n   - Expected Outcome: Their transaction summaries and RFM scores remain zero or null; they still appear in the final table with appropriate segmentation (e.g., “Low Value”, “Hibernating” or “Lost”).\n\n5. Retention Enforcement  \n   - Input: Insert data older than the retention period, run the pipeline, then check if old data is deleted from CustomerAnalytics.  \n   - Expected Outcome: Records older than 365 days (default) are removed from the gold tables.\n\n6. Error Handling Test  \n   - Introduce a data integrity issue (e.g., invalid ID references, null keys) or simulate a Spark failure.  \n   - Expected Outcome: The process logs an error, fails gracefully, or quarantines bad data as per the design.\n\n────────────────────────────────────────────────────────────────────────\n\nThis plan and sample code illustrate how to migrate the existing SQL Stored Procedure’s business logic into a PySpark-based pipeline under Microsoft Fabric’s Medallion architecture. The above steps consolidate raw data in Bronze, curate transformations in Silver, and produce refined analytics in Gold. By aligning the business logic with robust data engineering practices (Delta merges, incremental loads, and time-partitioned data), this new solution will be maintainable, scalable, and more easily integrated with modern BI/ML workflows within Microsoft Fabric.",
    "role": "user",
    "name": "BusinessAnalyst"
  },
  {
    "content": "Below is a comprehensive plan and example implementation for migrating the given SQL stored procedure to a PySpark solution within Microsoft Fabric, following a Medallion (Bronze–Silver–Gold) architecture. The answer is organized by the different roles and requested structure:\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n1) BUSINESS ANALYSIS AND MIGRATION PLAN (BusinessAnalyst & ProductOwner)\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\nA. Business Purpose\n• The original SQL stored procedure usp_CustomerAnalytics_Processing delivers insights about customer behavior, lifetime value, churn risk, and segmentation.\n• It primarily supports marketing, sales, and customer success teams by producing daily (“ProcessDate”-based) analytics and summary tables.\n• The logic produces RFM metrics, churn probability, loyalty index, and marketing recommendations, then stores data in CustomerProfiles, CustomerAnalytics, and a summary table to track daily metrics.\n\nB. Requirements & Logic Highlights\n• RFM (Recency, Frequency, Monetary) scoring to gauge engagement.  \n• Churn probability thresholds based on DaysSinceLastPurchase relative to configuration (e.g., 30, 60, 90, 180 days).\n• Segmentation (ValueSegment, BehaviorSegment, LifecycleSegment, TargetGroup, MarketingRecommendation).\n• Summaries (active vs. churning customers, average LTV, etc.).\n\nC. Data Sources\n• Customers, CustomerAddresses, CustomerInteractions (for basic customer info).\n• Orders, OrderDetails, Products, Returns (for transaction and returns data).\n• Target tables: CustomerProfiles, CustomerAnalytics, CustomerAnalyticsSummary.\n\nD. Migration Approach\n• Leverage Microsoft Fabric Medallion architecture:\n  1) Bronze: Raw ingestion of source tables (Customers, Orders, etc.).  \n  2) Silver: Transformations (joins, aggregations, RFM calculation, churn prediction, segmentation).  \n  3) Gold: Final analytics tables for consumption (CustomerProfiles, CustomerAnalytics, daily summary).  \n\n• Use Delta tables (Fabric Lakehouse) for ACID merges, incremental updates, and standard PySpark transformations.\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n2) P Y S P A R K   C O D E  (AzureDataEngineer)\n   BRONZE → SILVER → GOLD\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\nBelow is a representative PySpark notebook/script. Adapt table paths, database/table names, and parameter storage as needed for your environment.\n\n────────────────────────────────────────────────────────────────\n-- SETUP & CONFIGURATION\n────────────────────────────────────────────────────────────────\nfrom pyspark.sql import SparkSession, functions as F, Window\nfrom pyspark.sql.types import DecimalType, IntegerType, StringType, DateType\nimport datetime\n\nspark = SparkSession.builder.getOrCreate()\n\n# Parameters (mirroring the SQL procedure defaults)\ndataDate = None  # If not provided, default to yesterday\nprocessType = 'FULL'\ndebugMode = False\nretentionPeriodDays = 365\nhighValueThreshold = 5000.0\nloyaltyThreshold = 5\nchurnDaysVeryHigh = 180\nchurnDaysHigh = 90\nchurnDaysMedium = 60\nchurnDaysLow = 30\n\n# Default the data date if None:\nif dataDate is None:\n    dataDate = datetime.date.today() - datetime.timedelta(days=1)\n\n# Read from your Bronze tables in the Fabric Lakehouse or Warehouse:\n# Example: spark.read.table(\"db.bronze.XXXX\")\ncustomersBronzeDF = spark.read.table(\"lakehouse_raw.Customers\")\naddressesBronzeDF = spark.read.table(\"lakehouse_raw.CustomerAddresses\")\ninteractionsBronzeDF = spark.read.table(\"lakehouse_raw.CustomerInteractions\")\nordersBronzeDF = spark.read.table(\"lakehouse_raw.Orders\")\norderDetailsBronzeDF = spark.read.table(\"lakehouse_raw.OrderDetails\")\nproductsBronzeDF = spark.read.table(\"lakehouse_raw.Products\")\nreturnsBronzeDF = spark.read.table(\"lakehouse_raw.Returns\")\n\n────────────────────────────────────────────────────────────────\n-- SILVER LAYER TRANSFORMATIONS\n────────────────────────────────────────────────────────────────\n\n# ----------------------------------------------------------------------------\n# Step 1: Create Customer Base (similar to #CustomerBase in SQL)\n# ----------------------------------------------------------------------------\ncustomerBaseDF = (\n    customersBronzeDF.alias(\"c\")\n    .join(\n        addressesBronzeDF.alias(\"a\"),\n        (F.col(\"c.CustomerID\") == F.col(\"a.CustomerID\")) & (F.col(\"a.AddressType\") == \"Primary\"),\n        \"left\"\n    )\n    .join(\n        interactionsBronzeDF.alias(\"i\"),\n        F.col(\"c.CustomerID\") == F.col(\"i.CustomerID\"),\n        \"left\"\n    )\n    .where(\n        (processType == 'FULL')\n        | (F.col(\"c.ModifiedDate\") >= F.lit(dataDate))\n        | (F.col(\"i.ContactDate\") >= F.lit(dataDate))\n    )\n    .groupBy(\n        \"c.CustomerID\",\n        \"c.FirstName\",\n        \"c.LastName\",\n        \"c.Email\",\n        \"c.Phone\",\n        \"a.StreetAddress\",\n        \"a.PostalCode\",\n        \"a.City\",\n        \"a.State\",\n        \"a.Country\",\n        \"c.CustomerType\",\n        \"c.AccountManagerID\",\n        \"c.CreatedDate\",\n        \"c.ModifiedDate\",\n        \"c.Status\"\n    )\n    .agg(F.max(\"i.ContactDate\").alias(\"LastContactDate\"))\n    .select(\n        F.col(\"CustomerID\"),\n        F.concat_ws(\" \", F.col(\"FirstName\"), F.col(\"LastName\")).alias(\"CustomerName\"),\n        \"Email\",\n        \"Phone\",\n        F.col(\"StreetAddress\").alias(\"Address\"),\n        \"PostalCode\",\n        \"City\",\n        \"State\",\n        \"Country\",\n        \"CustomerType\",\n        F.col(\"AccountManagerID\").alias(\"AccountManager\"),\n        \"CreatedDate\",\n        \"ModifiedDate\",\n        (F.col(\"Status\") == \"Active\").cast(\"boolean\").alias(\"IsActive\"),\n        \"LastContactDate\"\n    )\n)\n\n# ----------------------------------------------------------------------------\n# Step 2: Process Transaction Summary (#TransactionSummary)\n# ----------------------------------------------------------------------------\n# Calculate Order Metrics first\norderSummaryDF = (\n    ordersBronzeDF.alias(\"o\")\n    .join(orderDetailsBronzeDF.alias(\"od\"), \"OrderID\")\n    .where(F.col(\"o.Status\") == \"Completed\")\n    .groupBy(\"o.CustomerID\")\n    .agg(\n        F.countDistinct(\"o.OrderID\").alias(\"TotalOrders\"),\n        F.sum(F.col(\"od.Quantity\") * F.col(\"od.UnitPrice\") * (1 - F.col(\"od.Discount\"))).alias(\"TotalSpent\"),\n        F.min(\"o.OrderDate\").alias(\"FirstPurchaseDate\"),\n        F.max(\"o.OrderDate\").alias(\"LastPurchaseDate\")\n    )\n)\n\ntransactionSummaryDF = (\n    orderSummaryDF.alias(\"os\")\n    .join(\n        customerBaseDF.select(\"CustomerID\").alias(\"cb\"),\n        F.col(\"os.CustomerID\") == F.col(\"cb.CustomerID\"),\n        \"inner\"\n    )\n    .select(\n        \"os.CustomerID\",\n        \"os.TotalOrders\",\n        \"os.TotalSpent\",\n        F.when(F.col(\"os.TotalOrders\") > 0, F.col(\"os.TotalSpent\") / F.col(\"os.TotalOrders\"))\n         .otherwise(F.lit(0.0)).alias(\"AvgOrderValue\"),\n        \"os.FirstPurchaseDate\",\n        \"os.LastPurchaseDate\",\n        F.datediff(F.lit(dataDate), F.col(\"os.LastPurchaseDate\")).alias(\"DaysSinceLastPurchase\")\n    )\n)\n\n# Identify top category per customer\ncategorySpendDF = (\n    ordersBronzeDF.alias(\"o\")\n    .join(orderDetailsBronzeDF.alias(\"od\"), \"OrderID\")\n    .join(productsBronzeDF.alias(\"p\"), \"ProductID\")\n    .where(F.col(\"o.Status\") == \"Completed\")\n    .groupBy(\"o.CustomerID\", \"p.Category\")\n    .agg(F.sum(F.col(\"od.Quantity\") * F.col(\"od.UnitPrice\") * (1 - F.col(\"od.Discount\"))).alias(\"CategorySpend\"))\n)\n\nwindowCategory = Window.partitionBy(\"CustomerID\").orderBy(F.col(\"CategorySpend\").desc())\ncategoryRankedDF = categorySpendDF.withColumn(\n    \"CategoryRank\", F.row_number().over(windowCategory)\n).filter(F.col(\"CategoryRank\") == 1)\n\n# Identify top product per customer\nproductQtyDF = (\n    ordersBronzeDF.alias(\"o\")\n    .join(orderDetailsBronzeDF.alias(\"od\"), \"OrderID\")\n    .join(productsBronzeDF.alias(\"p\"), \"ProductID\")\n    .where(F.col(\"o.Status\") == \"Completed\")\n    .groupBy(\"o.CustomerID\", \"p.ProductName\")\n    .agg(F.sum(\"od.Quantity\").alias(\"TotalQuantity\"))\n)\n\nwindowProduct = Window.partitionBy(\"CustomerID\").orderBy(F.col(\"TotalQuantity\").desc())\nproductRankedDF = productQtyDF.withColumn(\n    \"ProductRank\", F.row_number().over(windowProduct)\n).filter(F.col(\"ProductRank\") == 1)\n\n# Return rate (count of returns / total orders)\nreturnsGroupedDF = (\n    returnsBronzeDF.groupBy(\"CustomerID\")\n    .agg(F.countDistinct(\"ReturnID\").alias(\"TotalReturns\"))\n)\n\n# Combine all partial metrics into final transaction summary\ntransactionSummaryDF = (\n    transactionSummaryDF.alias(\"ts\")\n    .join(categoryRankedDF.alias(\"cat\"), \"CustomerID\", \"left\")\n    .join(productRankedDF.alias(\"prod\"), \"CustomerID\", \"left\")\n    .join(returnsGroupedDF.alias(\"ret\"), \"CustomerID\", \"left\")\n    .select(\n        F.col(\"ts.CustomerID\"),\n        \"ts.TotalOrders\",\n        \"ts.TotalSpent\",\n        \"ts.AvgOrderValue\",\n        \"ts.FirstPurchaseDate\",\n        \"ts.LastPurchaseDate\",\n        \"ts.DaysSinceLastPurchase\",\n        F.col(\"cat.Category\").alias(\"TopCategory\"),\n        F.col(\"prod.ProductName\").alias(\"TopProduct\"),\n        F.when(F.col(\"ts.TotalOrders\") > 0,\n               (F.col(\"ret.TotalReturns\") / F.col(\"ts.TotalOrders\")) * 100\n              ).otherwise(F.lit(0.0)).alias(\"ReturnRate\")\n    )\n)\n\n# ----------------------------------------------------------------------------\n# Step 3: Calculate Customer Metrics (#CustomerMetrics)\n# ----------------------------------------------------------------------------\n# ---- RFM Scoring (enforce consistent window definitions) ----\nwindowRecency = Window.orderBy(F.col(\"DaysSinceLastPurchase\").asc())\nwindowFrequency = Window.orderBy(F.col(\"TotalOrders\").desc())\nwindowMonetary = Window.orderBy(F.col(\"TotalSpent\").desc())\n\ntransactionSummaryDF = (\n    transactionSummaryDF\n    .withColumn(\"RecencyScore\", F.ntile(5).over(windowRecency))\n    .withColumn(\"FrequencyScore\", F.ntile(5).over(windowFrequency))\n    .withColumn(\"MonetaryScore\", F.ntile(5).over(windowMonetary))\n)\n\nrfmDF = transactionSummaryDF.select(\n    \"CustomerID\",\n    \"RecencyScore\",\n    \"FrequencyScore\",\n    \"MonetaryScore\",\n    (F.col(\"RecencyScore\") + F.col(\"FrequencyScore\") + F.col(\"MonetaryScore\")).alias(\"RFMScore\")\n)\n\n# ---- Churn Probability, Next Purchase Propensity, Loyalty Index ----\nchurnDF = (\n    transactionSummaryDF.select(\n        \"CustomerID\",\n        F.when(F.col(\"DaysSinceLastPurchase\") > churnDaysVeryHigh, 0.8)\n         .when(F.col(\"DaysSinceLastPurchase\") > churnDaysHigh, 0.5)\n         .when(F.col(\"DaysSinceLastPurchase\") > churnDaysMedium, 0.3)\n         .when(F.col(\"DaysSinceLastPurchase\") > churnDaysLow, 0.1)\n         .otherwise(0.05)\n         .alias(\"ChurnProbability\"),\n        F.when((F.col(\"DaysSinceLastPurchase\") < churnDaysLow) & (F.col(\"TotalOrders\") > 3), 0.8)\n         .when((F.col(\"DaysSinceLastPurchase\") < churnDaysMedium) & (F.col(\"TotalOrders\") > 2), 0.6)\n         .when(F.col(\"DaysSinceLastPurchase\") < churnDaysHigh, 0.4)\n         .otherwise(0.2)\n         .alias(\"NextPurchasePropensity\"),\n        (\n            F.when(\n                (F.col(\"TotalOrders\") >= loyaltyThreshold) & (F.col(\"ReturnRate\") < 10),\n                (F.col(\"TotalOrders\") * 0.6) + ((100 - F.col(\"ReturnRate\")) * 0.4)\n            )\n            .otherwise(F.col(\"TotalOrders\") * 0.4)\n            / 10\n        ).alias(\"LoyaltyIndex\")\n    )\n)\n\ncustomerMetricsDF = (\n    rfmDF.alias(\"r\")\n    .join(transactionSummaryDF.alias(\"t\"), \"CustomerID\")\n    .join(churnDF.alias(\"ch\"), \"CustomerID\")\n    .select(\n        F.col(\"r.CustomerID\"),\n        (F.col(\"t.TotalSpent\") * (1 + (1 - F.col(\"ch.ChurnProbability\")))).alias(\"CustomerLifetimeValue\"),\n        \"r.RecencyScore\",\n        \"r.FrequencyScore\",\n        \"r.MonetaryScore\",\n        \"r.RFMScore\",\n        \"ch.ChurnProbability\",\n        \"ch.NextPurchasePropensity\",\n        \"ch.LoyaltyIndex\",\n        F.when(F.col(\"ch.ChurnProbability\") > 0.7, \"At Risk\")\n         .when(F.col(\"ch.LoyaltyIndex\") > 0.8, \"Excellent\")\n         .when(F.col(\"ch.LoyaltyIndex\") > 0.6, \"Good\")\n         .when(F.col(\"ch.LoyaltyIndex\") > 0.4, \"Average\")\n         .otherwise(\"Needs Attention\")\n         .alias(\"CustomerHealth\")\n    )\n)\n\n# ----------------------------------------------------------------------------\n# Step 4: Customer Segmentation (#CustomerSegments)\n# ----------------------------------------------------------------------------\nsegmentationDF = (\n    customerBaseDF.alias(\"cb\")\n    .join(customerMetricsDF.alias(\"cm\"), \"CustomerID\")\n    .join(transactionSummaryDF.alias(\"ts\"), \"CustomerID\")\n    .select(\n        F.col(\"cb.CustomerID\"),\n        F.when(F.col(\"cm.CustomerLifetimeValue\") >= highValueThreshold, \"High Value\")\n         .when(F.col(\"cm.CustomerLifetimeValue\") >= highValueThreshold * 0.5, \"Medium Value\")\n         .otherwise(\"Low Value\").alias(\"ValueSegment\"),\n        F.when(F.col(\"cm.RFMScore\") >= 13, \"Champions\")\n         .when((F.col(\"cm.RecencyScore\") >= 4) & (F.col(\"cm.FrequencyScore\") >= 3), \"Loyal Customers\")\n         .when((F.col(\"cm.RecencyScore\") >= 4) & (F.col(\"cm.FrequencyScore\") <= 2), \"Potential Loyalists\")\n         .when((F.col(\"cm.RecencyScore\") <= 2) & (F.col(\"cm.FrequencyScore\") >= 3) & (F.col(\"cm.MonetaryScore\") >= 3), \"At Risk\")\n         .when((F.col(\"cm.RecencyScore\") <= 2) & (F.col(\"cm.FrequencyScore\") <= 2) & (F.col(\"cm.MonetaryScore\") <= 2), \"Hibernating\")\n         .when(F.col(\"cm.RecencyScore\") <= 1, \"Lost\")\n         .otherwise(\"Others\")\n         .alias(\"BehaviorSegment\"),\n        F.when((F.col(\"ts.TotalOrders\") == 1) & (F.col(\"ts.DaysSinceLastPurchase\") <= 30), \"New Customer\")\n         .when((F.col(\"ts.TotalOrders\") > 1) & (F.col(\"cm.ChurnProbability\") < 0.3), \"Active\")\n         .when((F.col(\"cm.ChurnProbability\") >= 0.3) & (F.col(\"cm.ChurnProbability\") < 0.7), \"At Risk\")\n         .when(F.col(\"cm.ChurnProbability\") >= 0.7, \"Churned\")\n         .otherwise(\"Inactive\")\n         .alias(\"LifecycleSegment\"),\n        F.when((F.col(\"cm.CustomerLifetimeValue\") >= highValueThreshold) & (F.col(\"cm.ChurnProbability\") < 0.3), \"VIP\")\n         .when((F.col(\"cm.CustomerLifetimeValue\") >= highValueThreshold * 0.5) & (F.col(\"cm.ChurnProbability\") >= 0.3), \"Retention Priority\")\n         .when((F.col(\"ts.TotalOrders\") == 1) & (F.col(\"ts.DaysSinceLastPurchase\") <= 30), \"Nurture New\")\n         .when(F.col(\"cm.NextPurchasePropensity\") > 0.6, \"Growth Opportunity\")\n         .when(F.col(\"ts.ReturnRate\") > 20, \"Service Improvement\")\n         .otherwise(\"Standard\")\n         .alias(\"TargetGroup\"),\n        F.when((F.col(\"cm.CustomerLifetimeValue\") >= highValueThreshold) & (F.col(\"cm.ChurnProbability\") < 0.3),\n               \"Exclusive offers, VIP events, Personal shopping assistance, Early access to new products\")\n         .when((F.col(\"cm.CustomerLifetimeValue\") >= highValueThreshold * 0.5) & (F.col(\"cm.ChurnProbability\") >= 0.3),\n               \"Re-engagement campaign, Loyalty rewards, Personalized recommendations based on past purchases\")\n         .when((F.col(\"ts.TotalOrders\") == 1) & (F.col(\"ts.DaysSinceLastPurchase\") <= 30),\n               \"Welcome series, Educational content, First purchase follow-up, Category exploration\")\n         .when(F.col(\"cm.NextPurchasePropensity\") > 0.6,\n               F.concat(\n                   F.lit(\"Cross-sell/upsell, Bundle offers based on \"),\n                   F.coalesce(F.col(\"ts.TopCategory\"), F.lit(\"preferred category\")),\n                   F.lit(\", Category expansion\")\n               ))\n         .when(F.col(\"ts.ReturnRate\") > 20,\n               \"Satisfaction survey, Improved product information, Size/fit guides, Service recovery\")\n         .otherwise(\n               F.concat(\n                   F.lit(\"Standard seasonal promotions, Category newsletters, Reactivation after \"),\n                   (F.col(\"ts.DaysSinceLastPurchase\") / 30).cast(\"int\"),\n                   F.lit(\" months\")\n               )\n         ).alias(\"MarketingRecommendation\")\n    )\n)\n\n# Write S ILVER Tables\ncustomerBaseDF.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"lakehouse_silver.CustomerBase\")\ntransactionSummaryDF.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"lakehouse_silver.TransactionSummary\")\ncustomerMetricsDF.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"lakehouse_silver.CustomerMetrics\")\nsegmentationDF.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"lakehouse_silver.CustomerSegments\")\n\n────────────────────────────────────────────────────────────────\n-- GOLD LAYER (Analytics Upserts & Summary)\n────────────────────────────────────────────────────────────────\n\n# 1) CustomerProfiles Upsert (Emulating MERGE)\n#    For brevity, shown as overwrite in this example. In production, use MERGE if needed.\ncustomerProfileGoldDF = (\n    customerBaseDF.alias(\"cb\")\n    .join(transactionSummaryDF.alias(\"ts\"), \"CustomerID\", \"left\")\n    .select(\n        \"cb.CustomerID\",\n        \"cb.CustomerName\",\n        \"cb.Email\",\n        \"cb.Phone\",\n        \"cb.Address\",\n        \"cb.PostalCode\",\n        \"cb.City\",\n        \"cb.State\",\n        \"cb.Country\",\n        \"cb.CustomerType\",\n        \"cb.AccountManager\",\n        \"cb.CreatedDate\",\n        \"cb.IsActive\",\n        \"cb.LastContactDate\",\n        \"ts.TotalOrders\",\n        \"ts.TotalSpent\",\n        \"ts.AvgOrderValue\",\n        \"ts.FirstPurchaseDate\",\n        \"ts.LastPurchaseDate\",\n        \"ts.DaysSinceLastPurchase\",\n        \"ts.TopCategory\",\n        \"ts.TopProduct\",\n        \"ts.ReturnRate\",\n        F.current_timestamp().alias(\"ModifiedDate\")\n    )\n)\n(\n    customerProfileGoldDF\n    .write\n    .format(\"delta\")\n    .mode(\"overwrite\")  # For demonstration; replace with MERGE pattern if needed.\n    .saveAsTable(\"lakehouse_gold.CustomerProfiles\")\n)\n\n# 2) CustomerAnalytics table (APPEND new daily analysis)\ncustomerAnalyticsGoldDF = (\n    customerMetricsDF.alias(\"cm\")\n    .join(segmentationDF.alias(\"cs\"), \"CustomerID\")\n    .select(\n        F.lit(dataDate).alias(\"ProcessDate\"),\n        \"cm.CustomerID\",\n        \"cm.CustomerLifetimeValue\",\n        \"cm.RecencyScore\",\n        \"cm.FrequencyScore\",\n        \"cm.MonetaryScore\",\n        \"cm.RFMScore\",\n        \"cm.ChurnProbability\",\n        \"cm.NextPurchasePropensity\",\n        \"cm.LoyaltyIndex\",\n        \"cm.CustomerHealth\",\n        \"cs.ValueSegment\",\n        \"cs.BehaviorSegment\",\n        \"cs.LifecycleSegment\",\n        \"cs.TargetGroup\",\n        \"cs.MarketingRecommendation\"\n    )\n)\n\n(\n    customerAnalyticsGoldDF\n    .write\n    .format(\"delta\")\n    .mode(\"append\")\n    .saveAsTable(\"lakehouse_gold.CustomerAnalytics\")\n)\n\n# Data retention\nspark.sql(f\"\"\"\n    DELETE FROM lakehouse_gold.CustomerAnalytics\n    WHERE ProcessDate < DATEADD(DAY, -{retentionPeriodDays}, '{str(dataDate)}')\n\"\"\")\n\n# 3) Daily Summary\n# We approximate the summary logic: we compute counts and stats across entire dataset.\n# Because we want a single row, we use window partition then select distinct.\nfrom pyspark.sql.window import Window\n\nsummaryWindow = Window.partitionBy()\nsummaryDF = (\n    customerBaseDF.alias(\"cb\")\n    .join(customerMetricsDF.alias(\"cm\"), \"CustomerID\")\n    .join(segmentationDF.alias(\"cs\"), \"CustomerID\")\n    .select(\n        F.lit(dataDate).alias(\"ProcessDate\"),\n        F.lit(processType).alias(\"ProcessType\"),\n        F.count_distinct(\"cb.CustomerID\").over(summaryWindow).alias(\"TotalCustomers\"),\n        F.sum(F.when(F.col(\"cs.LifecycleSegment\") == \"Active\", 1).otherwise(0)).over(summaryWindow).alias(\"ActiveCustomers\"),\n        F.sum(F.when(F.col(\"cs.LifecycleSegment\") == \"New Customer\", 1).otherwise(0)).over(summaryWindow).alias(\"NewCustomers\"),\n        F.sum(F.when((F.col(\"cs.LifecycleSegment\") == \"At Risk\") | (F.col(\"cs.LifecycleSegment\") == \"Churned\"), 1).otherwise(0)).over(summaryWindow).alias(\"ChurningCustomers\"),\n        F.sum(F.when(F.col(\"cs.ValueSegment\") == \"High Value\", 1).otherwise(0)).over(summaryWindow).alias(\"HighValueCustomers\"),\n        F.avg(\"cm.CustomerLifetimeValue\").over(summaryWindow).alias(\"AverageLifetimeValue\"),\n        # Approx total processing time: difference between now and dataDate\n        (F.unix_timestamp(F.current_timestamp()) - F.unix_timestamp(F.lit(str(dataDate)))) * 1000,\n        F.lit(\"Success\").alias(\"ProcessingStatus\")\n    )\n).distinct()\n\n(\n    summaryDF\n    .write\n    .format(\"delta\")\n    .mode(\"append\")\n    .saveAsTable(\"lakehouse_gold.CustomerAnalyticsSummary\")\n)\n\nif debugMode:\n    summaryDF.show(truncate=False)\n\n# End of pipeline\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n3) IMPLEMENTATION GUIDELINES AND BEST PRACTICES (TechLead)\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n• Store Parameters & Paths Externally:\n  - Use config tables or files to manage thresholds (churnDays, highValueThreshold, etc.) for easy updates.\n\n• Delta Lake Advantages:\n  - Use Delta's MERGE for incremental upserts into Gold tables.  \n  - Partition large tables by date fields if data volumes become large.\n\n• Error Handling:\n  - Wrap transformations in try/except blocks or rely on pipeline orchestrations.  \n  - Log exceptions and partial progress in separate logging tables or your orchestration framework.\n\n• Performance Optimizations:\n  - Cache or broadcast smaller tables (e.g., reference data) as needed for speed.\n  - Consider partitioning or bucketing Delta tables for large data sets.  \n  - Use cluster sizing that accommodates daily volumes.\n\n• DevOps & CI/CD:\n  - Check in this notebook/script into a Git repo, integrate with automated testing, then release changes to production with minimal downtime.\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n4) TEST CASES FOR VALIDATION (TestingAgent)\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\nBelow are recommended test scenarios:\n\n1) Full Load with Known Dataset\n   - Include new/active/churned/high-value/low-value customers.  \n   - Validate final Gold tables match expected RFM scores, segments, and summary counts.\n\n2) Incremental Load Test\n   - Provide new or updated records (e.g., Orders with a date after dataDate).  \n   - Confirm that only new or changed records are processed and no duplicates appear.\n\n3) Parameter Variation\n   - Adjust churnDays and highValueThreshold, ensure outputs (churn probability, value segment) change accordingly.\n\n4) Edge Cases: No Orders or Interactions\n   - Include customers with zero orders or interactions.  \n   - Ensure the pipeline handles them gracefully, reflecting zero or null transaction metrics.\n\n5) Retention Check\n   - Provide data older than 365 days, run the pipeline.  \n   - Confirm older CustomerAnalytics records are removed from Gold.\n\n6) Error Handling / Malformed Data\n   - Introduce data integrity issues or simulate Spark job failures.  \n   - Expect the pipeline to fail gracefully, log errors, and/or skip invalid records.\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\nBy applying the medallion approach (Bronze → Silver → Gold) with well-organized transformations, you modernize the original stored procedure into a scalable PySpark pipeline in Microsoft Fabric. This design ensures reliable incremental processing, flexible parameter management, and compliance with best practices around code versioning, data quality, and structured analytics output.",
    "role": "user",
    "name": "DomainExpert"
  },
  {
    "content": "Below is a comprehensive plan and example implementation for migrating the given SQL stored procedure to a PySpark solution within Microsoft Fabric, following a Medallion (Bronze–Silver–Gold) architecture. This answer is segmented by role for clarity:\n\n────────────────────────────────────────────────────────────────────────────\n1) BUSINESS ANALYSIS AND MIGRATION PLAN (BusinessAnalyst & ProductOwner)\n────────────────────────────────────────────────────────────────────────────\n\nA. Business Purpose  \n• The original SQL stored procedure (usp_CustomerAnalytics_Processing) produces customer analytics for marketing, sales, and other business operations.  \n• It calculates RFM (Recency, Frequency, Monetary) scores, churn probability, lifetime value, loyalty, and segmentation; then stores both daily analytics and summary statistics into target tables.\n\nB. Requirements & Logic Highlights  \n• RFM scoring (5-point scales) for recency, frequency, and monetary value.  \n• Churn probability thresholds based on DaysSinceLastPurchase compared to configurable parameters (ChurnDays_xxx).  \n• Additional metrics: next purchase propensity, loyalty index, and overall customer health.  \n• Segmentation logic: ValueSegment, BehaviorSegment, and LifecycleSegment, plus target groups and marketing recommendations.  \n• Daily summary aggregates (counts of active, new, churning, high-value customers, average LTV, etc.).\n\nC. Data Sources  \n• Customers, CustomerAddresses, CustomerInteractions.  \n• Orders, OrderDetails, Products, Returns.  \n• Destination tables: CustomerProfiles, CustomerAnalytics, CustomerAnalyticsSummary.\n\nD. Migration Approach  \n• Use the Microsoft Fabric Medallion architecture with Delta tables:  \n  1) Bronze: Raw data ingestion (Customers, Orders, etc.).  \n  2) Silver: Intermediate transformations (joins, RFM, churn calc, segmentation).  \n  3) Gold: Final analytics (CustomerProfiles upserts, daily CustomerAnalytics, summary logs).  \n\n• Use PySpark in Microsoft Fabric notebooks or pipelines to replicate the stored procedure’s functionality.  \n• Leverage Delta Lake for incremental loading via MERGE/append and ACID transactions.  \n• Store parameters in an external configuration or as variables in the notebook for easy updates.\n\n────────────────────────────────────────────────────────────────────────────\n2) PYS PARK CODE FOR BRONZE, SILVER, AND GOLD (AzureDataEngineer)\n────────────────────────────────────────────────────────────────────────────\n\nBelow is a sample PySpark notebook/script illustrating how to replace the SQL logic with a structured data engineering approach in Fabric. Adapt table/database names and paths to match your environment. Comments guide key transformations. You can orchestrate this code with Fabric Pipelines or Azure Data Factory.\n\n-------------------------------------------------------------------------------\n-- SETUP & CONFIGURATION\n-------------------------------------------------------------------------------\nfrom pyspark.sql import SparkSession, functions as F, Window\nimport datetime\n\nspark = SparkSession.builder.getOrCreate()\n\n# Parameters (mirroring SQL stored proc defaults)\ndataDate = None             # If not provided, default to yesterday.\nprocessType = 'FULL'        # 'FULL' or 'INCREMENTAL'\ndebugMode = False           # If True, print or store additional logs.\nretentionPeriodDays = 365\nhighValueThreshold = 5000.00\nloyaltyThreshold = 5\nchurnDaysVeryHigh = 180\nchurnDaysHigh = 90\nchurnDaysMedium = 60\nchurnDaysLow = 30\n\n# Default the data date if None:\nif dataDate is None:\n    dataDate = datetime.date.today() - datetime.timedelta(days=1)\n\n# 1) BRONZE LAYER: Read from raw tables in the Fabric Lakehouse/Warehouse\ncustomersBronzeDF = spark.read.table(\"lakehouse_raw.Customers\")\naddressesBronzeDF = spark.read.table(\"lakehouse_raw.CustomerAddresses\")\ninteractionsBronzeDF = spark.read.table(\"lakehouse_raw.CustomerInteractions\")\nordersBronzeDF = spark.read.table(\"lakehouse_raw.Orders\")\norderDetailsBronzeDF = spark.read.table(\"lakehouse_raw.OrderDetails\")\nproductsBronzeDF = spark.read.table(\"lakehouse_raw.Products\")\nreturnsBronzeDF = spark.read.table(\"lakehouse_raw.Returns\")\n\n-------------------------------------------------------------------------------\n-- SILVER LAYER: Transformations (merging logic from the SQL stored procedure)\n-------------------------------------------------------------------------------\n\n# -----------------------\n# Step 1: Build Customer Base (like #CustomerBase)\n# -----------------------\ncustomerBaseDF = (\n    customersBronzeDF.alias(\"c\")\n    .join(\n        addressesBronzeDF.alias(\"a\"),\n        (F.col(\"c.CustomerID\") == F.col(\"a.CustomerID\")) & (F.col(\"a.AddressType\") == \"Primary\"),\n        \"left\"\n    )\n    .join(\n        interactionsBronzeDF.alias(\"i\"),\n        F.col(\"c.CustomerID\") == F.col(\"i.CustomerID\"),\n        \"left\"\n    )\n    .where(\n        (processType == 'FULL')\n        | (F.col(\"c.ModifiedDate\") >= F.lit(dataDate))\n        | (F.col(\"i.ContactDate\") >= F.lit(dataDate))\n    )\n    .groupBy(\n        \"c.CustomerID\",\n        \"c.FirstName\",\n        \"c.LastName\",\n        \"c.Email\",\n        \"c.Phone\",\n        \"a.StreetAddress\",\n        \"a.PostalCode\",\n        \"a.City\",\n        \"a.State\",\n        \"a.Country\",\n        \"c.CustomerType\",\n        \"c.AccountManagerID\",\n        \"c.CreatedDate\",\n        \"c.ModifiedDate\",\n        \"c.Status\"\n    )\n    .agg(F.max(\"i.ContactDate\").alias(\"LastContactDate\"))\n    .select(\n        F.col(\"CustomerID\"),\n        F.concat_ws(\" \", F.col(\"FirstName\"), F.col(\"LastName\")).alias(\"CustomerName\"),\n        \"Email\",\n        \"Phone\",\n        F.col(\"StreetAddress\").alias(\"Address\"),\n        \"PostalCode\",\n        \"City\",\n        \"State\",\n        \"Country\",\n        \"CustomerType\",\n        F.col(\"AccountManagerID\").alias(\"AccountManager\"),\n        \"CreatedDate\",\n        \"ModifiedDate\",\n        (F.col(\"Status\") == \"Active\").cast(\"boolean\").alias(\"IsActive\"),\n        \"LastContactDate\"\n    )\n)\n\n# -----------------------\n# Step 2: Transaction Summary (#TransactionSummary)\n# -----------------------\n\n# (a) Summarize orders\norderSummaryDF = (\n    ordersBronzeDF.alias(\"o\")\n    .join(orderDetailsBronzeDF.alias(\"od\"), \"OrderID\")\n    .where(F.col(\"o.Status\") == \"Completed\")\n    .groupBy(\"o.CustomerID\")\n    .agg(\n        F.countDistinct(\"o.OrderID\").alias(\"TotalOrders\"),\n        F.sum(F.col(\"od.Quantity\") * F.col(\"od.UnitPrice\") * (1 - F.col(\"od.Discount\"))).alias(\"TotalSpent\"),\n        F.min(\"o.OrderDate\").alias(\"FirstPurchaseDate\"),\n        F.max(\"o.OrderDate\").alias(\"LastPurchaseDate\")\n    )\n)\n\ntransactionSummaryDF = (\n    orderSummaryDF.alias(\"os\")\n    .join(\n        customerBaseDF.select(\"CustomerID\").alias(\"cb\"),\n        F.col(\"os.CustomerID\") == F.col(\"cb.CustomerID\"),\n        \"inner\"\n    )\n    .select(\n        F.col(\"os.CustomerID\"),\n        F.col(\"os.TotalOrders\"),\n        F.col(\"os.TotalSpent\"),\n        F.when(F.col(\"os.TotalOrders\") > 0,\n               F.col(\"os.TotalSpent\") / F.col(\"os.TotalOrders\"))\n         .otherwise(F.lit(0.0)).alias(\"AvgOrderValue\"),\n        F.col(\"os.FirstPurchaseDate\"),\n        F.col(\"os.LastPurchaseDate\"),\n        F.datediff(F.lit(dataDate), F.col(\"os.LastPurchaseDate\")).alias(\"DaysSinceLastPurchase\")\n    )\n)\n\n# (b) Compute Top Category per customer\ncategorySpendDF = (\n    ordersBronzeDF.alias(\"o\")\n    .join(orderDetailsBronzeDF.alias(\"od\"), \"OrderID\")\n    .join(productsBronzeDF.alias(\"p\"), \"ProductID\")\n    .where(F.col(\"o.Status\") == \"Completed\")\n    .groupBy(\"o.CustomerID\", \"p.Category\")\n    .agg(F.sum(F.col(\"od.Quantity\") * F.col(\"od.UnitPrice\") * (1 - F.col(\"od.Discount\"))).alias(\"CategorySpend\"))\n)\n\nwindowCategory = Window.partitionBy(\"CustomerID\").orderBy(F.col(\"CategorySpend\").desc())\ncategoryRankedDF = categorySpendDF.withColumn(\n    \"CategoryRank\",\n    F.row_number().over(windowCategory)\n).filter(F.col(\"CategoryRank\") == 1)\n\n# (c) Compute Top Product per customer\nproductQtyDF = (\n    ordersBronzeDF.alias(\"o\")\n    .join(orderDetailsBronzeDF.alias(\"od\"), \"OrderID\")\n    .join(productsBronzeDF.alias(\"p\"), \"ProductID\")\n    .where(F.col(\"o.Status\") == \"Completed\")\n    .groupBy(\"o.CustomerID\", \"p.ProductName\")\n    .agg(F.sum(\"od.Quantity\").alias(\"TotalQuantity\"))\n)\n\nwindowProduct = Window.partitionBy(\"CustomerID\").orderBy(F.col(\"TotalQuantity\").desc())\nproductRankedDF = productQtyDF.withColumn(\n    \"ProductRank\",\n    F.row_number().over(windowProduct)\n).filter(F.col(\"ProductRank\") == 1)\n\n# (d) Calculate Return Rate per customer\nreturnsGroupedDF = (\n    returnsBronzeDF.groupBy(\"CustomerID\")\n    .agg(F.countDistinct(\"ReturnID\").alias(\"TotalReturns\"))\n)\n\n# (e) Combine all into final transaction summary\ntransactionSummaryDF = (\n    transactionSummaryDF.alias(\"ts\")\n    .join(categoryRankedDF.alias(\"cat\"), \"CustomerID\", \"left\")\n    .join(productRankedDF.alias(\"prod\"), \"CustomerID\", \"left\")\n    .join(returnsGroupedDF.alias(\"ret\"), \"CustomerID\", \"left\")\n    .select(\n        F.col(\"ts.CustomerID\"),\n        \"ts.TotalOrders\",\n        \"ts.TotalSpent\",\n        \"ts.AvgOrderValue\",\n        \"ts.FirstPurchaseDate\",\n        \"ts.LastPurchaseDate\",\n        \"ts.DaysSinceLastPurchase\",\n        F.col(\"cat.Category\").alias(\"TopCategory\"),\n        F.col(\"prod.ProductName\").alias(\"TopProduct\"),\n        F.when(F.col(\"ts.TotalOrders\") > 0,\n               (F.col(\"ret.TotalReturns\") / F.col(\"ts.TotalOrders\")) * 100\n              ).otherwise(F.lit(0.0)).alias(\"ReturnRate\")\n    )\n)\n\n# -----------------------\n# Step 3: Calculate Customer Metrics (#CustomerMetrics)\n# -----------------------\n# RFM (Recency, Frequency, Monetary) scoring with NTILE(5)\n\nwindowRecency = Window.orderBy(F.col(\"DaysSinceLastPurchase\").asc())\nwindowFrequency = Window.orderBy(F.col(\"TotalOrders\").desc())\nwindowMonetary = Window.orderBy(F.col(\"TotalSpent\").desc())\n\ntransactionSummaryDF = (\n    transactionSummaryDF\n    .withColumn(\"RecencyScore\", F.ntile(5).over(windowRecency))\n    .withColumn(\"FrequencyScore\", F.ntile(5).over(windowFrequency))\n    .withColumn(\"MonetaryScore\", F.ntile(5).over(windowMonetary))\n)\n\nrfmDF = transactionSummaryDF.select(\n    \"CustomerID\",\n    \"RecencyScore\",\n    \"FrequencyScore\",\n    \"MonetaryScore\",\n    (F.col(\"RecencyScore\") + F.col(\"FrequencyScore\") + F.col(\"MonetaryScore\")).alias(\"RFMScore\")\n)\n\n# Churn probability & next purchase propensity\nchurnDF = (\n    transactionSummaryDF.select(\n        \"CustomerID\",\n        F.when(F.col(\"DaysSinceLastPurchase\") > churnDaysVeryHigh, 0.8)\n         .when(F.col(\"DaysSinceLastPurchase\") > churnDaysHigh, 0.5)\n         .when(F.col(\"DaysSinceLastPurchase\") > churnDaysMedium, 0.3)\n         .when(F.col(\"DaysSinceLastPurchase\") > churnDaysLow, 0.1)\n         .otherwise(0.05).alias(\"ChurnProbability\"),\n        F.when((F.col(\"DaysSinceLastPurchase\") < churnDaysLow) & (F.col(\"TotalOrders\") > 3), 0.8)\n         .when((F.col(\"DaysSinceLastPurchase\") < churnDaysMedium) & (F.col(\"TotalOrders\") > 2), 0.6)\n         .when(F.col(\"DaysSinceLastPurchase\") < churnDaysHigh, 0.4)\n         .otherwise(0.2).alias(\"NextPurchasePropensity\"),\n        (\n            F.when(\n                (F.col(\"TotalOrders\") >= loyaltyThreshold) & (F.col(\"ReturnRate\") < 10),\n                (F.col(\"TotalOrders\") * 0.6) + ((100 - F.col(\"ReturnRate\")) * 0.4)\n            )\n            .otherwise(F.col(\"TotalOrders\") * 0.4)\n            / 10\n        ).alias(\"LoyaltyIndex\")\n    )\n)\n\n# Combine RFM and Churn to produce final CustomerMetrics\ncustomerMetricsDF = (\n    rfmDF.alias(\"r\")\n    .join(transactionSummaryDF.alias(\"t\"), \"CustomerID\")\n    .join(churnDF.alias(\"ch\"), \"CustomerID\")\n    .select(\n        F.col(\"r.CustomerID\"),\n        (F.col(\"t.TotalSpent\") * (1 + (1 - F.col(\"ch.ChurnProbability\")))).alias(\"CustomerLifetimeValue\"),\n        \"r.RecencyScore\",\n        \"r.FrequencyScore\",\n        \"r.MonetaryScore\",\n        \"r.RFMScore\",\n        \"ch.ChurnProbability\",\n        \"ch.NextPurchasePropensity\",\n        \"ch.LoyaltyIndex\",\n        F.when(F.col(\"ch.ChurnProbability\") > 0.7, \"At Risk\")\n         .when(F.col(\"ch.LoyaltyIndex\") > 0.8, \"Excellent\")\n         .when(F.col(\"ch.LoyaltyIndex\") > 0.6, \"Good\")\n         .when(F.col(\"ch.LoyaltyIndex\") > 0.4, \"Average\")\n         .otherwise(\"Needs Attention\").alias(\"CustomerHealth\")\n    )\n)\n\n# -----------------------\n# Step 4: Customer Segmentation (#CustomerSegments)\n# -----------------------\nsegmentationDF = (\n    customerBaseDF.alias(\"cb\")\n    .join(customerMetricsDF.alias(\"cm\"), \"CustomerID\")\n    .join(transactionSummaryDF.alias(\"ts\"), \"CustomerID\")\n    .select(\n        F.col(\"cb.CustomerID\"),\n        # ValueSegment\n        F.when(F.col(\"cm.CustomerLifetimeValue\") >= highValueThreshold, \"High Value\")\n         .when(F.col(\"cm.CustomerLifetimeValue\") >= highValueThreshold * 0.5, \"Medium Value\")\n         .otherwise(\"Low Value\").alias(\"ValueSegment\"),\n        # BehaviorSegment\n        F.when(F.col(\"cm.RFMScore\") >= 13, \"Champions\")\n         .when((F.col(\"cm.RecencyScore\") >= 4) & (F.col(\"cm.FrequencyScore\") >= 3), \"Loyal Customers\")\n         .when((F.col(\"cm.RecencyScore\") >= 4) & (F.col(\"cm.FrequencyScore\") <= 2), \"Potential Loyalists\")\n         .when((F.col(\"cm.RecencyScore\") <= 2) & (F.col(\"cm.FrequencyScore\") >= 3) & (F.col(\"cm.MonetaryScore\") >= 3), \"At Risk\")\n         .when((F.col(\"cm.RecencyScore\") <= 2) & (F.col(\"cm.FrequencyScore\") <= 2) & (F.col(\"cm.MonetaryScore\") <= 2), \"Hibernating\")\n         .when(F.col(\"cm.RecencyScore\") <= 1, \"Lost\")\n         .otherwise(\"Others\").alias(\"BehaviorSegment\"),\n        # LifecycleSegment\n        F.when((F.col(\"ts.TotalOrders\") == 1) & (F.col(\"ts.DaysSinceLastPurchase\") <= 30), \"New Customer\")\n         .when((F.col(\"ts.TotalOrders\") > 1) & (F.col(\"cm.ChurnProbability\") < 0.3), \"Active\")\n         .when((F.col(\"cm.ChurnProbability\") >= 0.3) & (F.col(\"cm.ChurnProbability\") < 0.7), \"At Risk\")\n         .when(F.col(\"cm.ChurnProbability\") >= 0.7, \"Churned\")\n         .otherwise(\"Inactive\").alias(\"LifecycleSegment\"),\n        # TargetGroup\n        F.when((F.col(\"cm.CustomerLifetimeValue\") >= highValueThreshold) & (F.col(\"cm.ChurnProbability\") < 0.3), \"VIP\")\n         .when((F.col(\"cm.CustomerLifetimeValue\") >= highValueThreshold * 0.5) & (F.col(\"cm.ChurnProbability\") >= 0.3), \"Retention Priority\")\n         .when((F.col(\"ts.TotalOrders\") == 1) & (F.col(\"ts.DaysSinceLastPurchase\") <= 30), \"Nurture New\")\n         .when(F.col(\"cm.NextPurchasePropensity\") > 0.6, \"Growth Opportunity\")\n         .when(F.col(\"ts.ReturnRate\") > 20, \"Service Improvement\")\n         .otherwise(\"Standard\").alias(\"TargetGroup\"),\n        # MarketingRecommendation\n        F.when(\n            (F.col(\"cm.CustomerLifetimeValue\") >= highValueThreshold) & (F.col(\"cm.ChurnProbability\") < 0.3),\n            \"Exclusive offers, VIP events, Personal shopping assistance, Early access to new products\"\n        ).when(\n            (F.col(\"cm.CustomerLifetimeValue\") >= highValueThreshold * 0.5) & (F.col(\"cm.ChurnProbability\") >= 0.3),\n            \"Re-engagement campaign, Loyalty rewards, Personalized recommendations based on past purchases\"\n        ).when(\n            (F.col(\"ts.TotalOrders\") == 1) & (F.col(\"ts.DaysSinceLastPurchase\") <= 30),\n            \"Welcome series, Educational content, First purchase follow-up, Category exploration\"\n        ).when(\n            F.col(\"cm.NextPurchasePropensity\") > 0.6,\n            F.concat(\n                F.lit(\"Cross-sell/upsell, Bundle offers based on \"),\n                F.coalesce(F.col(\"ts.TopCategory\"), F.lit(\"preferred category\")),\n                F.lit(\", Category expansion\")\n            )\n        ).when(\n            F.col(\"ts.ReturnRate\") > 20,\n            \"Satisfaction survey, Improved product information, Size/fit guides, Service recovery\"\n        ).otherwise(\n            F.concat(\n                F.lit(\"Standard seasonal promotions, Category newsletters, Reactivation after \"),\n                (F.col(\"ts.DaysSinceLastPurchase\") / 30).cast(\"int\"),\n                F.lit(\" months\")\n            )\n        ).alias(\"MarketingRecommendation\")\n    )\n)\n\n# Write Silver Tables (Delta format to support upserts/merges if needed)\ncustomerBaseDF.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"lakehouse_silver.CustomerBase\")\ntransactionSummaryDF.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"lakehouse_silver.TransactionSummary\")\ncustomerMetricsDF.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"lakehouse_silver.CustomerMetrics\")\nsegmentationDF.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"lakehouse_silver.CustomerSegments\")\n\n-------------------------------------------------------------------------------\n-- GOLD LAYER: Upserts & Aggregations\n-------------------------------------------------------------------------------\n\n# Example 1: CustomerProfiles (Emulate MERGE - shown as overwrite for simplicity)\ncustomerProfileGoldDF = (\n    customerBaseDF.alias(\"cb\")\n    .join(transactionSummaryDF.alias(\"ts\"), \"CustomerID\", \"left\")\n    .select(\n        \"cb.CustomerID\",\n        \"cb.CustomerName\",\n        \"cb.Email\",\n        \"cb.Phone\",\n        \"cb.Address\",\n        \"cb.PostalCode\",\n        \"cb.City\",\n        \"cb.State\",\n        \"cb.Country\",\n        \"cb.CustomerType\",\n        \"cb.AccountManager\",\n        \"cb.CreatedDate\",\n        \"cb.IsActive\",\n        \"cb.LastContactDate\",\n        \"ts.TotalOrders\",\n        \"ts.TotalSpent\",\n        \"ts.AvgOrderValue\",\n        \"ts.FirstPurchaseDate\",\n        \"ts.LastPurchaseDate\",\n        \"ts.DaysSinceLastPurchase\",\n        \"ts.TopCategory\",\n        \"ts.TopProduct\",\n        \"ts.ReturnRate\",\n        F.current_timestamp().alias(\"ModifiedDate\")\n    )\n)\n\n(\n    customerProfileGoldDF\n    .write\n    .format(\"delta\")\n    .mode(\"overwrite\")  # In production, consider Delta MERGE instead for incremental updates\n    .saveAsTable(\"lakehouse_gold.CustomerProfiles\")\n)\n\n# Example 2: CustomerAnalytics (append daily snapshots)\ncustomerAnalyticsGoldDF = (\n    customerMetricsDF.alias(\"cm\")\n    .join(segmentationDF.alias(\"cs\"), \"CustomerID\")\n    .select(\n        F.lit(dataDate).alias(\"ProcessDate\"),\n        \"cm.CustomerID\",\n        \"cm.CustomerLifetimeValue\",\n        \"cm.RecencyScore\",\n        \"cm.FrequencyScore\",\n        \"cm.MonetaryScore\",\n        \"cm.RFMScore\",\n        \"cm.ChurnProbability\",\n        \"cm.NextPurchasePropensity\",\n        \"cm.LoyaltyIndex\",\n        \"cm.CustomerHealth\",\n        \"cs.ValueSegment\",\n        \"cs.BehaviorSegment\",\n        \"cs.LifecycleSegment\",\n        \"cs.TargetGroup\",\n        \"cs.MarketingRecommendation\"\n    )\n)\n\n(\n    customerAnalyticsGoldDF\n    .write\n    .format(\"delta\")\n    .mode(\"append\")\n    .saveAsTable(\"lakehouse_gold.CustomerAnalytics\")\n)\n\n# Retention: Remove records older than retentionPeriodDays\nspark.sql(f\"\"\"\n    DELETE FROM lakehouse_gold.CustomerAnalytics\n    WHERE ProcessDate < DATEADD(DAY, -{retentionPeriodDays}, '{str(dataDate)}')\n\"\"\")\n\n# Example 3: Summary aggregated metrics (CustomerAnalyticsSummary)\nfrom pyspark.sql.window import Window\n\nsummaryWindow = Window.partitionBy()\nsummaryDF = (\n    customerBaseDF.alias(\"cb\")\n    .join(customerMetricsDF.alias(\"cm\"), \"CustomerID\")\n    .join(segmentationDF.alias(\"cs\"), \"CustomerID\")\n    .select(\n        F.lit(dataDate).alias(\"ProcessDate\"),\n        F.lit(processType).alias(\"ProcessType\"),\n        F.count_distinct(\"cb.CustomerID\").over(summaryWindow).alias(\"TotalCustomers\"),\n        F.sum(F.when(F.col(\"cs.LifecycleSegment\") == \"Active\", 1).otherwise(0)).over(summaryWindow).alias(\"ActiveCustomers\"),\n        F.sum(F.when(F.col(\"cs.LifecycleSegment\") == \"New Customer\", 1).otherwise(0)).over(summaryWindow).alias(\"NewCustomers\"),\n        F.sum(F.when((F.col(\"cs.LifecycleSegment\") == \"At Risk\") | (F.col(\"cs.LifecycleSegment\") == \"Churned\"), 1).otherwise(0)).over(summaryWindow).alias(\"ChurningCustomers\"),\n        F.sum(F.when(F.col(\"cs.ValueSegment\") == \"High Value\", 1).otherwise(0)).over(summaryWindow).alias(\"HighValueCustomers\"),\n        F.avg(\"cm.CustomerLifetimeValue\").over(summaryWindow).alias(\"AverageLifetimeValue\"),\n        # Approx total processing time in ms: difference between now and the dataDate\n        (F.unix_timestamp(F.current_timestamp()) - F.unix_timestamp(F.lit(str(dataDate)))) * 1000,\n        F.lit(\"Success\").alias(\"ProcessingStatus\")\n    )\n).distinct()\n\n(\n    summaryDF\n    .write\n    .format(\"delta\")\n    .mode(\"append\")\n    .saveAsTable(\"lakehouse_gold.CustomerAnalyticsSummary\")\n)\n\nif debugMode:\n    summaryDF.show(truncate=False)\n\n# End of PySpark pipeline\n\n────────────────────────────────────────────────────────────────────────────\n3) IMPLEMENTATION GUIDELINES & BEST PRACTICES (TechLead)\n────────────────────────────────────────────────────────────────────────────\n\n• Parameter Management:  \n  - Store thresholds (churnDays, highValueThreshold, etc.) in a config table or file, rather than hardcoding in code.  \n  - This makes updates and environment-specific overrides easier.\n\n• Use Delta Format:  \n  - Leverage Delta Lake for ACID merges, time-travel, and optimized reads/writes.  \n  - When updating large analytical tables, prefer Delta MERGE over “overwrite” to handle incremental changes gracefully.\n\n• Incremental Loads:  \n  - For big data sets, run incremental logic by filtering on ModifiedDate or date partitions (similar to the original stored procedure’s approach).  \n  - Use partitioning on large dimension or fact tables.\n\n• Performance Optimization:  \n  - Cache or broadcast smaller dimension tables if symmetrical joins are repeated.  \n  - Evaluate cluster sizing (number and type of nodes) for daily peak loads.\n\n• Error Handling & Logging:  \n  - Enclose critical steps in try/except blocks; handle exceptions or log them in your orchestration pipeline.  \n  - Use a centralized log table or metadata store to capture transformation progress and row counts (similar to #ProcessLog in the stored procedure).\n\n• DevOps & CI/CD:  \n  - Commit this PySpark notebook/script to a Git repository.  \n  - Automate testing and deployment with Microsoft Fabric pipelines or Azure DevOps, ensuring that changes are validated before production.\n\n────────────────────────────────────────────────────────────────────────────\n4) TEST CASES FOR VALIDATION (TestingAgent)\n────────────────────────────────────────────────────────────────────────────\n\nBelow are some recommended test scenarios:\n\n1) Full Load with Known Data  \n   • Use a pre-defined dataset containing both active, new, churned, and high-value customers.  \n   • Verify that the final Gold tables accurately reflect RFM scores, segmentation, and summary stats.\n\n2) Incremental Load Test  \n   • Insert new or updated records with dates after dataDate.  \n   • Confirm that only these new or changed records appear in the Silver/Gold tables, without duplicating existing data.\n\n3) Parameter Variation Test  \n   • Adjust churnDays (e.g., churnDaysVeryHigh=200) or highValueThreshold=10000.  \n   • Confirm that metrics (churn probability, segment classification) adjust accordingly.\n\n4) Edge Cases (No Orders or Interactions)  \n   • Include customers with 0 orders or interactions.  \n   • Validate that they are handled gracefully and appear in final tables with 0 or null metrics.\n\n5) Retention Check  \n   • Load historical data older than the retention period, run the pipeline.  \n   • Confirm data older than X days is removed from CustomerAnalytics.\n\n6) Error Handling / Bad Data Simulation  \n   • Insert invalid references or cause a transformation error.  \n   • Expect the pipeline to log or raise the error without silently corrupting the data.\n\n────────────────────────────────────────────────────────────────────────────\n\nBy following this medallion architecture in Microsoft Fabric, the originally monolithic SQL stored procedure’s functionality is broken into clear stages—Bronze (raw), Silver (transformed), and Gold (analytics)—providing better maintainability, scalability, and alignment with modern cloud data engineering best practices. The code above includes the core logic, recommended design patterns, and thorough testing guidelines to ensure a smooth migration to PySpark on Microsoft Fabric.",
    "role": "user",
    "name": "AzureExpert"
  },
  {
    "content": "Below is a comprehensive end-to-end migration plan and example PySpark implementation for Microsoft Fabric, presented through the lens of multiple roles. It includes:\n\n• A high-level business analysis and migration plan (BusinessAnalyst and ProductOwner)  \n• Detailed PySpark code for Bronze, Silver, and Gold layers (AzureDataEngineer)  \n• Implementation guidelines and best practices (TechLead)  \n• Test cases to validate the migration (TestingAgent)  \n\nThis approach transforms the original SQL stored procedure into a modern, scalable, and maintainable PySpark pipeline following the Medallion architecture in Microsoft Fabric (Bronze → Silver → Gold).  \n \n─────────────────────────────────────────────────────────────────────────\n1) BUSINESS ANALYSIS & MIGRATION PLAN\n   (BusinessAnalyst & ProductOwner)\n─────────────────────────────────────────────────────────────────────────\n\nA. Business Purpose and Requirements\n\n• The stored procedure usp_CustomerAnalytics_Processing consolidated multiple steps:\n  – Extracting customer data (active and recently modified).  \n  – Calculating transaction summaries (orders, spend, returns).  \n  – Computing RFM (Recency, Frequency, Monetary) scores.  \n  – Estimating churn probability and loyalty.  \n  – Segmenting customers (ValueSegment, BehaviorSegment, LifecycleSegment, TargetGroup).  \n  – Storing daily analytics and generating a summary of key metrics.\n\n• Target Teams & Use Cases:\n  – Marketing: Identify marketing campaigns for “At Risk” vs. “VIP” customers.  \n  – Sales & Customer Success: Focus on high-value, high-churn segments.  \n  – Executives: Monitor daily summaries, churn trends, and average lifetime value.\n\n• Primary Business Requirements:\n  1. Accurate daily calculations of customer metrics and segments.  \n  2. Ability to handle both full (historical) and incremental loads.  \n  3. Retention control (e.g., 365 days) for older analytics records.  \n  4. Clear separation of raw vs. processed data (Medallion architecture).  \n  5. Configurable thresholds (churn days, high-value limit, loyalty threshold).\n\nB. Migration Approach – Medallion Architecture\n\n• Bronze Layer:\n  – Raw data ingestion from source tables: Customers, Orders, Products, Returns, etc.  \n  – No transformations or filtering except minimal data-type conversions if needed.\n\n• Silver Layer:\n  – Apply transformations that replicate the stored procedure logic:\n     1. Create a “Customer Base” equivalent (joins and deduplication).  \n     2. Calculate transaction summary (orders, average spend, top category/product, returns).  \n     3. Compute advanced metrics (RFM scores, churn probability, loyalty index).  \n     4. Derive customer segmentation for marketing.\n\n• Gold Layer:\n  – Curate and store final analytics tables:\n     1. CustomerProfiles (like dbo.CustomerProfiles).  \n     2. CustomerAnalytics (daily snapshots with RFM, churn, segment data).  \n     3. CustomerAnalyticsSummary (aggregate stats each day).\n\n• Incremental vs. Full Loads:\n  – Full loads reread all source data (often for initial deployment).  \n  – Incremental runs filter on date fields (ModifiedDate, ContactDate, etc.) to minimize processing.\n\n• Key Parameters (Mirroring the SP Defaults):\n  – dataDate (default: yesterday)  \n  – processType (FULL or INCREMENTAL)  \n  – debugMode (True/False)  \n  – retentionPeriodDays (365)  \n  – highValueThreshold (5000.0)  \n  – loyaltyThreshold (5)  \n  – churnDaysVeryHigh (180), churnDaysHigh (90), churnDaysMedium (60), churnDaysLow (30)\n\n─────────────────────────────────────────────────────────────────────────\n2) PYS PARK CODE (AZUREDATAENGINEER)\n   BRONZE → SILVER → GOLD\n─────────────────────────────────────────────────────────────────────────\n\nBelow is sample PySpark code you can run within a Microsoft Fabric notebook or pipeline. Adjust table/database names, paths, and parameter storage to suit your environment.\n\n--------------------------------------------------------------------------------\n# SETUP & CONFIG\n--------------------------------------------------------------------------------\nfrom pyspark.sql import SparkSession, functions as F, Window\nimport datetime\n\n# Initialize Spark - in Fabric, SparkSession is available in notebooks by default\nspark = SparkSession.builder.getOrCreate()\n\n# Parameters analogous to the SQL stored procedure\ndataDate = None  # If None, defaults to yesterday\nprocessType = 'FULL'\ndebugMode = False\nretentionPeriodDays = 365\nhighValueThreshold = 5000.0\nloyaltyThreshold = 5\nchurnDaysVeryHigh = 180\nchurnDaysHigh = 90\nchurnDaysMedium = 60\nchurnDaysLow = 30\n\n# Determine default data date (yesterday) if not provided\nif dataDate is None:\n    dataDate = datetime.date.today() - datetime.timedelta(days=1)\n\n# BRONZE: Read raw tables from your Fabric Lakehouse or Warehouse\ncustomersBronzeDF = spark.read.table(\"lakehouse_raw.Customers\")\naddressesBronzeDF = spark.read.table(\"lakehouse_raw.CustomerAddresses\")\ninteractionsBronzeDF = spark.read.table(\"lakehouse_raw.CustomerInteractions\")\nordersBronzeDF = spark.read.table(\"lakehouse_raw.Orders\")\norderDetailsBronzeDF = spark.read.table(\"lakehouse_raw.OrderDetails\")\nproductsBronzeDF = spark.read.table(\"lakehouse_raw.Products\")\nreturnsBronzeDF = spark.read.table(\"lakehouse_raw.Returns\")\n\n--------------------------------------------------------------------------------\n# SILVER LAYER: Transformations Replicating the SP Logic\n--------------------------------------------------------------------------------\n\n# -------------------------------\n# Step 1: Create \"Customer Base\"\n# -------------------------------\n# Emulates #CustomerBase by joining customers, addresses, interactions\ncustomerBaseDF = (\n    customersBronzeDF.alias(\"c\")\n    .join(\n        addressesBronzeDF.alias(\"a\"),\n        ((F.col(\"c.CustomerID\") == F.col(\"a.CustomerID\")) & (F.col(\"a.AddressType\") == \"Primary\")),\n        \"left\"\n    )\n    .join(\n        interactionsBronzeDF.alias(\"i\"),\n        F.col(\"c.CustomerID\") == F.col(\"i.CustomerID\"),\n        \"left\"\n    )\n    .where(\n        (processType == 'FULL') |\n        (F.col(\"c.ModifiedDate\") >= F.lit(dataDate)) |\n        (F.col(\"i.ContactDate\") >= F.lit(dataDate))\n    )\n    .groupBy(\n        \"c.CustomerID\", \"c.FirstName\", \"c.LastName\", \"c.Email\", \"c.Phone\",\n        \"a.StreetAddress\", \"a.PostalCode\", \"a.City\", \"a.State\", \"a.Country\",\n        \"c.CustomerType\", \"c.AccountManagerID\", \"c.CreatedDate\", \"c.ModifiedDate\",\n        \"c.Status\"\n    )\n    .agg(F.max(\"i.ContactDate\").alias(\"LastContactDate\"))\n    .select(\n        F.col(\"CustomerID\"),\n        F.concat_ws(\" \", F.col(\"FirstName\"), F.col(\"LastName\")).alias(\"CustomerName\"),\n        F.col(\"Email\"),\n        F.col(\"Phone\"),\n        F.col(\"StreetAddress\").alias(\"Address\"),\n        F.col(\"PostalCode\"),\n        F.col(\"City\"),\n        F.col(\"State\"),\n        F.col(\"Country\"),\n        F.col(\"CustomerType\"),\n        F.col(\"AccountManagerID\").alias(\"AccountManager\"),\n        F.col(\"CreatedDate\"),\n        F.col(\"ModifiedDate\"),\n        (F.col(\"Status\") == \"Active\").cast(\"boolean\").alias(\"IsActive\"),\n        F.col(\"LastContactDate\")\n    )\n)\n\n# -------------------------------\n# Step 2: Transaction Summary\n# -------------------------------\n# Calculate order metrics, top category/product, and return rate\n\n# (a) Base order aggregation\norderSummaryDF = (\n    ordersBronzeDF.alias(\"o\")\n    .join(orderDetailsBronzeDF.alias(\"od\"), \"OrderID\")\n    .where(F.col(\"o.Status\") == \"Completed\")\n    .groupBy(\"o.CustomerID\")\n    .agg(\n        F.countDistinct(\"o.OrderID\").alias(\"TotalOrders\"),\n        F.sum(F.col(\"od.Quantity\") * F.col(\"od.UnitPrice\") * (1 - F.col(\"od.Discount\"))).alias(\"TotalSpent\"),\n        F.min(\"o.OrderDate\").alias(\"FirstPurchaseDate\"),\n        F.max(\"o.OrderDate\").alias(\"LastPurchaseDate\")\n    )\n)\n\n# (b) Join with CustomerBase\ntransactionSummaryDF = (\n    orderSummaryDF.alias(\"os\")\n    .join(\n        customerBaseDF.select(\"CustomerID\").alias(\"cb\"),\n        F.col(\"os.CustomerID\") == F.col(\"cb.CustomerID\"),\n        \"inner\"\n    )\n    .select(\n        F.col(\"os.CustomerID\"),\n        F.col(\"os.TotalOrders\"),\n        F.col(\"os.TotalSpent\"),\n        F.when(F.col(\"os.TotalOrders\") > 0, F.col(\"os.TotalSpent\") / F.col(\"os.TotalOrders\"))\n         .otherwise(F.lit(0.0)).alias(\"AvgOrderValue\"),\n        F.col(\"os.FirstPurchaseDate\"),\n        F.col(\"os.LastPurchaseDate\"),\n        F.datediff(F.lit(dataDate), F.col(\"os.LastPurchaseDate\")).alias(\"DaysSinceLastPurchase\")\n    )\n)\n\n# (c) Identify top category\ncatSpendDF = (\n    ordersBronzeDF.alias(\"o\")\n    .join(orderDetailsBronzeDF.alias(\"od\"), \"OrderID\")\n    .join(productsBronzeDF.alias(\"p\"), \"ProductID\")\n    .where(F.col(\"o.Status\") == \"Completed\")\n    .groupBy(\"o.CustomerID\", \"p.Category\")\n    .agg(F.sum(F.col(\"od.Quantity\") * F.col(\"od.UnitPrice\") * (1 - F.col(\"od.Discount\"))).alias(\"CategorySpend\"))\n)\n\ncatWindow = Window.partitionBy(\"CustomerID\").orderBy(F.col(\"CategorySpend\").desc())\ncatTopDF = catSpendDF.withColumn(\"CategoryRank\", F.row_number().over(catWindow)).filter(F.col(\"CategoryRank\") == 1)\n\n# (d) Identify top product\nprodQtyDF = (\n    ordersBronzeDF.alias(\"o\")\n    .join(orderDetailsBronzeDF.alias(\"od\"), \"OrderID\")\n    .join(productsBronzeDF.alias(\"p\"), \"ProductID\")\n    .where(F.col(\"o.Status\") == \"Completed\")\n    .groupBy(\"o.CustomerID\", \"p.ProductName\")\n    .agg(F.sum(\"od.Quantity\").alias(\"TotalQuantity\"))\n)\n\nprodWindow = Window.partitionBy(\"CustomerID\").orderBy(F.col(\"TotalQuantity\").desc())\nprodTopDF = prodQtyDF.withColumn(\"ProductRank\", F.row_number().over(prodWindow)).filter(F.col(\"ProductRank\") == 1)\n\n# (e) Calculate return rate\nreturnsGroupedDF = (\n    returnsBronzeDF.groupBy(\"CustomerID\")\n    .agg(F.countDistinct(\"ReturnID\").alias(\"TotalReturns\"))\n)\n\n# (f) Combine partial metrics\ntransactionSummaryDF = (\n    transactionSummaryDF.alias(\"ts\")\n    .join(catTopDF.alias(\"cat\"), \"CustomerID\", \"left\")\n    .join(prodTopDF.alias(\"prod\"), \"CustomerID\", \"left\")\n    .join(returnsGroupedDF.alias(\"ret\"), \"CustomerID\", \"left\")\n    .select(\n        F.col(\"ts.CustomerID\"),\n        F.col(\"ts.TotalOrders\"),\n        F.col(\"ts.TotalSpent\"),\n        F.col(\"ts.AvgOrderValue\"),\n        F.col(\"ts.FirstPurchaseDate\"),\n        F.col(\"ts.LastPurchaseDate\"),\n        F.col(\"ts.DaysSinceLastPurchase\"),\n        F.col(\"cat.Category\").alias(\"TopCategory\"),\n        F.col(\"prod.ProductName\").alias(\"TopProduct\"),\n        F.when(F.col(\"ts.TotalOrders\") > 0,\n               (F.col(\"ret.TotalReturns\") / F.col(\"ts.TotalOrders\")) * 100\n              ).otherwise(F.lit(0.0)).alias(\"ReturnRate\")\n    )\n)\n\n# -------------------------------\n# Step 3: Customer Metrics\n# -------------------------------\n# RFM scoring, churn probability, next purchase propensity, loyalty index\n\n# (a) RFM scoring with NTILE(5)\nwindowRecency = Window.orderBy(F.col(\"DaysSinceLastPurchase\").asc())\nwindowFrequency = Window.orderBy(F.col(\"TotalOrders\").desc())\nwindowMonetary = Window.orderBy(F.col(\"TotalSpent\").desc())\n\ntransactionSummaryDF = (\n    transactionSummaryDF\n    .withColumn(\"RecencyScore\", F.ntile(5).over(windowRecency))\n    .withColumn(\"FrequencyScore\", F.ntile(5).over(windowFrequency))\n    .withColumn(\"MonetaryScore\", F.ntile(5).over(windowMonetary))\n)\n\nrfmDF = transactionSummaryDF.select(\n    \"CustomerID\",\n    \"RecencyScore\",\n    \"FrequencyScore\",\n    \"MonetaryScore\",\n    (F.col(\"RecencyScore\") + F.col(\"FrequencyScore\") + F.col(\"MonetaryScore\")).alias(\"RFMScore\")\n)\n\n# (b) Churn Probability & Next Purchase Propensity\nchurnDF = (\n    transactionSummaryDF.select(\n        \"CustomerID\",\n        F.when(F.col(\"DaysSinceLastPurchase\") > churnDaysVeryHigh, 0.8)\n         .when(F.col(\"DaysSinceLastPurchase\") > churnDaysHigh, 0.5)\n         .when(F.col(\"DaysSinceLastPurchase\") > churnDaysMedium, 0.3)\n         .when(F.col(\"DaysSinceLastPurchase\") > churnDaysLow, 0.1)\n         .otherwise(0.05).alias(\"ChurnProbability\"),\n        F.when((F.col(\"DaysSinceLastPurchase\") < churnDaysLow) & (F.col(\"TotalOrders\") > 3), 0.8)\n         .when((F.col(\"DaysSinceLastPurchase\") < churnDaysMedium) & (F.col(\"TotalOrders\") > 2), 0.6)\n         .when(F.col(\"DaysSinceLastPurchase\") < churnDaysHigh, 0.4)\n         .otherwise(0.2).alias(\"NextPurchasePropensity\"),\n        (\n            F.when(\n                (F.col(\"TotalOrders\") >= loyaltyThreshold) & (F.col(\"ReturnRate\") < 10),\n                (F.col(\"TotalOrders\") * 0.6) + ((100 - F.col(\"ReturnRate\")) * 0.4)\n            )\n            .otherwise(F.col(\"TotalOrders\") * 0.4)\n            / 10\n        ).alias(\"LoyaltyIndex\")\n    )\n)\n\n# (c) Combine RFM & churn info into #CustomerMetrics\ncustomerMetricsDF = (\n    rfmDF.alias(\"r\")\n    .join(transactionSummaryDF.alias(\"t\"), \"CustomerID\")\n    .join(churnDF.alias(\"ch\"), \"CustomerID\")\n    .select(\n        F.col(\"r.CustomerID\"),\n        (F.col(\"t.TotalSpent\") * (1 + (1 - F.col(\"ch.ChurnProbability\")))).alias(\"CustomerLifetimeValue\"),\n        \"r.RecencyScore\",\n        \"r.FrequencyScore\",\n        \"r.MonetaryScore\",\n        \"r.RFMScore\",\n        \"ch.ChurnProbability\",\n        \"ch.NextPurchasePropensity\",\n        \"ch.LoyaltyIndex\",\n        F.when(F.col(\"ch.ChurnProbability\") > 0.7, \"At Risk\")\n         .when(F.col(\"ch.LoyaltyIndex\") > 0.8, \"Excellent\")\n         .when(F.col(\"ch.LoyaltyIndex\") > 0.6, \"Good\")\n         .when(F.col(\"ch.LoyaltyIndex\") > 0.4, \"Average\")\n         .otherwise(\"Needs Attention\").alias(\"CustomerHealth\")\n    )\n)\n\n# -------------------------------\n# Step 4: Customer Segmentation\n# -------------------------------\nsegmentationDF = (\n    customerBaseDF.alias(\"cb\")\n    .join(customerMetricsDF.alias(\"cm\"), \"CustomerID\")\n    .join(transactionSummaryDF.alias(\"ts\"), \"CustomerID\")\n    .select(\n        F.col(\"cb.CustomerID\"),\n        # ValueSegment\n        F.when(F.col(\"cm.CustomerLifetimeValue\") >= highValueThreshold, \"High Value\")\n         .when(F.col(\"cm.CustomerLifetimeValue\") >= highValueThreshold*0.5, \"Medium Value\")\n         .otherwise(\"Low Value\").alias(\"ValueSegment\"),\n        # BehaviorSegment\n        F.when(F.col(\"cm.RFMScore\") >= 13, \"Champions\")\n         .when((F.col(\"cm.RecencyScore\") >= 4) & (F.col(\"cm.FrequencyScore\") >= 3), \"Loyal Customers\")\n         .when((F.col(\"cm.RecencyScore\") >= 4) & (F.col(\"cm.FrequencyScore\") <= 2), \"Potential Loyalists\")\n         .when((F.col(\"cm.RecencyScore\") <= 2) & (F.col(\"cm.FrequencyScore\") >= 3) & (F.col(\"cm.MonetaryScore\") >= 3), \"At Risk\")\n         .when((F.col(\"cm.RecencyScore\") <= 2) & (F.col(\"cm.FrequencyScore\") <= 2) & (F.col(\"cm.MonetaryScore\") <= 2), \"Hibernating\")\n         .when(F.col(\"cm.RecencyScore\") <= 1, \"Lost\")\n         .otherwise(\"Others\").alias(\"BehaviorSegment\"),\n        # LifecycleSegment\n        F.when((F.col(\"ts.TotalOrders\") == 1) & (F.col(\"ts.DaysSinceLastPurchase\") <= 30), \"New Customer\")\n         .when((F.col(\"ts.TotalOrders\") > 1) & (F.col(\"cm.ChurnProbability\") < 0.3), \"Active\")\n         .when((F.col(\"cm.ChurnProbability\") >= 0.3) & (F.col(\"cm.ChurnProbability\") < 0.7), \"At Risk\")\n         .when(F.col(\"cm.ChurnProbability\") >= 0.7, \"Churned\")\n         .otherwise(\"Inactive\").alias(\"LifecycleSegment\"),\n        # TargetGroup\n        F.when((F.col(\"cm.CustomerLifetimeValue\") >= highValueThreshold) & (F.col(\"cm.ChurnProbability\") < 0.3), \"VIP\")\n         .when((F.col(\"cm.CustomerLifetimeValue\") >= highValueThreshold*0.5) & (F.col(\"cm.ChurnProbability\") >= 0.3), \"Retention Priority\")\n         .when((F.col(\"ts.TotalOrders\") == 1) & (F.col(\"ts.DaysSinceLastPurchase\") <= 30), \"Nurture New\")\n         .when(F.col(\"cm.NextPurchasePropensity\") > 0.6, \"Growth Opportunity\")\n         .when(F.col(\"ts.ReturnRate\") > 20, \"Service Improvement\")\n         .otherwise(\"Standard\").alias(\"TargetGroup\"),\n        # MarketingRecommendation\n        F.when(\n            (F.col(\"cm.CustomerLifetimeValue\") >= highValueThreshold) & (F.col(\"cm.ChurnProbability\") < 0.3),\n            \"Exclusive offers, VIP events, Personal shopping assistance, Early access to new products\"\n        ).when(\n            (F.col(\"cm.CustomerLifetimeValue\") >= (highValueThreshold*0.5)) & (F.col(\"cm.ChurnProbability\") >= 0.3),\n            \"Re-engagement campaign, Loyalty rewards, Personalized recommendations based on past purchases\"\n        ).when(\n            (F.col(\"ts.TotalOrders\") == 1) & (F.col(\"ts.DaysSinceLastPurchase\") <= 30),\n            \"Welcome series, Educational content, First purchase follow-up, Category exploration\"\n        ).when(\n            F.col(\"cm.NextPurchasePropensity\") > 0.6,\n            F.concat(\n                F.lit(\"Cross-sell/upsell, Bundle offers based on \"),\n                F.coalesce(F.col(\"ts.TopCategory\"), F.lit(\"preferred category\")),\n                F.lit(\", Category expansion\")\n            )\n        ).when(\n            F.col(\"ts.ReturnRate\") > 20,\n            \"Satisfaction survey, Improved product information, Size/fit guides, Service recovery\"\n        ).otherwise(\n            F.concat(\n                F.lit(\"Standard seasonal promotions, Category newsletters, Reactivation after \"),\n                (F.col(\"ts.DaysSinceLastPurchase\") / 30).cast(\"int\"),\n                F.lit(\" months\")\n            )\n        ).alias(\"MarketingRecommendation\")\n    )\n)\n\n# Write S I L V E R Tables (Delta format)\ncustomerBaseDF.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"lakehouse_silver.CustomerBase\")\ntransactionSummaryDF.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"lakehouse_silver.TransactionSummary\")\ncustomerMetricsDF.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"lakehouse_silver.CustomerMetrics\")\nsegmentationDF.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"lakehouse_silver.CustomerSegments\")\n\n--------------------------------------------------------------------------------\n# GOLD LAYER: Upserts & Summaries\n--------------------------------------------------------------------------------\n\n# 1) CustomerProfiles table (like dbo.CustomerProfiles)\ncustomerProfileGoldDF = (\n    customerBaseDF.alias(\"cb\")\n    .join(transactionSummaryDF.alias(\"ts\"), \"CustomerID\", \"left\")\n    .select(\n        \"cb.CustomerID\",\n        \"cb.CustomerName\",\n        \"cb.Email\",\n        \"cb.Phone\",\n        \"cb.Address\",\n        \"cb.PostalCode\",\n        \"cb.City\",\n        \"cb.State\",\n        \"cb.Country\",\n        \"cb.CustomerType\",\n        \"cb.AccountManager\",\n        \"cb.CreatedDate\",\n        \"cb.IsActive\",\n        \"cb.LastContactDate\",\n        \"ts.TotalOrders\",\n        \"ts.TotalSpent\",\n        \"ts.AvgOrderValue\",\n        \"ts.FirstPurchaseDate\",\n        \"ts.LastPurchaseDate\",\n        \"ts.DaysSinceLastPurchase\",\n        \"ts.TopCategory\",\n        \"ts.TopProduct\",\n        \"ts.ReturnRate\",\n        F.current_timestamp().alias(\"ModifiedDate\")\n    )\n)\n\n# In production, consider Delta MERGE for incremental updates instead of overwrite\n(\n    customerProfileGoldDF\n    .write\n    .format(\"delta\")\n    .mode(\"overwrite\")\n    .saveAsTable(\"lakehouse_gold.CustomerProfiles\")\n)\n\n# 2) Append daily analytics to CustomerAnalytics table\ncustomerAnalyticsGoldDF = (\n    customerMetricsDF.alias(\"cm\")\n    .join(segmentationDF.alias(\"cs\"), \"CustomerID\")\n    .select(\n        F.lit(dataDate).alias(\"ProcessDate\"),\n        F.col(\"cm.CustomerID\"),\n        F.col(\"cm.CustomerLifetimeValue\"),\n        F.col(\"cm.RecencyScore\"),\n        F.col(\"cm.FrequencyScore\"),\n        F.col(\"cm.MonetaryScore\"),\n        F.col(\"cm.RFMScore\"),\n        F.col(\"cm.ChurnProbability\"),\n        F.col(\"cm.NextPurchasePropensity\"),\n        F.col(\"cm.LoyaltyIndex\"),\n        F.col(\"cm.CustomerHealth\"),\n        F.col(\"cs.ValueSegment\"),\n        F.col(\"cs.BehaviorSegment\"),\n        F.col(\"cs.LifecycleSegment\"),\n        F.col(\"cs.TargetGroup\"),\n        F.col(\"cs.MarketingRecommendation\")\n    )\n)\n\n(\n    customerAnalyticsGoldDF\n    .write\n    .format(\"delta\")\n    .mode(\"append\")\n    .saveAsTable(\"lakehouse_gold.CustomerAnalytics\")\n)\n\n# Apply retention / cleanup older data\nspark.sql(f\"\"\"\n  DELETE FROM lakehouse_gold.CustomerAnalytics\n  WHERE ProcessDate < DATEADD(DAY, -{retentionPeriodDays}, '{str(dataDate)}')\n\"\"\")\n\n# 3) CustomerAnalyticsSummary\n# Summarize daily metrics (active, churned, average LTV, etc.)\nfrom pyspark.sql.window import Window\n\nsummaryWindow = Window.partitionBy()\nsummaryDF = (\n    customerBaseDF.alias(\"cb\")\n    .join(customerMetricsDF.alias(\"cm\"), \"CustomerID\")\n    .join(segmentationDF.alias(\"cs\"), \"CustomerID\")\n    .select(\n        F.lit(dataDate).alias(\"ProcessDate\"),\n        F.lit(processType).alias(\"ProcessType\"),\n        F.count_distinct(\"cb.CustomerID\").over(summaryWindow).alias(\"TotalCustomers\"),\n        F.sum(F.when(F.col(\"cs.LifecycleSegment\") == \"Active\", 1).otherwise(0)).over(summaryWindow).alias(\"ActiveCustomers\"),\n        F.sum(F.when(F.col(\"cs.LifecycleSegment\") == \"New Customer\", 1).otherwise(0)).over(summaryWindow).alias(\"NewCustomers\"),\n        F.sum(F.when((F.col(\"cs.LifecycleSegment\") == \"At Risk\") | (F.col(\"cs.LifecycleSegment\") == \"Churned\"), 1).otherwise(0)).over(summaryWindow).alias(\"ChurningCustomers\"),\n        F.sum(F.when(F.col(\"cs.ValueSegment\") == \"High Value\", 1).otherwise(0)).over(summaryWindow).alias(\"HighValueCustomers\"),\n        F.avg(\"cm.CustomerLifetimeValue\").over(summaryWindow).alias(\"AverageLifetimeValue\"),\n        # Approx total processing time in ms (for reference)\n        (F.unix_timestamp(F.current_timestamp()) - F.unix_timestamp(F.lit(str(dataDate)))) * 1000,\n        F.lit(\"Success\").alias(\"ProcessingStatus\")\n    )\n).distinct()\n\n(\n    summaryDF\n    .write\n    .format(\"delta\")\n    .mode(\"append\")\n    .saveAsTable(\"lakehouse_gold.CustomerAnalyticsSummary\")\n)\n\n# Optional debug logs\nif debugMode:\n    summaryDF.show(truncate=False)\n\n# End of PySpark pipeline\n\n─────────────────────────────────────────────────────────────────────────\n3) IMPLEMENTATION GUIDELINES & BEST PRACTICES (TechLead)\n─────────────────────────────────────────────────────────────────────────\n\n1. Parameter Management:  \n   • Store these thresholds (churnDays, highValueThreshold, retentionPeriodDays, etc.) in a config table or external file rather than hard-coding.\n\n2. Use Delta Lake:  \n   • Delta merges allow efficient upserts, ACID transactions, and time-travel for improved auditing.  \n   • Partition large tables by date or region if data volume is significant.\n\n3. Incremental Load Strategy:  \n   • For large datasets, rely on processType = 'INCREMENTAL' to filter on ModifiedDate or OrderDate ranges.  \n   • In the code, adapt “where (ModifiedDate >= dataDate)” logic or use Delta Lake’s built-in change data feed (if enabled).\n\n4. Error Handling & Logging:  \n   • In production, wrap each major transformation in try/except or rely on orchestration to catch errors.  \n   • Maintain a separate “ProcessLog” table or use pipeline logs to track step timings and row counts.\n\n5. Performance Tuning:  \n   • Broadcast small dimension data or cache repeated joins.  \n   • Use an appropriately sized Spark cluster.  \n   • Avoid overly broad “FULL” loads when incremental logic suffices.\n\n6. CI/CD & DevOps:  \n   • Version your PySpark notebooks/scripts in Git.  \n   • Automate unit testing and integration testing in a staging environment.  \n   • Deploy changes to production after validated builds.\n\n─────────────────────────────────────────────────────────────────────────\n4) TEST CASES FOR VALIDATION (TestingAgent)\n─────────────────────────────────────────────────────────────────────────\n\nBelow are recommended test scenarios to ensure the pipeline’s correctness and robustness:\n\n1. Full Load with Known Dataset  \n   • Include a varied dataset: new/active/churned/high-value customers.  \n   • Verify final Gold tables match expected calculations (RFM scores, churn rates, segments, summary counts).\n\n2. Incremental Load Test  \n   • Insert/Update some records after the dataDate.  \n   • Confirm only those changed and new records appear in transactionSummaryDF, CustomerAnalytics, etc.\n\n3. Parameter Variation  \n   • Adjust churnDays (e.g., churnDaysVeryHigh=200) or highValueThreshold=10000.  \n   • Verify that churnProbability, ValueSegment, etc. update as expected.\n\n4. Edge Cases (No Orders/Interactions)  \n   • Include customers with zero orders or interactions.  \n   • Ensure their records appear with zero-based or null-based transaction metrics and appropriate segments.\n\n5. Retention Enforcement  \n   • Provide data older than retentionPeriodDays (default 365).  \n   • After the pipeline, verify that old CustomerAnalytics records are deleted from the Gold table.\n\n6. Error Handling  \n   • Introduce invalid references (e.g., nonexistent ProductID) or break a join.  \n   • Expect the pipeline to fail gracefully or log errors without silently corrupting the data.\n\n─────────────────────────────────────────────────────────────────────────\nS U M M A R Y\n─────────────────────────────────────────────────────────────────────────\n\nBy adopting this Medallion approach in Microsoft Fabric (Bronze → Silver → Gold), the original stored procedure’s logic is refactored into modular PySpark transformations. This yields:\n\n• Improved maintainability (clear separation of raw vs. curated vs. final analytics).  \n• Scalability for large datasets (Delta optimizations, partitioning, incremental loads).  \n• Enhanced observability with explicit tables for each stage, flexible debugging, and logging.  \n• Ongoing agility to evolve thresholds, churn logic, or segmentation criteria without rewriting a huge stored procedure.  \n\nUse the above plan, code, and test suite to confidently migrate from SQL Stored Procedure logic to a modern PySpark pipeline on Microsoft Fabric.",
    "role": "user",
    "name": "ProductOwner"
  },
  {
    "content": "Below is a complete answer addressing all roles (BusinessAnalyst, ProductOwner, AzureDataEngineer, TechLead, TestingAgent). It includes:\n\n1) Business Analysis and Migration Plan  \n2) PySpark Code for Bronze (Raw), Stage 1 (Base Data), Stage 2 (Advanced Analytics), and Gold (Final) layers  \n3) Implementation Guidelines and Best Practices  \n4) Test Cases to Validate the Migration  \n\n────────────────────────────────────────────────────────────────────────\n1) BUSINESS ANALYSIS & MIGRATION PLAN\n────────────────────────────────────────────────────────────────────────\n(Combined perspective of BusinessAnalyst & ProductOwner)\n\nA. Business Purpose and Requirements  \n• The original SQL stored procedure ([dbo].[usp_CustomerAnalytics_Processing]) calculates customer-related analytics. It produces RFM (Recency, Frequency, Monetary) metrics, churn probability, segmentation (ValueSegment, LifecycleSegment, etc.), and daily summary statistics.  \n• It supports marketing, sales, and executive stakeholders by highlighting customer health, loyalty, and potential churn.  \n\nB. Data Sources and Targets  \n• Source:  \n  – Customers, CustomerAddresses, CustomerInteractions (customer data)  \n  – Orders, OrderDetails, Products, Returns (transaction data)  \n• Destination (Target Tables):  \n  – CustomerProfiles (upserted)  \n  – CustomerAnalytics (daily appended snapshot)  \n  – CustomerAnalyticsSummary (daily aggregates and retention logic)  \n\nC. Core Logic Extracted from the Stored Procedure  \n• Build a “Customer Base” that filters on active or recently updated customers.  \n• Summarize transactions (order counts, total spend, top category/product, returns).  \n• Calculate advanced metrics (RFM, churn probability, loyalty index).  \n• Segmentation by value, behavior, and lifecycle, plus marketing recommendations.  \n• Insert daily records into analytics tables and generate summary logs.  \n• Enforce a retention period to remove older data.  \n\nD. Medallion Architecture (Bronze–Silver–Gold in Microsoft Fabric)  \n1. Bronze: Ingest raw data (Customers, Orders, etc.) with minimal transformations.  \n2. Silver (Stages 1 & 2):  \n   – Stage 1: Create base data sets (#CustomerBase, #TransactionSummary).  \n   – Stage 2: Perform advanced calculations (#CustomerMetrics, #CustomerSegments).  \n3. Gold: Produce final consolidated tables (CustomerProfiles, CustomerAnalytics) and daily summary (CustomerAnalyticsSummary).  \n\nE. Key Parameters & Considerations  \n• dataDate (defaults to yesterday), processType (FULL or INCREMENTAL), debugMode, retentionPeriodDays, churn thresholds (30, 60, 90, 180 days), and highValueThreshold (e.g., 5000).  \n• Use Delta format in Fabric Lakehouse for ACID merges and versioning.  \n• Emulate the stored procedure’s incremental logic where feasible, but allow a “full” mode for reprocessing.  \n\n────────────────────────────────────────────────────────────────────────\n2) PYS PARK CODE\n────────────────────────────────────────────────────────────────────────\n(Role: AzureDataEngineer)\n\nBelow are the distinct code blocks that reflect each stage in the Medallion flow.  \nMake sure you have an active Spark session and have set up your table references in Microsoft Fabric.  \nAdjust table/database names as needed.\n\n------------------------------------------------------------------------\n(1) BRONZE LAYER\n------------------------------------------------------------------------\n```python\n# BRONZE LAYER START\n# -------------------------------------------------------------\n# This stage reads raw data from the source or raw tables.\n# In Microsoft Fabric, these might be Lakehouse bronze tables.\n\nfrom pyspark.sql import SparkSession, functions as F, Window\nimport datetime\n\nspark = SparkSession.builder.getOrCreate()\n\n# Example parameter defaults\ndataDate = None\nprocessType = 'FULL'\ndebugMode = False\nretentionPeriodDays = 365\nhighValueThreshold = 5000.0\nloyaltyThreshold = 5\nchurnDaysVeryHigh = 180\nchurnDaysHigh = 90\nchurnDaysMedium = 60\nchurnDaysLow = 30\n\n# Set dataDate to yesterday if not provided\nif dataDate is None:\n    dataDate = datetime.date.today() - datetime.timedelta(days=1)\n\n# Read raw (bronze) tables\ncustomersBronzeDF = spark.read.table(\"lakehouse_raw.Customers\")\naddressesBronzeDF = spark.read.table(\"lakehouse_raw.CustomerAddresses\")\ninteractionsBronzeDF = spark.read.table(\"lakehouse_raw.CustomerInteractions\")\nordersBronzeDF = spark.read.table(\"lakehouse_raw.Orders\")\norderDetailsBronzeDF = spark.read.table(\"lakehouse_raw.OrderDetails\")\nproductsBronzeDF = spark.read.table(\"lakehouse_raw.Products\")\nreturnsBronzeDF = spark.read.table(\"lakehouse_raw.Returns\")\n\n# BRONZE LAYER END\n```\n\n------------------------------------------------------------------------\n(2) STAGE 1: BASE DATA\n------------------------------------------------------------------------\n```python\n# STAGE 1: BASE DATA START\n# -------------------------------------------------------------\n# This stage creates intermediate DataFrames replicating #CustomerBase\n# and #TransactionSummary from the SQL procedure.\n\n# 1) Create #CustomerBase equivalent\ncustomerBaseDF = (\n    customersBronzeDF.alias(\"c\")\n    .join(\n        addressesBronzeDF.alias(\"a\"),\n        (F.col(\"c.CustomerID\") == F.col(\"a.CustomerID\")) & (F.col(\"a.AddressType\") == \"Primary\"),\n        \"left\"\n    )\n    .join(\n        interactionsBronzeDF.alias(\"i\"),\n        F.col(\"c.CustomerID\") == F.col(\"i.CustomerID\"),\n        \"left\"\n    )\n    .where(\n        (processType == 'FULL')\n        | (F.col(\"c.ModifiedDate\") >= F.lit(dataDate))\n        | (F.col(\"i.ContactDate\") >= F.lit(dataDate))\n    )\n    .groupBy(\n        \"c.CustomerID\",\n        \"c.FirstName\",\n        \"c.LastName\",\n        \"c.Email\",\n        \"c.Phone\",\n        \"a.StreetAddress\",\n        \"a.PostalCode\",\n        \"a.City\",\n        \"a.State\",\n        \"a.Country\",\n        \"c.CustomerType\",\n        \"c.AccountManagerID\",\n        \"c.CreatedDate\",\n        \"c.ModifiedDate\",\n        \"c.Status\"\n    )\n    .agg(F.max(\"i.ContactDate\").alias(\"LastContactDate\"))\n    .select(\n        F.col(\"CustomerID\"),\n        F.concat_ws(\" \", F.col(\"FirstName\"), F.col(\"LastName\")).alias(\"CustomerName\"),\n        F.col(\"Email\"),\n        F.col(\"Phone\"),\n        F.col(\"StreetAddress\").alias(\"Address\"),\n        F.col(\"PostalCode\"),\n        F.col(\"City\"),\n        F.col(\"State\"),\n        F.col(\"Country\"),\n        F.col(\"CustomerType\"),\n        F.col(\"AccountManagerID\").alias(\"AccountManager\"),\n        F.col(\"CreatedDate\"),\n        F.col(\"ModifiedDate\"),\n        (F.col(\"Status\") == \"Active\").cast(\"boolean\").alias(\"IsActive\"),\n        F.col(\"LastContactDate\")\n    )\n)\n\n# 2) Create #TransactionSummary equivalent\n# 2a) Order Summaries\norderSummaryDF = (\n    ordersBronzeDF.alias(\"o\")\n    .join(orderDetailsBronzeDF.alias(\"od\"), \"OrderID\")\n    .where(F.col(\"o.Status\") == \"Completed\")\n    .groupBy(\"o.CustomerID\")\n    .agg(\n        F.countDistinct(\"o.OrderID\").alias(\"TotalOrders\"),\n        F.sum(F.col(\"od.Quantity\") * F.col(\"od.UnitPrice\") * (1 - F.col(\"od.Discount\"))).alias(\"TotalSpent\"),\n        F.min(\"o.OrderDate\").alias(\"FirstPurchaseDate\"),\n        F.max(\"o.OrderDate\").alias(\"LastPurchaseDate\")\n    )\n)\n\n# 2b) Join with #CustomerBase subset\ntransactionSummaryDF = (\n    orderSummaryDF.alias(\"os\")\n    .join(\n        customerBaseDF.select(\"CustomerID\").alias(\"cb\"),\n        F.col(\"os.CustomerID\") == F.col(\"cb.CustomerID\"),\n        \"inner\"\n    )\n    .select(\n        F.col(\"os.CustomerID\"),\n        F.col(\"os.TotalOrders\"),\n        F.col(\"os.TotalSpent\"),\n        F.when(F.col(\"os.TotalOrders\") > 0, F.col(\"os.TotalSpent\") / F.col(\"os.TotalOrders\"))\n         .otherwise(F.lit(0.0)).alias(\"AvgOrderValue\"),\n        F.col(\"os.FirstPurchaseDate\"),\n        F.col(\"os.LastPurchaseDate\"),\n        F.datediff(F.lit(dataDate), F.col(\"os.LastPurchaseDate\")).alias(\"DaysSinceLastPurchase\")\n    )\n)\n\n# STAGE 1: BASE DATA END\n```\n\n------------------------------------------------------------------------\n(3) STAGE 2: ADVANCED ANALYTICS\n------------------------------------------------------------------------\n```python\n# STAGE 2: ADVANCED ANALYTICS START\n# -------------------------------------------------------------\n# This stage computes advanced analytics: RFM, churn probability,\n# next purchase propensity, loyalty index, and segmentation.\n# Equivalent to #CustomerMetrics and #CustomerSegments.\n\n# 3a) Identify top category & product, plus return rates, finishing #TransactionSummary logic\nfrom pyspark.sql.window import Window\n\n# Top category\ncategorySpendDF = (\n    ordersBronzeDF.alias(\"o\")\n    .join(orderDetailsBronzeDF.alias(\"od\"), \"OrderID\")\n    .join(productsBronzeDF.alias(\"p\"), \"ProductID\")\n    .where(F.col(\"o.Status\") == \"Completed\")\n    .groupBy(\"o.CustomerID\", \"p.Category\")\n    .agg(F.sum(F.col(\"od.Quantity\") * F.col(\"od.UnitPrice\") * (1 - F.col(\"od.Discount\"))).alias(\"CategorySpend\"))\n)\n\ncatWindow = Window.partitionBy(\"CustomerID\").orderBy(F.col(\"CategorySpend\").desc())\ncatTopDF = categorySpendDF.withColumn(\"CategoryRank\", F.row_number().over(catWindow)).filter(F.col(\"CategoryRank\") == 1)\n\n# Top product\nproductQtyDF = (\n    ordersBronzeDF.alias(\"o\")\n    .join(orderDetailsBronzeDF.alias(\"od\"), \"OrderID\")\n    .join(productsBronzeDF.alias(\"p\"), \"ProductID\")\n    .where(F.col(\"o.Status\") == \"Completed\")\n    .groupBy(\"o.CustomerID\", \"p.ProductName\")\n    .agg(F.sum(\"od.Quantity\").alias(\"TotalQuantity\"))\n)\n\nprodWindow = Window.partitionBy(\"CustomerID\").orderBy(F.col(\"TotalQuantity\").desc())\nprodTopDF = productQtyDF.withColumn(\"ProductRank\", F.row_number().over(prodWindow)).filter(F.col(\"ProductRank\") == 1)\n\n# Return rate\nreturnsGroupedDF = (\n    returnsBronzeDF.groupBy(\"CustomerID\")\n    .agg(F.countDistinct(\"ReturnID\").alias(\"TotalReturns\"))\n)\n\n# Complete the transactionSummaryDF with top category, product, and return rate\ntransactionSummaryDF = (\n    transactionSummaryDF.alias(\"ts\")\n    .join(catTopDF.alias(\"cat\"), \"CustomerID\", \"left\")\n    .join(prodTopDF.alias(\"prod\"), \"CustomerID\", \"left\")\n    .join(returnsGroupedDF.alias(\"ret\"), \"CustomerID\", \"left\")\n    .select(\n        F.col(\"ts.CustomerID\"),\n        F.col(\"ts.TotalOrders\"),\n        F.col(\"ts.TotalSpent\"),\n        F.col(\"ts.AvgOrderValue\"),\n        F.col(\"ts.FirstPurchaseDate\"),\n        F.col(\"ts.LastPurchaseDate\"),\n        F.col(\"ts.DaysSinceLastPurchase\"),\n        F.col(\"cat.Category\").alias(\"TopCategory\"),\n        F.col(\"prod.ProductName\").alias(\"TopProduct\"),\n        F.when(F.col(\"ts.TotalOrders\") > 0, (F.col(\"ret.TotalReturns\") / F.col(\"ts.TotalOrders\")) * 100)\n         .otherwise(F.lit(0.0)).alias(\"ReturnRate\")\n    )\n)\n\n# 3b) Calculate RFM (Recency, Frequency, Monetary) scores\nwindowRecency = Window.orderBy(F.col(\"DaysSinceLastPurchase\").asc())\nwindowFrequency = Window.orderBy(F.col(\"TotalOrders\").desc())\nwindowMonetary = Window.orderBy(F.col(\"TotalSpent\").desc())\n\ntransactionSummaryDF = (\n    transactionSummaryDF\n    .withColumn(\"RecencyScore\", F.ntile(5).over(windowRecency))\n    .withColumn(\"FrequencyScore\", F.ntile(5).over(windowFrequency))\n    .withColumn(\"MonetaryScore\", F.ntile(5).over(windowMonetary))\n)\n\nrfmDF = transactionSummaryDF.select(\n    \"CustomerID\",\n    \"RecencyScore\",\n    \"FrequencyScore\",\n    \"MonetaryScore\",\n    (F.col(\"RecencyScore\") + F.col(\"FrequencyScore\") + F.col(\"MonetaryScore\")).alias(\"RFMScore\")\n)\n\n# 3c) Calculate churn probability, next purchase propensity, loyalty\nchurnDF = (\n    transactionSummaryDF.select(\n        \"CustomerID\",\n        F.when(F.col(\"DaysSinceLastPurchase\") > churnDaysVeryHigh, 0.8)\n         .when(F.col(\"DaysSinceLastPurchase\") > churnDaysHigh, 0.5)\n         .when(F.col(\"DaysSinceLastPurchase\") > churnDaysMedium, 0.3)\n         .when(F.col(\"DaysSinceLastPurchase\") > churnDaysLow, 0.1)\n         .otherwise(0.05).alias(\"ChurnProbability\"),\n        F.when((F.col(\"DaysSinceLastPurchase\") < churnDaysLow) & (F.col(\"TotalOrders\") > 3), 0.8)\n         .when((F.col(\"DaysSinceLastPurchase\") < churnDaysMedium) & (F.col(\"TotalOrders\") > 2), 0.6)\n         .when(F.col(\"DaysSinceLastPurchase\") < churnDaysHigh, 0.4)\n         .otherwise(0.2).alias(\"NextPurchasePropensity\"),\n        (\n            F.when(\n                (F.col(\"TotalOrders\") >= loyaltyThreshold) & (F.col(\"ReturnRate\") < 10),\n                (F.col(\"TotalOrders\") * 0.6) + ((100 - F.col(\"ReturnRate\")) * 0.4)\n            )\n            .otherwise(F.col(\"TotalOrders\") * 0.4)\n            / 10\n        ).alias(\"LoyaltyIndex\")\n    )\n)\n\ncustomerMetricsDF = (\n    rfmDF.alias(\"r\")\n    .join(transactionSummaryDF.alias(\"t\"), \"CustomerID\")\n    .join(churnDF.alias(\"ch\"), \"CustomerID\")\n    .select(\n        F.col(\"r.CustomerID\"),\n        (F.col(\"t.TotalSpent\") * (1 + (1 - F.col(\"ch.ChurnProbability\")))).alias(\"CustomerLifetimeValue\"),\n        \"r.RecencyScore\",\n        \"r.FrequencyScore\",\n        \"r.MonetaryScore\",\n        \"r.RFMScore\",\n        \"ch.ChurnProbability\",\n        \"ch.NextPurchasePropensity\",\n        \"ch.LoyaltyIndex\",\n        F.when(F.col(\"ch.ChurnProbability\") > 0.7, \"At Risk\")\n         .when(F.col(\"ch.LoyaltyIndex\") > 0.8, \"Excellent\")\n         .when(F.col(\"ch.LoyaltyIndex\") > 0.6, \"Good\")\n         .when(F.col(\"ch.LoyaltyIndex\") > 0.4, \"Average\")\n         .otherwise(\"Needs Attention\").alias(\"CustomerHealth\")\n    )\n)\n\n# 3d) Create customer segmentation\nsegmentationDF = (\n    customerBaseDF.alias(\"cb\")\n    .join(customerMetricsDF.alias(\"cm\"), \"CustomerID\")\n    .join(transactionSummaryDF.alias(\"ts\"), \"CustomerID\")\n    .select(\n        F.col(\"cb.CustomerID\"),\n        # ValueSegment\n        F.when(F.col(\"cm.CustomerLifetimeValue\") >= highValueThreshold, \"High Value\")\n         .when(F.col(\"cm.CustomerLifetimeValue\") >= highValueThreshold * 0.5, \"Medium Value\")\n         .otherwise(\"Low Value\").alias(\"ValueSegment\"),\n        # BehaviorSegment\n        F.when(F.col(\"cm.RFMScore\") >= 13, \"Champions\")\n         .when((F.col(\"cm.RecencyScore\") >= 4) & (F.col(\"cm.FrequencyScore\") >= 3), \"Loyal Customers\")\n         .when((F.col(\"cm.RecencyScore\") >= 4) & (F.col(\"cm.FrequencyScore\") <= 2), \"Potential Loyalists\")\n         .when((F.col(\"cm.RecencyScore\") <= 2) & (F.col(\"cm.FrequencyScore\") >= 3) & (F.col(\"cm.MonetaryScore\") >= 3), \"At Risk\")\n         .when((F.col(\"cm.RecencyScore\") <= 2) & (F.col(\"cm.FrequencyScore\") <= 2) & (F.col(\"cm.MonetaryScore\") <= 2), \"Hibernating\")\n         .when(F.col(\"cm.RecencyScore\") <= 1, \"Lost\")\n         .otherwise(\"Others\").alias(\"BehaviorSegment\"),\n        # LifecycleSegment\n        F.when((F.col(\"ts.TotalOrders\") == 1) & (F.col(\"ts.DaysSinceLastPurchase\") <= 30), \"New Customer\")\n         .when((F.col(\"ts.TotalOrders\") > 1) & (F.col(\"cm.ChurnProbability\") < 0.3), \"Active\")\n         .when((F.col(\"cm.ChurnProbability\") >= 0.3) & (F.col(\"cm.ChurnProbability\") < 0.7), \"At Risk\")\n         .when(F.col(\"cm.ChurnProbability\") >= 0.7, \"Churned\")\n         .otherwise(\"Inactive\").alias(\"LifecycleSegment\"),\n        # TargetGroup\n        F.when((F.col(\"cm.CustomerLifetimeValue\") >= highValueThreshold) & (F.col(\"cm.ChurnProbability\") < 0.3), \"VIP\")\n         .when((F.col(\"cm.CustomerLifetimeValue\") >= highValueThreshold * 0.5) & (F.col(\"cm.ChurnProbability\") >= 0.3), \"Retention Priority\")\n         .when((F.col(\"ts.TotalOrders\") == 1) & (F.col(\"ts.DaysSinceLastPurchase\") <= 30), \"Nurture New\")\n         .when(F.col(\"cm.NextPurchasePropensity\") > 0.6, \"Growth Opportunity\")\n         .when(F.col(\"ts.ReturnRate\") > 20, \"Service Improvement\")\n         .otherwise(\"Standard\").alias(\"TargetGroup\"),\n        # MarketingRecommendation\n        F.when(\n            (F.col(\"cm.CustomerLifetimeValue\") >= highValueThreshold) & (F.col(\"cm.ChurnProbability\") < 0.3),\n            \"Exclusive offers, VIP events, Personal shopping assistance, Early access to new products\"\n        ).when(\n            (F.col(\"cm.CustomerLifetimeValue\") >= highValueThreshold * 0.5) & (F.col(\"cm.ChurnProbability\") >= 0.3),\n            \"Re-engagement campaign, Loyalty rewards, Personalized recommendations based on past purchases\"\n        ).when(\n            (F.col(\"ts.TotalOrders\") == 1) & (F.col(\"ts.DaysSinceLastPurchase\") <= 30),\n            \"Welcome series, Educational content, First purchase follow-up, Category exploration\"\n        ).when(\n            F.col(\"cm.NextPurchasePropensity\") > 0.6,\n            F.concat(\n                F.lit(\"Cross-sell/upsell, Bundle offers based on \"),\n                F.coalesce(F.col(\"ts.TopCategory\"), F.lit(\"preferred category\")),\n                F.lit(\", Category expansion\")\n            )\n        ).when(\n            F.col(\"ts.ReturnRate\") > 20,\n            \"Satisfaction survey, Improved product information, Size/fit guides, Service recovery\"\n        ).otherwise(\n            F.concat(\n                F.lit(\"Standard seasonal promotions, Category newsletters, Reactivation after \"),\n                (F.col(\"ts.DaysSinceLastPurchase\") / 30).cast(\"int\"),\n                F.lit(\" months\")\n            )\n        ).alias(\"MarketingRecommendation\")\n    )\n)\n\n# Optionally, write out these silver DataFrames (RFM, Metrics, Segments)\ncustomerBaseDF.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"lakehouse_silver.CustomerBase\")\ntransactionSummaryDF.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"lakehouse_silver.TransactionSummary\")\ncustomerMetricsDF.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"lakehouse_silver.CustomerMetrics\")\nsegmentationDF.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"lakehouse_silver.CustomerSegments\")\n\n# STAGE 2: ADVANCED ANALYTICS END\n```\n\n------------------------------------------------------------------------\n(4) GOLD LAYER\n------------------------------------------------------------------------\n```python\n# GOLD LAYER START\n# -------------------------------------------------------------\n# Produce final analytics tables (CustomerProfiles, CustomerAnalytics)\n# and daily summary (CustomerAnalyticsSummary). We can use merges or\n# simple overwrites/appends for demonstration.\n\n# 1) Update or overwrite the CustomerProfiles table\ncustomerProfileGoldDF = (\n    customerBaseDF.alias(\"cb\")\n    .join(transactionSummaryDF.alias(\"ts\"), \"CustomerID\", \"left\")\n    .select(\n        \"cb.CustomerID\",\n        \"cb.CustomerName\",\n        \"cb.Email\",\n        \"cb.Phone\",\n        \"cb.Address\",\n        \"cb.PostalCode\",\n        \"cb.City\",\n        \"cb.State\",\n        \"cb.Country\",\n        \"cb.CustomerType\",\n        \"cb.AccountManager\",\n        \"cb.CreatedDate\",\n        \"cb.IsActive\",\n        \"cb.LastContactDate\",\n        \"ts.TotalOrders\",\n        \"ts.TotalSpent\",\n        \"ts.AvgOrderValue\",\n        \"ts.FirstPurchaseDate\",\n        \"ts.LastPurchaseDate\",\n        \"ts.DaysSinceLastPurchase\",\n        \"ts.TopCategory\",\n        \"ts.TopProduct\",\n        \"ts.ReturnRate\",\n        F.current_timestamp().alias(\"ModifiedDate\")  # for auditing\n    )\n)\n\n(\n    customerProfileGoldDF\n    .write\n    .format(\"delta\")\n    .mode(\"overwrite\")  # For incremental, consider MERGE. \n    .saveAsTable(\"lakehouse_gold.CustomerProfiles\")\n)\n\n# 2) Insert daily snapshot into CustomerAnalytics\ncustomerAnalyticsGoldDF = (\n    customerMetricsDF.alias(\"cm\")\n    .join(segmentationDF.alias(\"cs\"), \"CustomerID\")\n    .select(\n        F.lit(dataDate).alias(\"ProcessDate\"),\n        \"cm.CustomerID\",\n        \"cm.CustomerLifetimeValue\",\n        \"cm.RecencyScore\",\n        \"cm.FrequencyScore\",\n        \"cm.MonetaryScore\",\n        \"cm.RFMScore\",\n        \"cm.ChurnProbability\",\n        \"cm.NextPurchasePropensity\",\n        \"cm.LoyaltyIndex\",\n        \"cm.CustomerHealth\",\n        \"cs.ValueSegment\",\n        \"cs.BehaviorSegment\",\n        \"cs.LifecycleSegment\",\n        \"cs.TargetGroup\",\n        \"cs.MarketingRecommendation\"\n    )\n)\n\n(\n    customerAnalyticsGoldDF\n    .write\n    .format(\"delta\")\n    .mode(\"append\")\n    .saveAsTable(\"lakehouse_gold.CustomerAnalytics\")\n)\n\n# Retention: remove older data\nspark.sql(f\"\"\"\n    DELETE FROM lakehouse_gold.CustomerAnalytics\n    WHERE ProcessDate < DATEADD(DAY, -{retentionPeriodDays}, '{str(dataDate)}')\n\"\"\")\n\n# 3) Generate daily summary (CustomerAnalyticsSummary)\nsummaryWindow = Window.partitionBy()\nsummaryDF = (\n    customerBaseDF.alias(\"cb\")\n    .join(customerMetricsDF.alias(\"cm\"), \"CustomerID\")\n    .join(segmentationDF.alias(\"cs\"), \"CustomerID\")\n    .select(\n        F.lit(dataDate).alias(\"ProcessDate\"),\n        F.lit(processType).alias(\"ProcessType\"),\n        F.count_distinct(\"cb.CustomerID\").over(summaryWindow).alias(\"TotalCustomers\"),\n        F.sum(F.when(F.col(\"cs.LifecycleSegment\") == \"Active\", 1).otherwise(0)).over(summaryWindow).alias(\"ActiveCustomers\"),\n        F.sum(F.when(F.col(\"cs.LifecycleSegment\") == \"New Customer\", 1).otherwise(0)).over(summaryWindow).alias(\"NewCustomers\"),\n        F.sum(F.when((F.col(\"cs.LifecycleSegment\") == \"At Risk\") | (F.col(\"cs.LifecycleSegment\") == \"Churned\"), 1).otherwise(0)).over(summaryWindow).alias(\"ChurningCustomers\"),\n        F.sum(F.when(F.col(\"cs.ValueSegment\") == \"High Value\", 1).otherwise(0)).over(summaryWindow).alias(\"HighValueCustomers\"),\n        F.avg(\"cm.CustomerLifetimeValue\").over(summaryWindow).alias(\"AverageLifetimeValue\"),\n        (F.unix_timestamp(F.current_timestamp()) - F.unix_timestamp(F.lit(str(dataDate)))) * 1000,\n        F.lit(\"Success\").alias(\"ProcessingStatus\")\n    )\n).distinct()\n\n(\n    summaryDF\n    .write\n    .format(\"delta\")\n    .mode(\"append\")\n    .saveAsTable(\"lakehouse_gold.CustomerAnalyticsSummary\")\n)\n\nif debugMode:\n    summaryDF.show()\n\n# GOLD LAYER END\n```\n\n────────────────────────────────────────────────────────────────────────\n3) IMPLEMENTATION GUIDELINES & BEST PRACTICES\n────────────────────────────────────────────────────────────────────────\n(Role: TechLead)\n\n• Parameter & Config Management  \n  – Store churn thresholds, highValueThreshold, retention days, etc. in a centralized config table or file.  \n  – Use environment-based parameters for dev/test/prod to avoid manual updates.  \n\n• Use Delta Lake for Scalability  \n  – Partition large tables by date or region if data volumes are high.  \n  – Use MERGE for incremental upserts rather than “Overwrite” if partial updates are needed.  \n\n• Error Handling & Logging  \n  – Wrap transformations in try/except in production.  \n  – Use pipeline logging to capture run details, row counts, and failures.  \n\n• Incremental vs. Full Loads  \n  – FULL loads re-process historical data.  \n  – INCREMENTAL loads limit processing to changed data (via ModifiedDate, etc.).  \n  – Ensure partition pruning or filter conditions are used for performance.  \n\n• Performance Optimization  \n  – Cache or broadcast smaller DataFrames if they are joined multiple times.  \n  – Adjust cluster size for daily peak loads.  \n\n• CI/CD & Version Control  \n  – Store notebooks/scripts in Git.  \n  – Automated tests and environment promotions (dev → test → prod) reduce errors.  \n\n────────────────────────────────────────────────────────────────────────\n4) TEST CASES FOR VALIDATION\n────────────────────────────────────────────────────────────────────────\n(Role: TestingAgent)\n\nA. Full Load with Known Dataset  \n• Create a test dataset with multiple segments: new customers, active repeat buyers, churned, and high-value.  \n• Validate outputs in each Gold table: RFM scores, churn probability, segmentation, final summary counts.\n\nB. Incremental Load Test  \n• Insert data with OrderDate or ModifiedDate > dataDate.  \n• Run the pipeline in INCREMENTAL mode and confirm only new or changed records are processed, with no duplication.\n\nC. Parameter Variation  \n• Change churn thresholds (e.g., churnDaysVeryHigh=200) or highValueThreshold=10000.  \n• Verify that churn probability and ValueSegment calculations reflect the new parameters.\n\nD. Edge Cases (No Orders/Interactions)  \n• Include some customers with zero orders, zero interactions.  \n• Ensure they appear with zero or null metrics in transaction summaries and are placed in correct lifecycle segments (e.g., “Inactive”, “Hibernating”, or “Lost”).\n\nE. Retention Enforcement  \n• Provide older data (beyond retentionPeriodDays) in CustomerAnalytics.  \n• Verify that rows older than the cutoff are removed after the pipeline.\n\nF. Error Handling & Partial Failures  \n• Introduce invalid references or simulate a DataFrame join error.  \n• Check that the pipeline fails gracefully and logs issues instead of silently corrupting data.\n\n────────────────────────────────────────────────────────────────────────\n\nThis plan and code outline demonstrate how to migrate the original SQL stored procedure logic into a well-structured, maintainable PySpark pipeline within Microsoft Fabric. By leveraging Delta Lake in the Bronze–Silver–Gold layers, you ensure clarity of data lineage, incremental processing, and streamlined analytics for your customer insights.",
    "role": "user",
    "name": "AzureDataEngineer"
  },
  {
    "content": "Below is a comprehensive, production-ready migration plan and PySpark implementation for Microsoft Fabric. It follows the Medallion (Bronze–Silver–Gold) architecture, incorporates best practices, and includes test cases. The solution is presented from multiple roles (BusinessAnalyst, ProductOwner, AzureDataEngineer, TechLead, and TestingAgent) to ensure clarity and completeness.\n\n────────────────────────────────────────────────────────────────────────────\n1) BUSINESS ANALYSIS & MIGRATION PLAN (BusinessAnalyst & ProductOwner)\n────────────────────────────────────────────────────────────────────────────\n\nA. Business Purpose and Requirements  \n• The original SQL stored procedure (usp_CustomerAnalytics_Processing) calculates customer metrics—RFM scores, churn probability, loyalty, and segmentation—then stores daily analytics and summary statistics in relational tables.  \n• This supports marketing, sales, and executive decision-making by identifying loyalty trends, churn risks, high-value segments, and recommended marketing actions.\n\nB. Core Objectives and Logic  \n1. Create a current “Customer Base” (active customers changed since a given date).  \n2. Summarize historical orders (total orders, total spend, returns, top categories/products).  \n3. Calculate advanced metrics: RFM, churn probability, loyalty index, and next purchase propensity.  \n4. Derive segmentation (Value, Behavior, Lifecycle, TargetGroup) along with marketing recommendations.  \n5. Upsert into daily analytics tables (CustomerProfiles, CustomerAnalytics) and produce summary statistics (CustomerAnalyticsSummary).  \n6. Apply data retention (typically 365 days) to remove older analytics records.\n\nC. Data Flow: Medallion Architecture in Microsoft Fabric  \n1. Bronze Layer: Raw ingestion of tables such as Customers, Orders, Products, etc.  \n2. Silver Layer:  \n   – Stage 1: Customer Base and Transaction Summary.  \n   – Stage 2: Metrics and Segmentation (RFM scores, churn, segmentation).  \n3. Gold Layer: Final analytics tables (CustomerProfiles, CustomerAnalytics) and summary (CustomerAnalyticsSummary).  \n\nD. Key Parameters  \n• dataDate (defaults to yesterday)  \n• processType (“FULL” or “INCREMENTAL”)  \n• debugMode (boolean)  \n• retentionPeriodDays (default 365)  \n• churnDays_VeryHigh/High/Medium/Low (defaults 180, 90, 60, 30)  \n• highValueThreshold (default 5000.0)  \n• loyaltyThreshold (default 5)  \n\nThese parameters can be stored as notebook variables, external config files, or in a Fabric table.  \n\n────────────────────────────────────────────────────────────────────────────\n2) P Y S P A R K   C O D E   F O R   B R O N Z E,   S I L V E R,   A N D   G O L D\n   (AzureDataEngineer)\n────────────────────────────────────────────────────────────────────────────\n\nBelow is an example PySpark notebook or pipeline script aligned with Microsoft Fabric Lakehouse. It is divided into four main sections—(1) Bronze, (2) Silver Stage 1, (3) Silver Stage 2, and (4) Gold layer.\n\n--------------------------------------------------------------------------------\n-- SETUP & CONFIGURATION\n--------------------------------------------------------------------------------\nfrom pyspark.sql import SparkSession, functions as F, Window\nimport datetime\n\n# 1) Create Spark Session (in Fabric notebooks, this is usually already available)\nspark = SparkSession.builder.getOrCreate()\n\n# 2) Define parameters (mirroring the SQL SP defaults)\ndataDate = None  # If not supplied, default to yesterday\nprocessType = 'FULL'  # or 'INCREMENTAL'\ndebugMode = False\nretentionPeriodDays = 365\nhighValueThreshold = 5000.0\nloyaltyThreshold = 5\nchurnDaysVeryHigh = 180\nchurnDaysHigh = 90\nchurnDaysMedium = 60\nchurnDaysLow = 30\n\n# 3) Default dataDate to yesterday\nif dataDate is None:\n    dataDate = datetime.date.today() - datetime.timedelta(days=1)\n\n--------------------------------------------------------------------------------\n-- BRONZE LAYER (Raw Data Ingestion)\n--------------------------------------------------------------------------------\n# Here, we read the raw/bronze tables. In Microsoft Fabric, these might be \"lakehouse_raw\" tables or files.\n\ncustomersBronzeDF = spark.read.table(\"lakehouse_raw.Customers\")\naddressesBronzeDF = spark.read.table(\"lakehouse_raw.CustomerAddresses\")\ninteractionsBronzeDF = spark.read.table(\"lakehouse_raw.CustomerInteractions\")\nordersBronzeDF = spark.read.table(\"lakehouse_raw.Orders\")\norderDetailsBronzeDF = spark.read.table(\"lakehouse_raw.OrderDetails\")\nproductsBronzeDF = spark.read.table(\"lakehouse_raw.Products\")\nreturnsBronzeDF = spark.read.table(\"lakehouse_raw.Returns\")\n\n--------------------------------------------------------------------------------\n-- SILVER LAYER: Stage 1 (Customer Base & Transaction Summaries)\n--------------------------------------------------------------------------------\n\n# (A) Build the \"Customer Base\" DataFrame (similar to #CustomerBase in SQL)\ncustomerBaseDF = (\n    customersBronzeDF.alias(\"c\")\n    .join(\n        addressesBronzeDF.alias(\"a\"),\n        (F.col(\"c.CustomerID\") == F.col(\"a.CustomerID\")) & (F.col(\"a.AddressType\") == \"Primary\"),\n        \"left\"\n    )\n    .join(\n        interactionsBronzeDF.alias(\"i\"),\n        F.col(\"c.CustomerID\") == F.col(\"i.CustomerID\"),\n        \"left\"\n    )\n    .where(\n        (processType == 'FULL')\n        | (F.col(\"c.ModifiedDate\") >= F.lit(dataDate))\n        | (F.col(\"i.ContactDate\") >= F.lit(dataDate))\n    )\n    .groupBy(\n        \"c.CustomerID\", \"c.FirstName\", \"c.LastName\", \"c.Email\", \"c.Phone\",\n        \"a.StreetAddress\", \"a.PostalCode\", \"a.City\", \"a.State\", \"a.Country\",\n        \"c.CustomerType\", \"c.AccountManagerID\",\n        \"c.CreatedDate\", \"c.ModifiedDate\", \"c.Status\"\n    )\n    .agg(F.max(\"i.ContactDate\").alias(\"LastContactDate\"))\n    .select(\n        F.col(\"CustomerID\"),\n        F.concat_ws(\" \", F.col(\"FirstName\"), F.col(\"LastName\")).alias(\"CustomerName\"),\n        \"Email\",\n        \"Phone\",\n        F.col(\"StreetAddress\").alias(\"Address\"),\n        \"PostalCode\",\n        \"City\",\n        \"State\",\n        \"Country\",\n        \"CustomerType\",\n        F.col(\"AccountManagerID\").alias(\"AccountManager\"),\n        \"CreatedDate\",\n        \"ModifiedDate\",\n        (F.col(\"Status\") == \"Active\").cast(\"boolean\").alias(\"IsActive\"),\n        \"LastContactDate\"\n    )\n)\n\n# (B) Order Summaries (#TransactionSummary initial logic)\norderSummaryDF = (\n    ordersBronzeDF.alias(\"o\")\n    .join(orderDetailsBronzeDF.alias(\"od\"), \"OrderID\")\n    .where(F.col(\"o.Status\") == \"Completed\")\n    .groupBy(\"o.CustomerID\")\n    .agg(\n        F.countDistinct(\"o.OrderID\").alias(\"TotalOrders\"),\n        F.sum(F.col(\"od.Quantity\") * F.col(\"od.UnitPrice\") * (1 - F.col(\"od.Discount\"))).alias(\"TotalSpent\"),\n        F.min(\"o.OrderDate\").alias(\"FirstPurchaseDate\"),\n        F.max(\"o.OrderDate\").alias(\"LastPurchaseDate\")\n    )\n)\n\ntransactionSummaryDF = (\n    orderSummaryDF.alias(\"os\")\n    .join(\n        customerBaseDF.select(\"CustomerID\").alias(\"cb\"),\n        F.col(\"os.CustomerID\") == F.col(\"cb.CustomerID\"),\n        \"inner\"\n    )\n    .select(\n        F.col(\"os.CustomerID\"),\n        F.col(\"os.TotalOrders\"),\n        F.col(\"os.TotalSpent\"),\n        F.when(F.col(\"os.TotalOrders\") > 0, F.col(\"os.TotalSpent\") / F.col(\"os.TotalOrders\"))\n         .otherwise(F.lit(0.0)).alias(\"AvgOrderValue\"),\n        F.col(\"os.FirstPurchaseDate\"),\n        F.col(\"os.LastPurchaseDate\"),\n        F.datediff(F.lit(dataDate), F.col(\"os.LastPurchaseDate\")).alias(\"DaysSinceLastPurchase\")\n    )\n)\n\n--------------------------------------------------------------------------------\n-- SILVER LAYER: Stage 2 (RFM, Churn Probability, Segmentation)\n--------------------------------------------------------------------------------\n\n# (A) Compute additional fields (top category/product, return rate)\ncategorySpendDF = (\n    ordersBronzeDF.alias(\"o\")\n    .join(orderDetailsBronzeDF.alias(\"od\"), \"OrderID\")\n    .join(productsBronzeDF.alias(\"p\"), \"ProductID\")\n    .where(F.col(\"o.Status\") == \"Completed\")\n    .groupBy(\"o.CustomerID\", \"p.Category\")\n    .agg(F.sum(F.col(\"od.Quantity\") * F.col(\"od.UnitPrice\") * (1 - F.col(\"od.Discount\"))).alias(\"CategorySpend\"))\n)\ncatWindow = Window.partitionBy(\"CustomerID\").orderBy(F.col(\"CategorySpend\").desc())\ncatTopDF = categorySpendDF.withColumn(\"CategoryRank\", F.row_number().over(catWindow)).filter(F.col(\"CategoryRank\") == 1)\n\nproductQtyDF = (\n    ordersBronzeDF.alias(\"o\")\n    .join(orderDetailsBronzeDF.alias(\"od\"), \"OrderID\")\n    .join(productsBronzeDF.alias(\"p\"), \"ProductID\")\n    .where(F.col(\"o.Status\") == \"Completed\")\n    .groupBy(\"o.CustomerID\", \"p.ProductName\")\n    .agg(F.sum(\"od.Quantity\").alias(\"TotalQuantity\"))\n)\nprodWindow = Window.partitionBy(\"CustomerID\").orderBy(F.col(\"TotalQuantity\").desc())\nprodTopDF = productQtyDF.withColumn(\"ProductRank\", F.row_number().over(prodWindow)).filter(F.col(\"ProductRank\") == 1)\n\nreturnsGroupedDF = (\n    returnsBronzeDF.groupBy(\"CustomerID\")\n    .agg(F.countDistinct(\"ReturnID\").alias(\"TotalReturns\"))\n)\n\ntransactionSummaryDF = (\n    transactionSummaryDF.alias(\"ts\")\n    .join(catTopDF.alias(\"cat\"), \"CustomerID\", \"left\")\n    .join(prodTopDF.alias(\"prod\"), \"CustomerID\", \"left\")\n    .join(returnsGroupedDF.alias(\"ret\"), \"CustomerID\", \"left\")\n    .select(\n        F.col(\"ts.CustomerID\"),\n        \"ts.TotalOrders\",\n        \"ts.TotalSpent\",\n        \"ts.AvgOrderValue\",\n        \"ts.FirstPurchaseDate\",\n        \"ts.LastPurchaseDate\",\n        \"ts.DaysSinceLastPurchase\",\n        F.col(\"cat.Category\").alias(\"TopCategory\"),\n        F.col(\"prod.ProductName\").alias(\"TopProduct\"),\n        F.when(F.col(\"ts.TotalOrders\") > 0, (F.col(\"ret.TotalReturns\") / F.col(\"ts.TotalOrders\")) * 100)\n         .otherwise(F.lit(0.0)).alias(\"ReturnRate\")\n    )\n)\n\n# (B) RFM scoring\nwindowRecency = Window.orderBy(F.col(\"DaysSinceLastPurchase\").asc())\nwindowFrequency = Window.orderBy(F.col(\"TotalOrders\").desc())\nwindowMonetary = Window.orderBy(F.col(\"TotalSpent\").desc())\n\ntransactionSummaryDF = (\n    transactionSummaryDF\n    .withColumn(\"RecencyScore\", F.ntile(5).over(windowRecency))\n    .withColumn(\"FrequencyScore\", F.ntile(5).over(windowFrequency))\n    .withColumn(\"MonetaryScore\", F.ntile(5).over(windowMonetary))\n)\n\nrfmDF = transactionSummaryDF.select(\n    \"CustomerID\",\n    \"RecencyScore\",\n    \"FrequencyScore\",\n    \"MonetaryScore\",\n    (F.col(\"RecencyScore\") + F.col(\"FrequencyScore\") + F.col(\"MonetaryScore\")).alias(\"RFMScore\")\n)\n\n# (C) Churn Probability and Loyalty\nchurnDF = (\n    transactionSummaryDF.select(\n        \"CustomerID\",\n        F.when(F.col(\"DaysSinceLastPurchase\") > churnDaysVeryHigh, 0.8)\n         .when(F.col(\"DaysSinceLastPurchase\") > churnDaysHigh, 0.5)\n         .when(F.col(\"DaysSinceLastPurchase\") > churnDaysMedium, 0.3)\n         .when(F.col(\"DaysSinceLastPurchase\") > churnDaysLow, 0.1)\n         .otherwise(0.05)\n         .alias(\"ChurnProbability\"),\n        F.when((F.col(\"DaysSinceLastPurchase\") < churnDaysLow) & (F.col(\"TotalOrders\") > 3), 0.8)\n         .when((F.col(\"DaysSinceLastPurchase\") < churnDaysMedium) & (F.col(\"TotalOrders\") > 2), 0.6)\n         .when(F.col(\"DaysSinceLastPurchase\") < churnDaysHigh, 0.4)\n         .otherwise(0.2)\n         .alias(\"NextPurchasePropensity\"),\n        (\n            F.when(\n                (F.col(\"TotalOrders\") >= loyaltyThreshold) & (F.col(\"ReturnRate\") < 10),\n                (F.col(\"TotalOrders\") * 0.6) + ((100 - F.col(\"ReturnRate\")) * 0.4)\n            )\n            .otherwise(F.col(\"TotalOrders\") * 0.4)\n            / 10\n        ).alias(\"LoyaltyIndex\")\n    )\n)\n\ncustomerMetricsDF = (\n    rfmDF.alias(\"r\")\n    .join(transactionSummaryDF.alias(\"t\"), \"CustomerID\")\n    .join(churnDF.alias(\"ch\"), \"CustomerID\")\n    .select(\n        F.col(\"r.CustomerID\"),\n        (F.col(\"t.TotalSpent\") * (1 + (1 - F.col(\"ch.ChurnProbability\")))).alias(\"CustomerLifetimeValue\"),\n        \"r.RecencyScore\",\n        \"r.FrequencyScore\",\n        \"r.MonetaryScore\",\n        \"r.RFMScore\",\n        \"ch.ChurnProbability\",\n        \"ch.NextPurchasePropensity\",\n        \"ch.LoyaltyIndex\",\n        F.when(F.col(\"ch.ChurnProbability\") > 0.7, \"At Risk\")\n         .when(F.col(\"ch.LoyaltyIndex\") > 0.8, \"Excellent\")\n         .when(F.col(\"ch.LoyaltyIndex\") > 0.6, \"Good\")\n         .when(F.col(\"ch.LoyaltyIndex\") > 0.4, \"Average\")\n         .otherwise(\"Needs Attention\")\n         .alias(\"CustomerHealth\")\n    )\n)\n\n# (D) Segmentation\nsegmentationDF = (\n    customerBaseDF.alias(\"cb\")\n    .join(customerMetricsDF.alias(\"cm\"), \"CustomerID\")\n    .join(transactionSummaryDF.alias(\"ts\"), \"CustomerID\")\n    .select(\n        F.col(\"cb.CustomerID\"),\n        # ValueSegment\n        F.when(F.col(\"cm.CustomerLifetimeValue\") >= highValueThreshold, \"High Value\")\n         .when(F.col(\"cm.CustomerLifetimeValue\") >= highValueThreshold * 0.5, \"Medium Value\")\n         .otherwise(\"Low Value\").alias(\"ValueSegment\"),\n        # BehaviorSegment\n        F.when(F.col(\"cm.RFMScore\") >= 13, \"Champions\")\n         .when((F.col(\"cm.RecencyScore\") >= 4) & (F.col(\"cm.FrequencyScore\") >= 3), \"Loyal Customers\")\n         .when((F.col(\"cm.RecencyScore\") >= 4) & (F.col(\"cm.FrequencyScore\") <= 2), \"Potential Loyalists\")\n         .when((F.col(\"cm.RecencyScore\") <= 2) & (F.col(\"cm.FrequencyScore\") >= 3) & (F.col(\"cm.MonetaryScore\") >= 3), \"At Risk\")\n         .when((F.col(\"cm.RecencyScore\") <= 2) & (F.col(\"cm.FrequencyScore\") <= 2) & (F.col(\"cm.MonetaryScore\") <= 2), \"Hibernating\")\n         .when(F.col(\"cm.RecencyScore\") <= 1, \"Lost\")\n         .otherwise(\"Others\").alias(\"BehaviorSegment\"),\n        # LifecycleSegment\n        F.when((F.col(\"ts.TotalOrders\") == 1) & (F.col(\"ts.DaysSinceLastPurchase\") <= 30), \"New Customer\")\n         .when((F.col(\"ts.TotalOrders\") > 1) & (F.col(\"cm.ChurnProbability\") < 0.3), \"Active\")\n         .when((F.col(\"cm.ChurnProbability\") >= 0.3) & (F.col(\"cm.ChurnProbability\") < 0.7), \"At Risk\")\n         .when(F.col(\"cm.ChurnProbability\") >= 0.7, \"Churned\")\n         .otherwise(\"Inactive\").alias(\"LifecycleSegment\"),\n        # TargetGroup\n        F.when((F.col(\"cm.CustomerLifetimeValue\") >= highValueThreshold) & (F.col(\"cm.ChurnProbability\") < 0.3), \"VIP\")\n         .when((F.col(\"cm.CustomerLifetimeValue\") >= highValueThreshold * 0.5) & (F.col(\"cm.ChurnProbability\") >= 0.3), \"Retention Priority\")\n         .when((F.col(\"ts.TotalOrders\") == 1) & (F.col(\"ts.DaysSinceLastPurchase\") <= 30), \"Nurture New\")\n         .when(F.col(\"cm.NextPurchasePropensity\") > 0.6, \"Growth Opportunity\")\n         .when(F.col(\"ts.ReturnRate\") > 20, \"Service Improvement\")\n         .otherwise(\"Standard\").alias(\"TargetGroup\"),\n        # MarketingRecommendations\n        F.when(\n            (F.col(\"cm.CustomerLifetimeValue\") >= highValueThreshold) & (F.col(\"cm.ChurnProbability\") < 0.3),\n            \"Exclusive offers, VIP events, Personal shopping assistance, Early access to new products\"\n        ).when(\n            (F.col(\"cm.CustomerLifetimeValue\") >= highValueThreshold * 0.5) & (F.col(\"cm.ChurnProbability\") >= 0.3),\n            \"Re-engagement campaign, Loyalty rewards, Personalized recommendations based on past purchases\"\n        ).when(\n            (F.col(\"ts.TotalOrders\") == 1) & (F.col(\"ts.DaysSinceLastPurchase\") <= 30),\n            \"Welcome series, Educational content, First purchase follow-up, Category exploration\"\n        ).when(\n            F.col(\"cm.NextPurchasePropensity\") > 0.6,\n            F.concat(\n                F.lit(\"Cross-sell/upsell, Bundle offers based on \"),\n                F.coalesce(F.col(\"ts.TopCategory\"), F.lit(\"preferred category\")),\n                F.lit(\", Category expansion\")\n            )\n        ).when(\n            F.col(\"ts.ReturnRate\") > 20,\n            \"Satisfaction survey, Improved product information, Size/fit guides, Service recovery\"\n        ).otherwise(\n            F.concat(\n                F.lit(\"Standard seasonal promotions, Category newsletters, Reactivation after \"),\n                (F.col(\"ts.DaysSinceLastPurchase\") / 30).cast(\"int\"),\n                F.lit(\" months\")\n            )\n        ).alias(\"MarketingRecommendation\")\n    )\n)\n\n# Write out Silver Data (optional or for debugging)\ncustomerBaseDF.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"lakehouse_silver.CustomerBase\")\ntransactionSummaryDF.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"lakehouse_silver.TransactionSummary\")\ncustomerMetricsDF.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"lakehouse_silver.CustomerMetrics\")\nsegmentationDF.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"lakehouse_silver.CustomerSegments\")\n\n--------------------------------------------------------------------------------\n-- GOLD LAYER (Final Analytics & Summary)\n--------------------------------------------------------------------------------\n\n# (A) Upsert/Overwrite CustomerProfiles (similar to dbo.CustomerProfiles)\ncustomerProfileGoldDF = (\n    customerBaseDF.alias(\"cb\")\n    .join(transactionSummaryDF.alias(\"ts\"), \"CustomerID\", \"left\")\n    .select(\n        \"cb.CustomerID\",\n        \"cb.CustomerName\",\n        \"cb.Email\",\n        \"cb.Phone\",\n        \"cb.Address\",\n        \"cb.PostalCode\",\n        \"cb.City\",\n        \"cb.State\",\n        \"cb.Country\",\n        \"cb.CustomerType\",\n        \"cb.AccountManager\",\n        \"cb.CreatedDate\",\n        \"cb.IsActive\",\n        \"cb.LastContactDate\",\n        \"ts.TotalOrders\",\n        \"ts.TotalSpent\",\n        \"ts.AvgOrderValue\",\n        \"ts.FirstPurchaseDate\",\n        \"ts.LastPurchaseDate\",\n        \"ts.DaysSinceLastPurchase\",\n        \"ts.TopCategory\",\n        \"ts.TopProduct\",\n        \"ts.ReturnRate\",\n        F.current_timestamp().alias(\"ModifiedDate\")\n    )\n)\n\n# For demonstration, we overwrite; in production, consider Delta MERGE for incremental upsert\n(\n    customerProfileGoldDF\n    .write\n    .format(\"delta\")\n    .mode(\"overwrite\")\n    .saveAsTable(\"lakehouse_gold.CustomerProfiles\")\n)\n\n# (B) Append daily snapshots to CustomerAnalytics\ncustomerAnalyticsGoldDF = (\n    customerMetricsDF.alias(\"cm\")\n    .join(segmentationDF.alias(\"cs\"), \"CustomerID\")\n    .select(\n        F.lit(dataDate).alias(\"ProcessDate\"),\n        F.col(\"cm.CustomerID\"),\n        F.col(\"cm.CustomerLifetimeValue\"),\n        F.col(\"cm.RecencyScore\"),\n        F.col(\"cm.FrequencyScore\"),\n        F.col(\"cm.MonetaryScore\"),\n        F.col(\"cm.RFMScore\"),\n        F.col(\"cm.ChurnProbability\"),\n        F.col(\"cm.NextPurchasePropensity\"),\n        F.col(\"cm.LoyaltyIndex\"),\n        F.col(\"cm.CustomerHealth\"),\n        F.col(\"cs.ValueSegment\"),\n        F.col(\"cs.BehaviorSegment\"),\n        F.col(\"cs.LifecycleSegment\"),\n        F.col(\"cs.TargetGroup\"),\n        F.col(\"cs.MarketingRecommendation\")\n    )\n)\n\n(\n    customerAnalyticsGoldDF\n    .write\n    .format(\"delta\")\n    .mode(\"append\")\n    .saveAsTable(\"lakehouse_gold.CustomerAnalytics\")\n)\n\n# Clean up older data (retention)\nspark.sql(f\"\"\"\n    DELETE FROM lakehouse_gold.CustomerAnalytics\n    WHERE ProcessDate < DATEADD(DAY, -{retentionPeriodDays}, '{str(dataDate)}')\n\"\"\")\n\n# (C) Daily summary: CustomerAnalyticsSummary\nsummaryWindow = Window.partitionBy()\nsummaryDF = (\n    customerBaseDF.alias(\"cb\")\n    .join(customerMetricsDF.alias(\"cm\"), \"CustomerID\")\n    .join(segmentationDF.alias(\"cs\"), \"CustomerID\")\n    .select(\n        F.lit(dataDate).alias(\"ProcessDate\"),\n        F.lit(processType).alias(\"ProcessType\"),\n        F.count_distinct(\"cb.CustomerID\").over(summaryWindow).alias(\"TotalCustomers\"),\n        F.sum(F.when(F.col(\"cs.LifecycleSegment\") == \"Active\", 1).otherwise(0)).over(summaryWindow).alias(\"ActiveCustomers\"),\n        F.sum(F.when(F.col(\"cs.LifecycleSegment\") == \"New Customer\", 1).otherwise(0)).over(summaryWindow).alias(\"NewCustomers\"),\n        F.sum(F.when((F.col(\"cs.LifecycleSegment\") == \"At Risk\") | (F.col(\"cs.LifecycleSegment\") == \"Churned\"), 1).otherwise(0)).over(summaryWindow).alias(\"ChurningCustomers\"),\n        F.sum(F.when(F.col(\"cs.ValueSegment\") == \"High Value\", 1).otherwise(0)).over(summaryWindow).alias(\"HighValueCustomers\"),\n        F.avg(\"cm.CustomerLifetimeValue\").over(summaryWindow).alias(\"AverageLifetimeValue\"),\n        (F.unix_timestamp(F.current_timestamp()) - F.unix_timestamp(F.lit(str(dataDate)))) * 1000,  # approximate\n        F.lit(\"Success\").alias(\"ProcessingStatus\")\n    )\n).distinct()\n\n(\n    summaryDF\n    .write\n    .format(\"delta\")\n    .mode(\"append\")\n    .saveAsTable(\"lakehouse_gold.CustomerAnalyticsSummary\")\n)\n\nif debugMode:\n    summaryDF.show(truncate=False)\n\n# End of PySpark pipeline\n\n────────────────────────────────────────────────────────────────────────────\n3) IMPLEMENTATION GUIDELINES & BEST PRACTICES (TechLead)\n────────────────────────────────────────────────────────────────────────────\n\n• Parameter Management  \n  – Store churn, value thresholds, and retention in a config table or external file to avoid code changes for parameter tweaks.  \n\n• Use Delta Lake in Fabric  \n  – Enables ACID transactions, MERGE for upserts, time travel, and partitioning for large datasets.  \n  – Replace “mode('overwrite')” with a MERGE pattern for partial updates if your pipeline demands true incremental merges.\n\n• Incremental Processing vs. Full Reload  \n  – Incremental logic is essential for large data volumes: filter or partition on ModifiedDate / OrderDate to reduce scanning overhead.  \n  – Full reload can be used occasionally for historical backfill or one-time reprocessing.\n\n• Error Handling & Logging  \n  – Consider adding try/except blocks at major steps, logging to a pipeline log table or using Fabric pipeline logs.  \n  – For debugging, store row counts, step durations, and any errors in a logging table (similar to #ProcessLog in SQL).\n\n• Performance Optimizations  \n  – Broadcast or cache smaller dimension DataFrames (e.g., addresses, products) if repeated joins degrade performance.  \n  – Ensure your spark cluster sizing meets daily concurrency and data volume requirements.\n\n• CI/CD & Version Control  \n  – Check in this notebook or .py scripts into a Git repo.  \n  – Integrate with Microsoft Fabric or Azure DevOps pipelines for automated testing and deployment to production.\n\n────────────────────────────────────────────────────────────────────────────\n4) TEST CASES FOR VALIDATION (TestingAgent)\n────────────────────────────────────────────────────────────────────────────\n\nBelow are recommended test scenarios to ensure correctness, robustness, and alignment with business expectations:\n\n1. Basic Full Load Test  \n   – Provide a controlled dataset with known new, active, and churned customers.  \n   – Verify final Gold tables match expected metrics, segments, and daily summary counts.\n\n2. Incremental Load Test  \n   – Set processType = 'INCREMENTAL', add new or updated records after dataDate.  \n   – Confirm only those changed records appear in the Silver→Gold pipeline, avoiding duplicates in CustomerAnalytics.\n\n3. Threshold Variation  \n   – Adjust churnDays (e.g., churnDaysVeryHigh=200) or highValueThreshold=10000.  \n   – Confirm churn probabilities or ValueSegment calculations reflect new config.\n\n4. Edge Cases (No Orders / No Interactions)  \n   – Include customers who never ordered, or have zero interactions.  \n   – Validate they still appear in CustomerProfiles with 0 or null metrics, and in segments like “Inactive” or “Hibernating.”\n\n5. Retention Enforcement  \n   – Insert historical data older than retentionPeriodDays.  \n   – After the pipeline, confirm that old CustomerAnalytics records are removed.\n\n6. Error Handling / Data Quality  \n   – Introduce invalid references or missing keys in Orders or Interactions.  \n   – Expect the pipeline to fail gracefully or skip invalid data, depending on your design, logging the issue.\n\n────────────────────────────────────────────────────────────────────────────\nS U M M A R Y\n────────────────────────────────────────────────────────────────────────────\n\nBy migrating the original SQL stored procedure to this PySpark pipeline in Microsoft Fabric, you gain:  \n• A clear Medallion architecture separating raw data (Bronze), curated transformations (Silver), and final analytics (Gold).  \n• Scalability and reliability via Delta Lake’s ACID transactions and partitioning for large data volumes.  \n• Modular, maintainable code with explicit stage outputs, simpler debug mode, and easily adjustable parameters (churn thresholds, retention, etc.).  \n\nFollowing the included best practices and test scenarios will ensure a successful move from the legacy T-SQL procedure to a modern, robust, and production-ready Microsoft Fabric data pipeline.",
    "role": "user",
    "name": "TechLead"
  },
  {
    "content": "Below is a comprehensive, production-ready migration plan and PySpark implementation for Microsoft Fabric. It follows the Medallion (Bronze–Silver–Gold) architecture, incorporates best practices, and includes test cases. The solution is presented from multiple roles (BusinessAnalyst, ProductOwner, AzureDataEngineer, TechLead, and TestingAgent) to ensure clarity and completeness.\n\n────────────────────────────────────────────────────────────────────────────\n1) BUSINESS ANALYSIS & MIGRATION PLAN (BusinessAnalyst & ProductOwner)\n────────────────────────────────────────────────────────────────────────────\n\nA) Business Requirements and Objectives  \n• The original SQL stored procedure, [dbo].[usp_CustomerAnalytics_Processing], calculates key customer metrics:  \n  – RFM (Recency, Frequency, Monetary) scores  \n  – Churn probability and loyalty index  \n  – Segmentation (ValueSegment, BehaviorSegment, LifecycleSegment)  \n  – Daily summary statistics stored in analytics tables  \n• End users: marketing, sales, executives, and customer success teams, who use these insights to tailor campaigns, track churn, and monitor high-value segments.\n\nB) Data Sources and Targets  \n• Source Data (Bronze layer):  \n  – Customers, CustomerAddresses, CustomerInteractions  \n  – Orders, OrderDetails, Products, Returns  \n• Target Tables (Gold layer):  \n  – CustomerProfiles (dim-like table with the latest customer info)  \n  – CustomerAnalytics (daily appended records with churn, RFM, segments)  \n  – CustomerAnalyticsSummary (daily aggregates for quick overview)\n\nC) Migration Approach Using the Medallion Architecture  \n• Bronze: Raw data from source systems.  \n• Silver:  \n  1. Create intermediate “Customer Base” and “Transaction Summary” datasets.  \n  2. Compute RFM scores, churn probability, loyalty, and segmentation.  \n• Gold:  \n  – Final upserts/loads into CustomerProfiles, appended inserts in CustomerAnalytics, and generation of a daily summary (CustomerAnalyticsSummary).  \n• Key Configuration/Parameters:  \n  – dataDate (default yesterday), processType (FULL or INCREMENTAL), churnDays thresholds, highValueThreshold, retentionPeriodDays, etc.\n\n────────────────────────────────────────────────────────────────────────────\n2) PYS PARK CODE FOR BRONZE, SILVER, AND GOLD  (AzureDataEngineer)\n────────────────────────────────────────────────────────────────────────────\n\nBelow is a production-ready PySpark outline. Adapt table paths and database names to your environment. Code is grouped by stages:\n\n------------------------------------------------\n-- SETUP & CONFIGURATION\n------------------------------------------------\nfrom pyspark.sql import SparkSession, functions as F, Window\nimport datetime\n\nspark = SparkSession.builder.getOrCreate()\n\n# Parameters (mirroring the stored procedure defaults)\ndataDate = None  # If not supplied, default to yesterday\nprocessType = 'FULL'  # 'FULL' or 'INCREMENTAL'\ndebugMode = False\nretentionPeriodDays = 365\nhighValueThreshold = 5000.0\nloyaltyThreshold = 5\nchurnDaysVeryHigh = 180\nchurnDaysHigh = 90\nchurnDaysMedium = 60\nchurnDaysLow = 30\n\n# Default dataDate to yesterday if None\nif dataDate is None:\n    dataDate = datetime.date.today() - datetime.timedelta(days=1)\n\n------------------------------------------------\n-- BRONZE LAYER: RAW INGESTION\n------------------------------------------------\n# Example: reading raw tables from “lakehouse_raw” (Microsoft Fabric)\ncustomersBronzeDF = spark.read.table(\"lakehouse_raw.Customers\")\naddressesBronzeDF = spark.read.table(\"lakehouse_raw.CustomerAddresses\")\ninteractionsBronzeDF = spark.read.table(\"lakehouse_raw.CustomerInteractions\")\nordersBronzeDF = spark.read.table(\"lakehouse_raw.Orders\")\norderDetailsBronzeDF = spark.read.table(\"lakehouse_raw.OrderDetails\")\nproductsBronzeDF = spark.read.table(\"lakehouse_raw.Products\")\nreturnsBronzeDF = spark.read.table(\"lakehouse_raw.Returns\")\n\n------------------------------------------------\n-- SILVER LAYER: STAGE 1 (CUSTOMER BASE & TRANSACTION SUMMARY)\n------------------------------------------------\n# 1) Build a \"Customer Base\" DataFrame (like #CustomerBase in SQL)\ncustomerBaseDF = (\n    customersBronzeDF.alias(\"c\")\n    .join(\n        addressesBronzeDF.alias(\"a\"),\n        (F.col(\"c.CustomerID\") == F.col(\"a.CustomerID\")) & (F.col(\"a.AddressType\") == \"Primary\"),\n        \"left\"\n    )\n    .join(\n        interactionsBronzeDF.alias(\"i\"),\n        F.col(\"c.CustomerID\") == F.col(\"i.CustomerID\"),\n        \"left\"\n    )\n    .where(\n        (processType == 'FULL') |\n        (F.col(\"c.ModifiedDate\") >= F.lit(dataDate)) |\n        (F.col(\"i.ContactDate\") >= F.lit(dataDate))\n    )\n    .groupBy(\n        \"c.CustomerID\", \"c.FirstName\", \"c.LastName\", \"c.Email\", \"c.Phone\",\n        \"a.StreetAddress\", \"a.PostalCode\", \"a.City\", \"a.State\", \"a.Country\",\n        \"c.CustomerType\", \"c.AccountManagerID\", \"c.CreatedDate\", \"c.ModifiedDate\",\n        \"c.Status\"\n    )\n    .agg(F.max(\"i.ContactDate\").alias(\"LastContactDate\"))\n    .select(\n        F.col(\"CustomerID\"),\n        F.concat_ws(\" \", F.col(\"FirstName\"), F.col(\"LastName\")).alias(\"CustomerName\"),\n        F.col(\"Email\"),\n        F.col(\"Phone\"),\n        F.col(\"StreetAddress\").alias(\"Address\"),\n        F.col(\"PostalCode\"),\n        F.col(\"City\"),\n        F.col(\"State\"),\n        F.col(\"Country\"),\n        F.col(\"CustomerType\"),\n        F.col(\"AccountManagerID\").alias(\"AccountManager\"),\n        F.col(\"CreatedDate\"),\n        F.col(\"ModifiedDate\"),\n        (F.col(\"Status\") == \"Active\").cast(\"boolean\").alias(\"IsActive\"),\n        F.col(\"LastContactDate\")\n    )\n)\n\n# 2) TransactionSummary base (aggregation of orders)\norderSummaryDF = (\n    ordersBronzeDF.alias(\"o\")\n    .join(orderDetailsBronzeDF.alias(\"od\"), \"OrderID\")\n    .where(F.col(\"o.Status\") == \"Completed\")\n    .groupBy(\"o.CustomerID\")\n    .agg(\n        F.countDistinct(\"o.OrderID\").alias(\"TotalOrders\"),\n        F.sum(F.col(\"od.Quantity\") * F.col(\"od.UnitPrice\") * (1 - F.col(\"od.Discount\"))).alias(\"TotalSpent\"),\n        F.min(\"o.OrderDate\").alias(\"FirstPurchaseDate\"),\n        F.max(\"o.OrderDate\").alias(\"LastPurchaseDate\")\n    )\n)\n\ntransactionSummaryDF = (\n    orderSummaryDF.alias(\"os\")\n    .join(\n        customerBaseDF.select(\"CustomerID\").alias(\"cb\"),\n        F.col(\"os.CustomerID\") == F.col(\"cb.CustomerID\"),\n        \"inner\"\n    )\n    .select(\n        F.col(\"os.CustomerID\"),\n        F.col(\"os.TotalOrders\"),\n        F.col(\"os.TotalSpent\"),\n        F.when(F.col(\"os.TotalOrders\") > 0, F.col(\"os.TotalSpent\") / F.col(\"os.TotalOrders\"))\n         .otherwise(F.lit(0.0)).alias(\"AvgOrderValue\"),\n        F.col(\"os.FirstPurchaseDate\"),\n        F.col(\"os.LastPurchaseDate\"),\n        F.datediff(F.lit(dataDate), F.col(\"os.LastPurchaseDate\")).alias(\"DaysSinceLastPurchase\")\n    )\n)\n\n------------------------------------------------\n-- SILVER LAYER: STAGE 2 (METRICS & SEGMENTATION)\n------------------------------------------------\n# (a) Identify top category/product and return rate\ncatSpendDF = (\n    ordersBronzeDF.alias(\"o\")\n    .join(orderDetailsBronzeDF.alias(\"od\"), \"OrderID\")\n    .join(productsBronzeDF.alias(\"p\"), \"ProductID\")\n    .where(F.col(\"o.Status\") == \"Completed\")\n    .groupBy(\"o.CustomerID\", \"p.Category\")\n    .agg(F.sum(F.col(\"od.Quantity\") * F.col(\"od.UnitPrice\") * (1 - F.col(\"od.Discount\"))).alias(\"CategorySpend\"))\n)\ncatWindow = Window.partitionBy(\"CustomerID\").orderBy(F.col(\"CategorySpend\").desc())\ncatTopDF = catSpendDF.withColumn(\"CategoryRank\", F.row_number().over(catWindow)).filter(F.col(\"CategoryRank\") == 1)\n\nprodQtyDF = (\n    ordersBronzeDF.alias(\"o\")\n    .join(orderDetailsBronzeDF.alias(\"od\"), \"OrderID\")\n    .join(productsBronzeDF.alias(\"p\"), \"ProductID\")\n    .where(F.col(\"o.Status\") == \"Completed\")\n    .groupBy(\"o.CustomerID\", \"p.ProductName\")\n    .agg(F.sum(\"od.Quantity\").alias(\"TotalQuantity\"))\n)\nprodWindow = Window.partitionBy(\"CustomerID\").orderBy(F.col(\"TotalQuantity\").desc())\nprodTopDF = prodQtyDF.withColumn(\"ProductRank\", F.row_number().over(prodWindow)).filter(F.col(\"ProductRank\") == 1)\n\nreturnsGroupedDF = (\n    returnsBronzeDF.groupBy(\"CustomerID\")\n    .agg(F.countDistinct(\"ReturnID\").alias(\"TotalReturns\"))\n)\n\ntransactionSummaryDF = (\n    transactionSummaryDF.alias(\"ts\")\n    .join(catTopDF.alias(\"cat\"), \"CustomerID\", \"left\")\n    .join(prodTopDF.alias(\"prod\"), \"CustomerID\", \"left\")\n    .join(returnsGroupedDF.alias(\"ret\"), \"CustomerID\", \"left\")\n    .select(\n        F.col(\"ts.CustomerID\"),\n        \"ts.TotalOrders\",\n        \"ts.TotalSpent\",\n        \"ts.AvgOrderValue\",\n        \"ts.FirstPurchaseDate\",\n        \"ts.LastPurchaseDate\",\n        \"ts.DaysSinceLastPurchase\",\n        F.col(\"cat.Category\").alias(\"TopCategory\"),\n        F.col(\"prod.ProductName\").alias(\"TopProduct\"),\n        F.when(F.col(\"ts.TotalOrders\") > 0, (F.col(\"ret.TotalReturns\") / F.col(\"ts.TotalOrders\")) * 100)\n         .otherwise(F.lit(0.0)).alias(\"ReturnRate\")\n    )\n)\n\n# (b) Compute RFM scoring\nwindowRecency = Window.orderBy(F.col(\"DaysSinceLastPurchase\").asc())\nwindowFrequency = Window.orderBy(F.col(\"TotalOrders\").desc())\nwindowMonetary = Window.orderBy(F.col(\"TotalSpent\").desc())\n\ntransactionSummaryDF = (\n    transactionSummaryDF\n    .withColumn(\"RecencyScore\", F.ntile(5).over(windowRecency))\n    .withColumn(\"FrequencyScore\", F.ntile(5).over(windowFrequency))\n    .withColumn(\"MonetaryScore\", F.ntile(5).over(windowMonetary))\n)\n\nrfmDF = transactionSummaryDF.select(\n    \"CustomerID\",\n    \"RecencyScore\",\n    \"FrequencyScore\",\n    \"MonetaryScore\",\n    (F.col(\"RecencyScore\") + F.col(\"FrequencyScore\") + F.col(\"MonetaryScore\")).alias(\"RFMScore\")\n)\n\n# (c) Churn Probability, Next Purchase Propensity, Loyalty Index\nchurnDF = (\n    transactionSummaryDF.select(\n        \"CustomerID\",\n        F.when(F.col(\"DaysSinceLastPurchase\") > churnDaysVeryHigh, 0.8)\n         .when(F.col(\"DaysSinceLastPurchase\") > churnDaysHigh, 0.5)\n         .when(F.col(\"DaysSinceLastPurchase\") > churnDaysMedium, 0.3)\n         .when(F.col(\"DaysSinceLastPurchase\") > churnDaysLow, 0.1)\n         .otherwise(0.05)\n         .alias(\"ChurnProbability\"),\n        F.when((F.col(\"DaysSinceLastPurchase\") < churnDaysLow) & (F.col(\"TotalOrders\") > 3), 0.8)\n         .when((F.col(\"DaysSinceLastPurchase\") < churnDaysMedium) & (F.col(\"TotalOrders\") > 2), 0.6)\n         .when(F.col(\"DaysSinceLastPurchase\") < churnDaysHigh, 0.4)\n         .otherwise(0.2)\n         .alias(\"NextPurchasePropensity\"),\n        (\n            F.when(\n                (F.col(\"TotalOrders\") >= loyaltyThreshold) & (F.col(\"ReturnRate\") < 10),\n                (F.col(\"TotalOrders\") * 0.6) + ((100 - F.col(\"ReturnRate\")) * 0.4)\n            )\n            .otherwise(F.col(\"TotalOrders\") * 0.4)\n            / 10\n        ).alias(\"LoyaltyIndex\")\n    )\n)\n\ncustomerMetricsDF = (\n    rfmDF.alias(\"r\")\n    .join(transactionSummaryDF.alias(\"t\"), \"CustomerID\")\n    .join(churnDF.alias(\"ch\"), \"CustomerID\")\n    .select(\n        F.col(\"r.CustomerID\"),\n        (F.col(\"t.TotalSpent\") * (1 + (1 - F.col(\"ch.ChurnProbability\")))).alias(\"CustomerLifetimeValue\"),\n        \"r.RecencyScore\",\n        \"r.FrequencyScore\",\n        \"r.MonetaryScore\",\n        \"r.RFMScore\",\n        \"ch.ChurnProbability\",\n        \"ch.NextPurchasePropensity\",\n        \"ch.LoyaltyIndex\",\n        F.when(F.col(\"ch.ChurnProbability\") > 0.7, \"At Risk\")\n         .when(F.col(\"ch.LoyaltyIndex\") > 0.8, \"Excellent\")\n         .when(F.col(\"ch.LoyaltyIndex\") > 0.6, \"Good\")\n         .when(F.col(\"ch.LoyaltyIndex\") > 0.4, \"Average\")\n         .otherwise(\"Needs Attention\")\n         .alias(\"CustomerHealth\")\n    )\n)\n\n# (d) Segmentation\nsegmentationDF = (\n    customerBaseDF.alias(\"cb\")\n    .join(customerMetricsDF.alias(\"cm\"), \"CustomerID\")\n    .join(transactionSummaryDF.alias(\"ts\"), \"CustomerID\")\n    .select(\n        F.col(\"cb.CustomerID\"),\n        # ValueSegment\n        F.when(F.col(\"cm.CustomerLifetimeValue\") >= highValueThreshold, \"High Value\")\n         .when(F.col(\"cm.CustomerLifetimeValue\") >= highValueThreshold*0.5, \"Medium Value\")\n         .otherwise(\"Low Value\").alias(\"ValueSegment\"),\n        # BehaviorSegment\n        F.when(F.col(\"cm.RFMScore\") >= 13, \"Champions\")\n         .when((F.col(\"cm.RecencyScore\") >= 4) & (F.col(\"cm.FrequencyScore\") >= 3), \"Loyal Customers\")\n         .when((F.col(\"cm.RecencyScore\") >= 4) & (F.col(\"cm.FrequencyScore\") <= 2), \"Potential Loyalists\")\n         .when((F.col(\"cm.RecencyScore\") <= 2) & (F.col(\"cm.FrequencyScore\") >= 3) & (F.col(\"cm.MonetaryScore\") >= 3), \"At Risk\")\n         .when((F.col(\"cm.RecencyScore\") <= 2) & (F.col(\"cm.FrequencyScore\") <= 2) & (F.col(\"cm.MonetaryScore\") <= 2), \"Hibernating\")\n         .when(F.col(\"cm.RecencyScore\") <= 1, \"Lost\")\n         .otherwise(\"Others\").alias(\"BehaviorSegment\"),\n        # LifecycleSegment\n        F.when((F.col(\"ts.TotalOrders\") == 1) & (F.col(\"ts.DaysSinceLastPurchase\") <= 30), \"New Customer\")\n         .when((F.col(\"ts.TotalOrders\") > 1) & (F.col(\"cm.ChurnProbability\") < 0.3), \"Active\")\n         .when((F.col(\"cm.ChurnProbability\") >= 0.3) & (F.col(\"cm.ChurnProbability\") < 0.7), \"At Risk\")\n         .when(F.col(\"cm.ChurnProbability\") >= 0.7, \"Churned\")\n         .otherwise(\"Inactive\").alias(\"LifecycleSegment\"),\n        # TargetGroup\n        F.when((F.col(\"cm.CustomerLifetimeValue\") >= highValueThreshold) & (F.col(\"cm.ChurnProbability\") < 0.3), \"VIP\")\n         .when((F.col(\"cm.CustomerLifetimeValue\") >= highValueThreshold * 0.5) & (F.col(\"cm.ChurnProbability\") >= 0.3), \"Retention Priority\")\n         .when((F.col(\"ts.TotalOrders\") == 1) & (F.col(\"ts.DaysSinceLastPurchase\") <= 30), \"Nurture New\")\n         .when(F.col(\"cm.NextPurchasePropensity\") > 0.6, \"Growth Opportunity\")\n         .when(F.col(\"ts.ReturnRate\") > 20, \"Service Improvement\")\n         .otherwise(\"Standard\").alias(\"TargetGroup\"),\n        # MarketingRecommendation\n        F.when(\n            (F.col(\"cm.CustomerLifetimeValue\") >= highValueThreshold) & (F.col(\"cm.ChurnProbability\") < 0.3),\n            \"Exclusive offers, VIP events, Personal shopping assistance, Early access to new products\"\n        ).when(\n            (F.col(\"cm.CustomerLifetimeValue\") >= highValueThreshold*0.5) & (F.col(\"cm.ChurnProbability\") >= 0.3),\n            \"Re-engagement campaign, Loyalty rewards, Personalized recommendations based on past purchases\"\n        ).when(\n            (F.col(\"ts.TotalOrders\") == 1) & (F.col(\"ts.DaysSinceLastPurchase\") <= 30),\n            \"Welcome series, Educational content, First purchase follow-up, Category exploration\"\n        ).when(\n            F.col(\"cm.NextPurchasePropensity\") > 0.6,\n            F.concat(\n                F.lit(\"Cross-sell/upsell, Bundle offers based on \"),\n                F.coalesce(F.col(\"ts.TopCategory\"), F.lit(\"preferred category\")),\n                F.lit(\", Category expansion\")\n            )\n        ).when(\n            F.col(\"ts.ReturnRate\") > 20,\n            \"Satisfaction survey, Improved product information, Size/fit guides, Service recovery\"\n        ).otherwise(\n            F.concat(\n                F.lit(\"Standard seasonal promotions, Category newsletters, Reactivation after \"),\n                (F.col(\"ts.DaysSinceLastPurchase\") / 30).cast(\"int\"),\n                F.lit(\" months\")\n            )\n        ).alias(\"MarketingRecommendation\")\n    )\n)\n\n# (Optional) Write Silver tables to Lakehouse\ncustomerBaseDF.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"lakehouse_silver.CustomerBase\")\ntransactionSummaryDF.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"lakehouse_silver.TransactionSummary\")\ncustomerMetricsDF.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"lakehouse_silver.CustomerMetrics\")\nsegmentationDF.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"lakehouse_silver.CustomerSegments\")\n\n------------------------------------------------\n-- GOLD LAYER: CUSTOMER PROFILES, ANALYTICS, SUMMARY\n------------------------------------------------\n\n# 1) CustomerProfiles (MERGE-like overwrite for demonstration)\ncustomerProfileGoldDF = (\n    customerBaseDF.alias(\"cb\")\n    .join(transactionSummaryDF.alias(\"ts\"), \"CustomerID\", \"left\")\n    .select(\n        \"cb.CustomerID\",\n        \"cb.CustomerName\",\n        \"cb.Email\",\n        \"cb.Phone\",\n        \"cb.Address\",\n        \"cb.PostalCode\",\n        \"cb.City\",\n        \"cb.State\",\n        \"cb.Country\",\n        \"cb.CustomerType\",\n        \"cb.AccountManager\",\n        \"cb.CreatedDate\",\n        \"cb.IsActive\",\n        \"cb.LastContactDate\",\n        \"ts.TotalOrders\",\n        \"ts.TotalSpent\",\n        \"ts.AvgOrderValue\",\n        \"ts.FirstPurchaseDate\",\n        \"ts.LastPurchaseDate\",\n        \"ts.DaysSinceLastPurchase\",\n        \"ts.TopCategory\",\n        \"ts.TopProduct\",\n        \"ts.ReturnRate\",\n        F.current_timestamp().alias(\"ModifiedDate\")  # for tracking\n    )\n)\n\n(\n    customerProfileGoldDF\n    .write\n    .format(\"delta\")\n    .mode(\"overwrite\")  # In production, consider MERGE for incremental changes\n    .saveAsTable(\"lakehouse_gold.CustomerProfiles\")\n)\n\n# 2) CustomerAnalytics: append daily snapshot\ncustomerAnalyticsGoldDF = (\n    customerMetricsDF.alias(\"cm\")\n    .join(segmentationDF.alias(\"cs\"), \"CustomerID\")\n    .select(\n        F.lit(dataDate).alias(\"ProcessDate\"),\n        F.col(\"cm.CustomerID\"),\n        F.col(\"cm.CustomerLifetimeValue\"),\n        F.col(\"cm.RecencyScore\"),\n        F.col(\"cm.FrequencyScore\"),\n        F.col(\"cm.MonetaryScore\"),\n        F.col(\"cm.RFMScore\"),\n        F.col(\"cm.ChurnProbability\"),\n        F.col(\"cm.NextPurchasePropensity\"),\n        F.col(\"cm.LoyaltyIndex\"),\n        F.col(\"cm.CustomerHealth\"),\n        F.col(\"cs.ValueSegment\"),\n        F.col(\"cs.BehaviorSegment\"),\n        F.col(\"cs.LifecycleSegment\"),\n        F.col(\"cs.TargetGroup\"),\n        F.col(\"cs.MarketingRecommendation\")\n    )\n)\n\n(\n    customerAnalyticsGoldDF\n    .write\n    .format(\"delta\")\n    .mode(\"append\")\n    .saveAsTable(\"lakehouse_gold.CustomerAnalytics\")\n)\n\n# Apply retention logic\nspark.sql(f\"\"\"\n    DELETE FROM lakehouse_gold.CustomerAnalytics\n    WHERE ProcessDate < DATEADD(DAY, -{retentionPeriodDays}, '{str(dataDate)}')\n\"\"\")\n\n# 3) Summary table: CustomerAnalyticsSummary\nsummaryWindow = Window.partitionBy()\nsummaryDF = (\n    customerBaseDF.alias(\"cb\")\n    .join(customerMetricsDF.alias(\"cm\"), \"CustomerID\")\n    .join(segmentationDF.alias(\"cs\"), \"CustomerID\")\n    .select(\n        F.lit(dataDate).alias(\"ProcessDate\"),\n        F.lit(processType).alias(\"ProcessType\"),\n        F.count_distinct(\"cb.CustomerID\").over(summaryWindow).alias(\"TotalCustomers\"),\n        F.sum(F.when(F.col(\"cs.LifecycleSegment\") == \"Active\", 1).otherwise(0)).over(summaryWindow).alias(\"ActiveCustomers\"),\n        F.sum(F.when(F.col(\"cs.LifecycleSegment\") == \"New Customer\", 1).otherwise(0)).over(summaryWindow).alias(\"NewCustomers\"),\n        F.sum(F.when((F.col(\"cs.LifecycleSegment\") == \"At Risk\") | (F.col(\"cs.LifecycleSegment\") == \"Churned\"), 1).otherwise(0)).over(summaryWindow).alias(\"ChurningCustomers\"),\n        F.sum(F.when(F.col(\"cs.ValueSegment\") == \"High Value\", 1).otherwise(0)).over(summaryWindow).alias(\"HighValueCustomers\"),\n        F.avg(\"cm.CustomerLifetimeValue\").over(summaryWindow).alias(\"AverageLifetimeValue\"),\n        (F.unix_timestamp(F.current_timestamp()) - F.unix_timestamp(F.lit(str(dataDate)))) * 1000,\n        F.lit(\"Success\").alias(\"ProcessingStatus\")\n    )\n).distinct()\n\n(\n    summaryDF\n    .write\n    .format(\"delta\")\n    .mode(\"append\")\n    .saveAsTable(\"lakehouse_gold.CustomerAnalyticsSummary\")\n)\n\nif debugMode:\n    summaryDF.show(truncate=False)\n\n# End of pipeline\n\n────────────────────────────────────────────────────────────────────────────\n3) IMPLEMENTATION GUIDELINES & BEST PRACTICES (TechLead)\n────────────────────────────────────────────────────────────────────────────\n\n• Parameter Management  \n  – Store thresholds (churnDays, highValueThreshold, retentionDays, etc.) externally in config tables or files for easy updates.\n\n• Use Delta Lake in Microsoft Fabric  \n  – Enables ACID transactions, MERGE, time-travel, and efficient partitioning for large datasets.  \n  – If you need partial updates (true incremental merges), replace overwrite with MERGE statements.\n\n• Error Handling & Logging  \n  – In production, wrap transformations in try/except blocks; log errors to a table or rely on pipeline logs.  \n  – Maintain run summaries (row counts, durations) to debug performance or data issues.\n\n• Incremental vs. Full Loads  \n  – For “FULL,” process entire historical data.  \n  – For “INCREMENTAL,” filter on ModifiedDate >= dataDate or use partitioning to minimize scanning.\n\n• Performance Optimizations  \n  – Cache or broadcast smaller dimension data.  \n  – Partition large tables by date (e.g., ProcessDate) if the dataset is large.  \n  – Ensure cluster sizing aligns with daily concurrency and data volumes.\n\n• CI/CD & Version Control  \n  – Store and version your PySpark scripts in a Git repo—automate tests and deployments.  \n  – Validate changes in a staging environment before promoting to production.\n\n────────────────────────────────────────────────────────────────────────────\n4) TEST CASES FOR VALIDATION (TestingAgent)\n────────────────────────────────────────────────────────────────────────────\n\n1) Full Load with Known Dataset  \n   – Include various customer types: new, active, churned, high-value, low-value.  \n   – Verify final metrics (transactions, RFM, churn) and segmentation.  \n   – Check daily summary matches expected counts.\n\n2) Incremental Load Test  \n   – Insert new or updated records with dates after dataDate.  \n   – Run pipeline with processType = 'INCREMENTAL'.  \n   – Validate only new or changed records appear, with no duplicates.\n\n3) Parameter Variation  \n   – Adjust churnDaysVeryHigh = 200 or highValueThreshold = 10000, etc.  \n   – Confirm churn probability, lifetime value, segment labels all reflect the new thresholds.\n\n4) Edge Cases (No Orders, No Interactions)  \n   – Include customers lacking orders or interactions.  \n   – Expect them to appear with zero-based or null metrics, and correct segmentation (maybe “Inactive” or “Hibernating”).\n\n5) Retention Check  \n   – Load older data beyond retentionPeriodDays (e.g., 365 days).  \n   – Confirm the pipeline removes older CustomerAnalytics records.\n\n6) Error Handling Scenarios  \n   – Introduce invalid references or partial data to simulate join errors.  \n   – Check that the pipeline fails gracefully or logs the error, avoiding silent corruption.\n\n────────────────────────────────────────────────────────────────────────────\nSUMMARY\n────────────────────────────────────────────────────────────────────────────\nThis end-to-end solution translates the original SQL stored procedure into a well-structured PySpark pipeline on Microsoft Fabric. By harnessing Delta Lake within the Medallion architecture, you achieve consistent, scalable, and maintainable analytics—covering raw ingestion (Bronze), curated transformations (Silver), and final analytics (Gold). Configurable parameters, robust testing, and best practices ensure the migrated solution remains flexible, performant, and easy to evolve as business requirements change.",
    "role": "user",
    "name": "TestingAgent"
  },
  {
    "content": "",
    "role": "user",
    "name": "User"
  },
  {
    "content": "",
    "role": "user",
    "name": "User"
  }
]